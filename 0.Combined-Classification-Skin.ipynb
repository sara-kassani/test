{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras import applications\n",
    "from keras.layers import Input\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "predict_data_dir = 'data/test'\n",
    "\n",
    "path_features = 'features/'\n",
    "path_folder='Combined_prediction/'\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27827 images belonging to 7 classes.\n",
      "Found 11233 images belonging to 7 classes.\n",
      "Found 1512 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "features_generator_train = datagen.flow_from_directory(train_data_dir,\n",
    "                                                target_size=(img_width, img_height),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode=None,\n",
    "                                                shuffle=False)\n",
    "features_generator_validation = datagen.flow_from_directory(validation_data_dir,\n",
    "                                                target_size=(img_width, img_height),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode=None,\n",
    "                                                shuffle=False)\n",
    "\n",
    "features_generator_test = datagen.flow_from_directory(predict_data_dir,\n",
    "                                                target_size=(img_width, img_height),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode=None,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatures(MODEL,features_generator_train,features_generator_validation,features_generator_test):\n",
    "    print('Extract features from', MODEL.__name__ )\n",
    "    #channel-last is for tensorflow backend.\n",
    "    input_tensor = Input(shape=(224, 224, 3))\n",
    "    feature_model = MODEL(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "    with h5py.File(path_features+\"Features_%s.h5\"%MODEL.__name__) as h:\n",
    "        features_train = feature_model.predict_generator(features_generator_train)\n",
    "        h.create_dataset(\"train\", data=features_train)\n",
    "        del features_train\n",
    "        gc.collect()\n",
    "\n",
    "        features_validation = feature_model.predict_generator(features_generator_validation)\n",
    "        h.create_dataset(\"validation\", data=features_validation)\n",
    "        del features_validation\n",
    "        gc.collect()\n",
    "\n",
    "        features_test = feature_model.predict_generator(features_generator_test)\n",
    "        h.create_dataset(\"test\", data=features_test)\n",
    "        del features_test\n",
    "        gc.collect()\n",
    "\n",
    "        h.create_dataset(\"label_train\", data=features_generator_train.classes)\n",
    "        h.create_dataset(\"label_validation\", data=features_generator_validation.classes)\n",
    "    print(MODEL.__name__,'feature saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract features from ResNet50\n",
      "ResNet50 feature saved\n",
      "Extract features from Xception\n",
      "Xception feature saved\n",
      "Extract features from InceptionV3\n",
      "InceptionV3 feature saved\n",
      "Extract features from VGG19\n",
      "VGG19 feature saved\n",
      "Extract features from VGG16\n",
      "VGG16 feature saved\n"
     ]
    }
   ],
   "source": [
    "for MODEL in [ResNet50,Xception,InceptionV3,VGG19,VGG16]:\n",
    "    ExtractFeatures(MODEL,features_generator_train,features_generator_validation,features_generator_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Features_Xception.h5\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b9ea3902963d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = os.listdir(path_features )\n",
    "\n",
    "for i,feature in enumerate(features):\n",
    "    if i != 3:\n",
    "        print(i,feature)\n",
    "        with h5py.File(path_features+feature, 'r') as h:\n",
    "            y_train = np.array(h['label_train'])\n",
    "            y_validation = np.array(h['label_validation'])\n",
    "\n",
    "            X_train = np.empty([len(y_train),0])\n",
    "            X_validation = np.empty([len(y_validation),0])\n",
    "\n",
    "\n",
    "            train=np.array(h['train'])\n",
    "            shape=train.shape\n",
    "            train=train.reshape(shape[0],shape[1]*shape[2]*shape[3])\n",
    "\n",
    "            validation=np.array(h['validation'])\n",
    "            shape=validation.shape\n",
    "            validation=validation.reshape(shape[0],shape[1]*shape[2]*shape[3])\n",
    "\n",
    "        X_train = np.concatenate((X_train,train),axis=1)\n",
    "        del train\n",
    "        gc.collect()\n",
    "\n",
    "        X_validation=np.concatenate((X_validation,validation),axis=1)\n",
    "        del validation\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape\",X_train.shape,\n",
    "        \"X_validation.shape\",X_validation.shape,\n",
    "        \"y_train.shape\",y_train.shape,\n",
    "        \"y_validation.shape\",y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the top model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu',input_shape=X_train.shape[1:]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "hist_combined=model.fit(X_train, y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_validation, y_validation)\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "#same the weights\n",
    "model.summary()\n",
    "model.save_weights(path_folder+'3_combined_top_weight.h5')\n",
    "print('Weight saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,10))\n",
    "epochs = range(1, len(hist_combined.history['acc']) + 1)\n",
    "plt.plot(epochs, hist_combined.history['loss'], label='Training Loss')\n",
    "plt.plot(epochs, hist_combined.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Loss of Top combined 4 Features')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(path_folder+'3_loss_combined_4.jpg')\n",
    "plt.show()\n",
    "print('Plot saved')\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "epochs = range(1, len(hist_combined.history['acc']) + 1)\n",
    "plt.plot(epochs, hist_combined.history['acc'], label='Training Accuracy')\n",
    "plt.plot(epochs, hist_combined.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy of Top combined 4 Features')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(path_folder+'3_acc_combined_4.jpg')\n",
    "plt.show()\n",
    "print('Plot saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the feature of test dataset and flatten\n",
    "# for i,feature in enumerate(features):\n",
    "#     if i != 3:\n",
    "#         print(i,feature)\n",
    "#         with h5py.File(path_features+feature, 'r') as h:\n",
    "#             test=np.array(h['test'])\n",
    "#             X_test = np.empty([len(test),0])\n",
    "#             shape=test.shape\n",
    "#             test=test.reshape(shape[0],shape[1]*shape[2]*shape[3])\n",
    "#         X_test=np.concatenate((X_test,test),axis=1)\n",
    "#         del test\n",
    "#         gc.collect()\n",
    "# print(X_test.shape)\n",
    "\n",
    "# #predict the class\n",
    "# pred_class = model.predict(X_test)\n",
    "# print(pred_class[:5])\n",
    "\n",
    "# #save the prediction\n",
    "# ID_n = np.arange(1,len(X_test)+1)\n",
    "# ID={'id':ID_n}\n",
    "\n",
    "\n",
    "# test_img = os.listdir(predict_data_dir+os.sep+'test')\n",
    "# print(test_img[0:5])\n",
    "\n",
    "# prediction_label_df=pd.DataFrame(ID)\n",
    "# prediction_label_df['label']=pred_class\n",
    "# prediction_label_df['Image']=test_img\n",
    "\n",
    "# print('Prediction save to ',path_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
