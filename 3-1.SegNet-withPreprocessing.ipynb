{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sara/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.layers.core import Layer \n",
    "from keras.layers.core import Activation, Lambda, Reshape, Permute\n",
    "from keras.layers.convolutional import Conv2D, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "\n",
    "img_rows = 480\n",
    "img_cols = 480\n",
    "\n",
    "smooth = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * i360ntersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def Specificity(y_true, y_pred):\n",
    "    true_negatives = K.abs(y_pred)- K.abs(y_true)\n",
    "    return ((true_negatives+smooth)/(y_pred+ smooth))\n",
    "\n",
    "def Sensitivity(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    return ((y_pred+smooth)/ (y_true+smooth))\n",
    "\n",
    "def Jaccard_index(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return ((intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, BatchNormalization, Dense, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "\n",
    "k_size=3\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_t = K.reshape(y_true,[-1,1])\n",
    "    y_p = K.reshape(y_pred,[-1,4])\n",
    "    losses = K.sparse_categorical_crossentropy(y_p,y_t, from_logits=True)\n",
    "    return K.sum(losses)\n",
    "\n",
    "def SegNet():\n",
    "    inputs = Input([480,480,1])\n",
    "\n",
    "    # Encoder\n",
    "    e = Convolution2D(32,k_size,k_size,border_mode='same')(inputs)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = Activation('relu')(e)\n",
    "    e = MaxPooling2D()(e)\n",
    "\n",
    "    e = Convolution2D(32,k_size,k_size,border_mode='same')(e)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = Activation('relu')(e)\n",
    "    e = MaxPooling2D()(e)\n",
    "\n",
    "    e = Convolution2D(32,k_size,k_size,border_mode='same')(e)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = Activation('relu')(e)\n",
    "    e = MaxPooling2D()(e)\n",
    "\n",
    "    # Decoder\n",
    "    d = UpSampling2D()(e)\n",
    "    d = Convolution2D(32,k_size,k_size,border_mode='same')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Activation('relu')(d)\n",
    "\n",
    "    d = UpSampling2D()(d)\n",
    "    d = Convolution2D(32,k_size,k_size,border_mode='same')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Activation('relu')(d)\n",
    "\n",
    "    d = UpSampling2D()(d)\n",
    "    d = Convolution2D(32,k_size,k_size,border_mode='same')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Activation('sigmoid')(d)\n",
    "\n",
    "    out = Convolution2D(1,1,1)(d)\n",
    "\n",
    "    model = Model(inputs, out)\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss=custom_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols, 1), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows), preserve_range=True)\n",
    "\n",
    "        #     imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    imgs_train = np.load('npy-with/preprocess/train-images.npy')\n",
    "    imgs_mask_train = np.load('npy-with/preprocess/train-masks.npy')\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "def load_validation_data():\n",
    "    imgs_validation = np.load('npy-with/preprocess/validation-images.npy')\n",
    "    imgs_mask_validation = np.load('npy-with/preprocess/validation-masks.npy')\n",
    "    return imgs_validation, imgs_mask_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Loading and preprocessing train data...')\n",
    "print('-'*30)\n",
    "imgs_train, imgs_mask_train = load_train_data()\n",
    "print(len(imgs_train))\n",
    "imgs_train = preprocess(imgs_train)\n",
    "imgs_mask_train = preprocess(imgs_mask_train)\n",
    "\n",
    "imgs_train = imgs_train.astype('float32')\n",
    "mean = np.mean(imgs_train)  # mean for data centering\n",
    "std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "imgs_train -= mean\n",
    "imgs_train /= std\n",
    "\n",
    "imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "imgs_mask_train /= 255.  # scale masks to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Loading and preprocessing validation data...')\n",
    "print('-'*30)\n",
    "imgs_validation, imgs_mask_validation = load_validation_data()\n",
    "print(len(imgs_validation))\n",
    "imgs_validation = preprocess(imgs_validation)\n",
    "imgs_mask_validation = preprocess(imgs_mask_validation)\n",
    "\n",
    "imgs_validation = imgs_validation.astype('float32')\n",
    "mean = np.mean(imgs_validation)  # mean for data centering\n",
    "std = np.std(imgs_validation)  # std for data normalization\n",
    "\n",
    "imgs_validation -= mean\n",
    "imgs_validation /= std\n",
    "\n",
    "imgs_mask_validation = imgs_mask_validation.astype('float32')\n",
    "imgs_mask_validation /= 255.  # scale masks to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Creating and compiling model...')\n",
    "print('-'*30)\n",
    "\n",
    "model = SegNet()\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),loss=dice_coef_loss, metrics=[dice_coef, 'acc',Jaccard_index, Specificity, Sensitivity])\n",
    "model_checkpoint = ModelCheckpoint('SegNet-with-weights.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Fitting model...')\n",
    "print('-'*30)\n",
    "history = model.fit(imgs_train, imgs_mask_train, batch_size=10, epochs=50, verbose=2, shuffle=True,\n",
    "          validation_data=(imgs_validation, imgs_mask_validation),\n",
    "          callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
