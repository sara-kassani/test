{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 6871946977877715155, name: \"/device:XLA_CPU:0\"\n device_type: \"XLA_CPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 12005190222062089101\n physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n device_type: \"XLA_GPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 11713313863965317364\n physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 15882446439\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 5413790482328475451\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, partial, rand, space_eval\nfrom sklearn.metrics import log_loss\nimport sys\nimport time\nimport math\nimport os\nimport pandas as pd\nfrom keras.callbacks import EarlyStopping\nimport numpy as np\nfrom glob import glob\nimport cv2\nimport skimage\nfrom skimage.transform import resize\nfrom keras.utils.np_utils import to_categorical\nimport keras\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.models import load_model\n# import keras.callbacks as kcall\nfrom keras.optimizers import Adam, RMSprop,SGD\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\n# from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\nfrom keras.applications.vgg19 import VGG19\nfrom keras.regularizers import l2, l1\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\n\nimport matplotlib.pyplot as plt\nfrom keras.layers import Input, concatenate\nfrom keras import optimizers, metrics, models\nfrom keras.layers import Input, Flatten, Dense\n\n%matplotlib inline","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\n\nprint(\"Keras Version\", keras.__version__)\nprint(\"tensorflow Version\", tf.__version__)\n# print(\"dim_ordering:\", K.image_dim_ordering())","execution_count":4,"outputs":[{"output_type":"stream","text":"Keras Version 2.2.4\ntensorflow Version 1.14.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nimg_height, img_width = 600, 450\ninput_shape = (img_height, img_width, 3)\nepochs = 1000","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/digestpath-classification/digestpath_classification/digestpath_classification\"))","execution_count":6,"outputs":[{"output_type":"stream","text":"['train', 'test']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/digestpath-classification/digestpath_classification/digestpath_classification/train/'\ntest_dir = '../input/digestpath-classification/digestpath_classification/digestpath_classification/test/'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(x):\n    # 'RGB'->'BGR'\n    x = x[:, :, ::-1]\n    # Zero-center by imagenet mean pixel\n    x[:, :, 0] -= 103.939\n    x[:, :, 1] -= 116.779\n    x[:, :, 2] -= 123.68\n    return x","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = np.random.seed(1142)\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n#     featurewise_center=True,\n#     featurewise_std_normalization=True,\n    preprocessing_function = preprocess_input,\n    validation_split= 0.2)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'training',\n    class_mode='categorical')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'validation',\n    class_mode='categorical')\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255, preprocessing_function = preprocess_input)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    class_mode='categorical')","execution_count":9,"outputs":[{"output_type":"stream","text":"Found 460 images belonging to 2 classes.\nFound 115 images belonging to 2 classes.\nFound 85 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(train_generator.filenames)\nnb_validation_samples = len(validation_generator.filenames)\nnb_test_samples = len(test_generator.filenames)\n\npredict_size_train = int(math.ceil(nb_train_samples / batch_size))\npredict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\npredict_size_test = int(math.ceil(nb_test_samples / batch_size))\n\nnum_classes = len(train_generator.class_indices)\n\nprint(\"nb_train_samples:\", nb_train_samples)\nprint(\"nb_validation_samples:\", nb_validation_samples)\nprint(\"nb_test_samples:\", nb_test_samples)\n\nprint(\"\\npredict_size_train:\", predict_size_train)\nprint(\"predict_size_validation:\", predict_size_validation)\nprint(\"predict_size_test:\", predict_size_test)\n\nprint(\"\\n num_classes:\", num_classes)","execution_count":10,"outputs":[{"output_type":"stream","text":"nb_train_samples: 460\nnb_validation_samples: 115\nnb_test_samples: 85\n\npredict_size_train: 8\npredict_size_validation: 2\npredict_size_test: 2\n\n num_classes: 2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir(\"extracted_features\")\nextracted_features_dir = \"extracted_features/\"\nmodel_name = \"MobileNet_descriptors\"","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg19_weights =\"../input/full-keras-pretrained-no-top/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_weights =\"../input/full-keras-pretrained-no-top//inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nvgg16_weights =\"../input/full-keras-pretrained-no-top/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet201_weights =\"../input/full-keras-pretrained-no-top/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet121_weights =\"../input/full-keras-pretrained-no-top/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nresenet50_weights =\"../input/full-keras-pretrained-no-top/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_resnet_v2_weights =\"../input/full-keras-pretrained-no-top/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nnasnet_weights =\"../input/full-keras-pretrained-no-top/nasnet_large_no_top.h5\"\nnasnet_mobile_weights =\"../input/full-keras-pretrained-no-top/nasnet_mobile_no_top.h5\"\nmobilenet_weights =\"../input/full-keras-pretrained-no-top/mobilenet_1_0_224_tf_no_top.h5\"\nxception_weights = \"../input/full-keras-pretrained-no-top/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.applications import DenseNet201\nfrom keras.applications import DenseNet121\nfrom keras.applications import ResNet50\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications import NASNetLarge, NASNetMobile\nfrom keras.applications import MobileNet","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model1=InceptionResNetV2(weights=inception_resnet_v2_weights, include_top=False, pooling = \"avg\")","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_final_model = base_model1","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i, layer in enumerate(model.layers):\n#     print(i, layer.name)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# c1 = model.layers[11].output \n# c1 = GlobalAveragePooling2D()(c1)       \n\n# c2 = model.layers[18].output\n# c2 = GlobalAveragePooling2D()(c2)       \n\n# c3 = model.layers[28].output\n# c3 = GlobalAveragePooling2D()(c3)       \n\n# c4 = model.layers[51].output\n# c4 = GlobalAveragePooling2D()(c4) \n\n# c5 = model.layers[74].output\n# c5 = GlobalAveragePooling2D()(c5) \n\n# c6 = model.layers[101].output\n# c6 = GlobalAveragePooling2D()(c6) \n\n# c7 = model.layers[120].output\n# c7 = GlobalAveragePooling2D()(c7) \n\n# c8 = model.layers[152].output\n# c8 = GlobalAveragePooling2D()(c8) \n\n# c9 = model.layers[184].output\n# c9 = GlobalAveragePooling2D()(c9) \n\n# c10 = model.layers[216].output\n# c10 = GlobalAveragePooling2D()(c10) \n\n# c11 = model.layers[248].output\n# c11 = GlobalAveragePooling2D()(c11) \n\n# c12 = model.layers[263].output\n# c12 = GlobalAveragePooling2D()(c12) \n\n# c13 = model.layers[294].output\n# c13 = GlobalAveragePooling2D()(c13) \n\n# con = concatenate([c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13])\n\n# bottleneck_final_model = Model(inputs=model.input, outputs=con)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_train = bottleneck_final_model.predict_generator(train_generator, predict_size_train, max_q_size=1, pickle_safe=False)\nnp.save(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_validation = bottleneck_final_model.predict_generator(validation_generator, predict_size_validation)\nnp.save(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_test = bottleneck_final_model.predict_generator(test_generator, predict_size_test)\nnp.save(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\n# from keras.backend.tensorflow_backend import get_session\n# from keras.backend.tensorflow_backend import clear_session\n# from keras.backend.tensorflow_backend import set_session\n\n# def reset_keras_tf_session():\n#     \"\"\"\n#     this function clears the gpu memory and set the \n#     tf session to not use the whole gpu\n#     \"\"\"\n#     sess = get_session()\n#     clear_session()\n#     sess.close()\n#     sess = get_session()\n\n# #     config = tf.ConfigProto()\n# #     config.gpu_options.allow_growth = True\n# #     set_session(tf.Session(config=config))\n\n# reset_keras_tf_session()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = np.load(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy')\nvalidation_data = np.load(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy')\ntest_data = np.load(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy')\n\ntrain_labels = train_generator.classes\ntrain_labels = to_categorical(train_labels, num_classes=num_classes)\n\nvalidation_labels = validation_generator.classes\nvalidation_labels = to_categorical(validation_labels, num_classes=num_classes)\n\ntest_labels = test_generator.classes\ntest_labels = to_categorical(test_labels, num_classes=num_classes)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adam_opt=Adam(lr=0.0001, beta_1=0.7, beta_2=0.5)\n\nmodel = Sequential()\nmodel.add(Dense(2048, activation=\"tanh\", kernel_regularizer=l2(0.01), bias_regularizer=l2(1e-06), activity_regularizer=l1(1e-05)))\n# model.add(Dropout(0.5))\n\nmodel.add(Dense(128, activation=\"tanh\", kernel_regularizer=l2(0.01), bias_regularizer=l2(1e-06), activity_regularizer=l1(1e-05)))\n# model.add(Dropout(0.5))\n\n# model.add(Dense(512, activation=\"relu\", kernel_regularizer=l2(1e-06), bias_regularizer=l2(0.01), activity_regularizer=l1(1e-07)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_classes, activation=\"softmax\"))\n\nmodel.compile(optimizer=adam_opt, loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_data, train_labels,\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    validation_data=(validation_data, validation_labels),\n                    verbose= 2)","execution_count":43,"outputs":[{"output_type":"stream","text":"Train on 460 samples, validate on 115 samples\nEpoch 1/1000\n - 4s - loss: 20.6041 - acc: 0.7109 - val_loss: 20.1297 - val_acc: 0.7913\nEpoch 2/1000\n - 0s - loss: 19.9161 - acc: 0.7957 - val_loss: 19.5037 - val_acc: 0.8000\nEpoch 3/1000\n - 0s - loss: 19.2005 - acc: 0.8457 - val_loss: 18.7726 - val_acc: 0.8522\nEpoch 4/1000\n - 0s - loss: 18.4287 - acc: 0.8674 - val_loss: 18.0999 - val_acc: 0.8087\nEpoch 5/1000\n - 0s - loss: 17.7558 - acc: 0.8652 - val_loss: 17.5437 - val_acc: 0.8000\nEpoch 6/1000\n - 0s - loss: 17.1464 - acc: 0.8500 - val_loss: 16.7976 - val_acc: 0.8435\nEpoch 7/1000\n - 0s - loss: 16.5289 - acc: 0.8870 - val_loss: 16.1916 - val_acc: 0.8348\nEpoch 8/1000\n - 0s - loss: 15.9036 - acc: 0.8761 - val_loss: 15.6148 - val_acc: 0.8348\nEpoch 9/1000\n - 0s - loss: 15.3985 - acc: 0.8783 - val_loss: 15.2342 - val_acc: 0.8174\nEpoch 10/1000\n - 0s - loss: 14.9097 - acc: 0.8826 - val_loss: 14.7728 - val_acc: 0.8087\nEpoch 11/1000\n - 0s - loss: 14.3405 - acc: 0.8935 - val_loss: 14.1029 - val_acc: 0.8348\nEpoch 12/1000\n - 0s - loss: 13.7368 - acc: 0.9065 - val_loss: 13.5328 - val_acc: 0.8261\nEpoch 13/1000\n - 0s - loss: 13.1839 - acc: 0.8891 - val_loss: 12.9381 - val_acc: 0.8522\nEpoch 14/1000\n - 0s - loss: 12.6811 - acc: 0.8957 - val_loss: 12.5217 - val_acc: 0.8261\nEpoch 15/1000\n - 0s - loss: 12.2172 - acc: 0.9022 - val_loss: 12.0730 - val_acc: 0.8261\nEpoch 16/1000\n - 0s - loss: 11.7932 - acc: 0.8978 - val_loss: 11.5988 - val_acc: 0.8522\nEpoch 17/1000\n - 0s - loss: 11.3283 - acc: 0.9174 - val_loss: 11.1598 - val_acc: 0.8348\nEpoch 18/1000\n - 0s - loss: 10.8554 - acc: 0.9174 - val_loss: 10.7599 - val_acc: 0.8261\nEpoch 19/1000\n - 0s - loss: 10.4779 - acc: 0.8848 - val_loss: 10.4520 - val_acc: 0.8087\nEpoch 20/1000\n - 0s - loss: 10.0893 - acc: 0.9130 - val_loss: 9.9417 - val_acc: 0.8435\nEpoch 21/1000\n - 0s - loss: 9.7127 - acc: 0.9130 - val_loss: 9.6242 - val_acc: 0.8348\nEpoch 22/1000\n - 0s - loss: 9.3592 - acc: 0.9196 - val_loss: 9.2264 - val_acc: 0.8696\nEpoch 23/1000\n - 0s - loss: 8.9728 - acc: 0.9304 - val_loss: 8.8735 - val_acc: 0.8348\nEpoch 24/1000\n - 0s - loss: 8.5834 - acc: 0.9304 - val_loss: 8.4796 - val_acc: 0.8348\nEpoch 25/1000\n - 0s - loss: 8.2277 - acc: 0.9217 - val_loss: 8.1721 - val_acc: 0.8000\nEpoch 26/1000\n - 0s - loss: 7.9665 - acc: 0.9152 - val_loss: 7.8874 - val_acc: 0.8348\nEpoch 27/1000\n - 0s - loss: 7.6580 - acc: 0.9152 - val_loss: 7.6630 - val_acc: 0.8261\nEpoch 28/1000\n - 0s - loss: 7.4079 - acc: 0.9043 - val_loss: 7.3904 - val_acc: 0.8261\nEpoch 29/1000\n - 0s - loss: 7.1121 - acc: 0.9304 - val_loss: 7.0541 - val_acc: 0.8696\nEpoch 30/1000\n - 0s - loss: 6.8241 - acc: 0.9217 - val_loss: 6.8206 - val_acc: 0.8174\nEpoch 31/1000\n - 0s - loss: 6.5417 - acc: 0.9370 - val_loss: 6.5328 - val_acc: 0.8435\nEpoch 32/1000\n - 0s - loss: 6.2846 - acc: 0.9370 - val_loss: 6.2648 - val_acc: 0.8696\nEpoch 33/1000\n - 0s - loss: 6.0149 - acc: 0.9435 - val_loss: 6.0059 - val_acc: 0.8696\nEpoch 34/1000\n - 0s - loss: 5.7525 - acc: 0.9391 - val_loss: 5.7522 - val_acc: 0.8783\nEpoch 35/1000\n - 0s - loss: 5.5369 - acc: 0.9326 - val_loss: 5.6003 - val_acc: 0.8261\nEpoch 36/1000\n - 0s - loss: 5.3004 - acc: 0.9239 - val_loss: 5.2983 - val_acc: 0.8783\nEpoch 37/1000\n - 0s - loss: 5.0369 - acc: 0.9500 - val_loss: 5.3450 - val_acc: 0.8087\nEpoch 38/1000\n - 0s - loss: 4.9253 - acc: 0.9109 - val_loss: 4.9391 - val_acc: 0.8522\nEpoch 39/1000\n - 0s - loss: 4.6815 - acc: 0.9413 - val_loss: 4.7336 - val_acc: 0.8696\nEpoch 40/1000\n - 0s - loss: 4.5296 - acc: 0.9391 - val_loss: 4.5827 - val_acc: 0.8609\nEpoch 41/1000\n - 0s - loss: 4.3506 - acc: 0.9435 - val_loss: 4.4137 - val_acc: 0.8870\nEpoch 42/1000\n - 0s - loss: 4.2255 - acc: 0.9522 - val_loss: 4.3053 - val_acc: 0.8522\nEpoch 43/1000\n - 0s - loss: 4.0641 - acc: 0.9522 - val_loss: 4.1473 - val_acc: 0.8696\nEpoch 44/1000\n - 0s - loss: 3.9139 - acc: 0.9457 - val_loss: 4.1047 - val_acc: 0.8087\nEpoch 45/1000\n - 0s - loss: 3.7618 - acc: 0.9500 - val_loss: 3.9055 - val_acc: 0.8261\nEpoch 46/1000\n - 0s - loss: 3.6639 - acc: 0.9457 - val_loss: 3.8939 - val_acc: 0.8174\nEpoch 47/1000\n - 0s - loss: 3.5350 - acc: 0.9478 - val_loss: 3.6460 - val_acc: 0.8522\nEpoch 48/1000\n - 0s - loss: 3.4060 - acc: 0.9500 - val_loss: 3.5461 - val_acc: 0.8348\nEpoch 49/1000\n - 0s - loss: 3.3041 - acc: 0.9370 - val_loss: 3.4076 - val_acc: 0.8870\nEpoch 50/1000\n - 0s - loss: 3.2271 - acc: 0.9348 - val_loss: 3.3381 - val_acc: 0.8783\nEpoch 51/1000\n - 0s - loss: 3.1373 - acc: 0.9543 - val_loss: 3.2754 - val_acc: 0.8435\nEpoch 52/1000\n - 0s - loss: 3.0399 - acc: 0.9522 - val_loss: 3.1842 - val_acc: 0.8522\nEpoch 53/1000\n - 0s - loss: 2.9748 - acc: 0.9478 - val_loss: 3.3466 - val_acc: 0.7913\nEpoch 54/1000\n - 0s - loss: 2.9067 - acc: 0.9457 - val_loss: 3.0049 - val_acc: 0.8783\nEpoch 55/1000\n - 0s - loss: 2.7875 - acc: 0.9565 - val_loss: 3.0824 - val_acc: 0.8087\nEpoch 56/1000\n - 0s - loss: 2.7121 - acc: 0.9457 - val_loss: 2.8723 - val_acc: 0.8435\nEpoch 57/1000\n - 0s - loss: 2.6016 - acc: 0.9630 - val_loss: 2.7635 - val_acc: 0.8348\nEpoch 58/1000\n - 0s - loss: 2.5170 - acc: 0.9478 - val_loss: 2.6830 - val_acc: 0.8522\nEpoch 59/1000\n - 0s - loss: 2.4296 - acc: 0.9674 - val_loss: 2.6242 - val_acc: 0.8348\nEpoch 60/1000\n - 0s - loss: 2.3665 - acc: 0.9630 - val_loss: 2.5550 - val_acc: 0.8348\nEpoch 61/1000\n - 0s - loss: 2.3067 - acc: 0.9609 - val_loss: 2.4727 - val_acc: 0.8522\nEpoch 62/1000\n - 0s - loss: 2.2317 - acc: 0.9587 - val_loss: 2.6755 - val_acc: 0.7913\nEpoch 63/1000\n - 0s - loss: 2.2188 - acc: 0.9326 - val_loss: 2.5718 - val_acc: 0.8000\nEpoch 64/1000\n - 0s - loss: 2.1588 - acc: 0.9457 - val_loss: 2.3233 - val_acc: 0.8609\nEpoch 65/1000\n - 0s - loss: 2.0644 - acc: 0.9717 - val_loss: 2.2389 - val_acc: 0.8957\nEpoch 66/1000\n - 0s - loss: 2.0245 - acc: 0.9500 - val_loss: 2.2013 - val_acc: 0.8783\nEpoch 67/1000\n - 0s - loss: 1.9427 - acc: 0.9761 - val_loss: 2.1215 - val_acc: 0.8696\nEpoch 68/1000\n - 0s - loss: 1.9091 - acc: 0.9478 - val_loss: 2.1883 - val_acc: 0.8174\nEpoch 69/1000\n - 0s - loss: 1.9007 - acc: 0.9174 - val_loss: 2.0963 - val_acc: 0.8261\nEpoch 70/1000\n - 0s - loss: 1.8090 - acc: 0.9609 - val_loss: 1.9858 - val_acc: 0.8957\nEpoch 71/1000\n - 0s - loss: 1.7558 - acc: 0.9674 - val_loss: 1.9744 - val_acc: 0.8435\nEpoch 72/1000\n - 0s - loss: 1.7059 - acc: 0.9630 - val_loss: 1.9207 - val_acc: 0.8435\nEpoch 73/1000\n - 0s - loss: 1.6901 - acc: 0.9500 - val_loss: 1.9640 - val_acc: 0.8174\nEpoch 74/1000\n - 0s - loss: 1.6374 - acc: 0.9696 - val_loss: 1.9537 - val_acc: 0.8174\nEpoch 75/1000\n - 0s - loss: 1.6161 - acc: 0.9543 - val_loss: 1.8677 - val_acc: 0.8435\nEpoch 76/1000\n - 0s - loss: 1.6054 - acc: 0.9457 - val_loss: 1.8254 - val_acc: 0.8348\nEpoch 77/1000\n - 0s - loss: 1.5331 - acc: 0.9696 - val_loss: 1.7614 - val_acc: 0.8609\nEpoch 78/1000\n - 0s - loss: 1.4923 - acc: 0.9783 - val_loss: 1.8595 - val_acc: 0.8261\nEpoch 79/1000\n - 0s - loss: 1.4892 - acc: 0.9565 - val_loss: 1.6941 - val_acc: 0.8609\nEpoch 80/1000\n - 0s - loss: 1.4547 - acc: 0.9674 - val_loss: 1.7978 - val_acc: 0.8174\nEpoch 81/1000\n - 0s - loss: 1.4213 - acc: 0.9674 - val_loss: 1.6502 - val_acc: 0.8522\nEpoch 82/1000\n - 0s - loss: 1.4076 - acc: 0.9543 - val_loss: 1.8232 - val_acc: 0.8087\nEpoch 83/1000\n - 0s - loss: 1.3896 - acc: 0.9587 - val_loss: 1.6016 - val_acc: 0.8522\nEpoch 84/1000\n - 0s - loss: 1.3415 - acc: 0.9804 - val_loss: 1.6127 - val_acc: 0.8348\nEpoch 85/1000\n - 0s - loss: 1.2999 - acc: 0.9783 - val_loss: 1.5541 - val_acc: 0.8522\nEpoch 86/1000\n - 0s - loss: 1.2742 - acc: 0.9717 - val_loss: 1.5227 - val_acc: 0.8522\nEpoch 87/1000\n - 0s - loss: 1.2469 - acc: 0.9739 - val_loss: 1.4772 - val_acc: 0.8957\nEpoch 88/1000\n - 0s - loss: 1.2187 - acc: 0.9652 - val_loss: 1.4587 - val_acc: 0.8609\nEpoch 89/1000\n - 0s - loss: 1.2184 - acc: 0.9696 - val_loss: 1.4629 - val_acc: 0.8435\nEpoch 90/1000\n - 0s - loss: 1.1761 - acc: 0.9804 - val_loss: 1.4845 - val_acc: 0.8435\nEpoch 91/1000\n - 0s - loss: 1.1661 - acc: 0.9652 - val_loss: 1.4512 - val_acc: 0.8348\nEpoch 92/1000\n - 0s - loss: 1.1398 - acc: 0.9804 - val_loss: 1.5100 - val_acc: 0.8174\nEpoch 93/1000\n - 0s - loss: 1.1368 - acc: 0.9761 - val_loss: 1.3929 - val_acc: 0.8522\nEpoch 94/1000\n - 0s - loss: 1.1397 - acc: 0.9652 - val_loss: 1.3592 - val_acc: 0.8957\nEpoch 95/1000\n - 0s - loss: 1.0873 - acc: 0.9848 - val_loss: 1.3346 - val_acc: 0.8870\n","name":"stdout"},{"output_type":"stream","text":"Epoch 96/1000\n - 0s - loss: 1.0659 - acc: 0.9826 - val_loss: 1.3368 - val_acc: 0.8522\nEpoch 97/1000\n - 0s - loss: 1.0424 - acc: 0.9783 - val_loss: 1.3121 - val_acc: 0.8522\nEpoch 98/1000\n - 0s - loss: 1.0355 - acc: 0.9739 - val_loss: 1.2972 - val_acc: 0.8522\nEpoch 99/1000\n - 0s - loss: 1.0048 - acc: 0.9848 - val_loss: 1.5284 - val_acc: 0.7826\nEpoch 100/1000\n - 0s - loss: 1.0109 - acc: 0.9652 - val_loss: 1.2854 - val_acc: 0.8348\nEpoch 101/1000\n - 0s - loss: 0.9713 - acc: 0.9804 - val_loss: 1.2479 - val_acc: 0.8870\nEpoch 102/1000\n - 0s - loss: 0.9666 - acc: 0.9739 - val_loss: 1.2395 - val_acc: 0.8522\nEpoch 103/1000\n - 0s - loss: 0.9488 - acc: 0.9804 - val_loss: 1.2282 - val_acc: 0.8522\nEpoch 104/1000\n - 0s - loss: 0.9240 - acc: 0.9848 - val_loss: 1.1972 - val_acc: 0.8783\nEpoch 105/1000\n - 0s - loss: 0.9328 - acc: 0.9652 - val_loss: 1.2053 - val_acc: 0.8435\nEpoch 106/1000\n - 0s - loss: 0.8990 - acc: 0.9804 - val_loss: 1.2016 - val_acc: 0.8348\nEpoch 107/1000\n - 0s - loss: 0.8866 - acc: 0.9804 - val_loss: 1.1966 - val_acc: 0.8435\nEpoch 108/1000\n - 0s - loss: 0.8706 - acc: 0.9913 - val_loss: 1.3029 - val_acc: 0.8261\nEpoch 109/1000\n - 0s - loss: 0.8654 - acc: 0.9761 - val_loss: 1.1495 - val_acc: 0.8522\nEpoch 110/1000\n - 0s - loss: 0.8812 - acc: 0.9717 - val_loss: 1.1329 - val_acc: 0.8609\nEpoch 111/1000\n - 0s - loss: 0.8343 - acc: 0.9891 - val_loss: 1.2145 - val_acc: 0.8261\nEpoch 112/1000\n - 0s - loss: 0.8237 - acc: 0.9891 - val_loss: 1.1601 - val_acc: 0.8435\nEpoch 113/1000\n - 0s - loss: 0.8663 - acc: 0.9543 - val_loss: 1.1647 - val_acc: 0.8348\nEpoch 114/1000\n - 0s - loss: 0.8141 - acc: 0.9826 - val_loss: 1.1043 - val_acc: 0.8522\nEpoch 115/1000\n - 0s - loss: 0.8284 - acc: 0.9696 - val_loss: 1.2146 - val_acc: 0.8261\nEpoch 116/1000\n - 0s - loss: 0.7863 - acc: 0.9891 - val_loss: 1.0892 - val_acc: 0.8696\nEpoch 117/1000\n - 0s - loss: 0.7735 - acc: 0.9891 - val_loss: 1.1440 - val_acc: 0.8261\nEpoch 118/1000\n - 0s - loss: 0.7578 - acc: 0.9870 - val_loss: 1.1181 - val_acc: 0.8261\nEpoch 119/1000\n - 0s - loss: 0.7444 - acc: 0.9826 - val_loss: 1.0485 - val_acc: 0.8696\nEpoch 120/1000\n - 0s - loss: 0.7782 - acc: 0.9609 - val_loss: 1.0298 - val_acc: 0.8696\nEpoch 121/1000\n - 0s - loss: 0.7597 - acc: 0.9652 - val_loss: 1.0592 - val_acc: 0.8435\nEpoch 122/1000\n - 0s - loss: 0.7184 - acc: 0.9891 - val_loss: 1.0457 - val_acc: 0.8348\nEpoch 123/1000\n - 0s - loss: 0.7208 - acc: 0.9870 - val_loss: 1.1267 - val_acc: 0.8261\nEpoch 124/1000\n - 0s - loss: 0.7317 - acc: 0.9674 - val_loss: 1.0847 - val_acc: 0.8435\nEpoch 125/1000\n - 0s - loss: 0.7155 - acc: 0.9848 - val_loss: 1.1779 - val_acc: 0.8174\nEpoch 126/1000\n - 0s - loss: 0.7038 - acc: 0.9826 - val_loss: 0.9976 - val_acc: 0.8696\nEpoch 127/1000\n - 0s - loss: 0.6973 - acc: 0.9783 - val_loss: 1.0071 - val_acc: 0.8522\nEpoch 128/1000\n - 0s - loss: 0.7039 - acc: 0.9717 - val_loss: 1.1399 - val_acc: 0.8174\nEpoch 129/1000\n - 0s - loss: 0.6951 - acc: 0.9739 - val_loss: 1.0328 - val_acc: 0.8435\nEpoch 130/1000\n - 0s - loss: 0.6993 - acc: 0.9783 - val_loss: 1.0080 - val_acc: 0.8435\nEpoch 131/1000\n - 0s - loss: 0.6582 - acc: 0.9870 - val_loss: 0.9852 - val_acc: 0.8522\nEpoch 132/1000\n - 0s - loss: 0.6477 - acc: 0.9848 - val_loss: 1.1072 - val_acc: 0.8174\nEpoch 133/1000\n - 0s - loss: 0.6546 - acc: 0.9717 - val_loss: 1.0215 - val_acc: 0.8348\nEpoch 134/1000\n - 0s - loss: 0.6772 - acc: 0.9652 - val_loss: 1.0096 - val_acc: 0.8261\nEpoch 135/1000\n - 0s - loss: 0.6423 - acc: 0.9891 - val_loss: 1.0420 - val_acc: 0.8261\nEpoch 136/1000\n - 0s - loss: 0.6275 - acc: 0.9848 - val_loss: 0.9393 - val_acc: 0.8609\nEpoch 137/1000\n - 0s - loss: 0.6105 - acc: 0.9957 - val_loss: 0.9236 - val_acc: 0.8609\nEpoch 138/1000\n - 0s - loss: 0.6017 - acc: 0.9891 - val_loss: 0.9178 - val_acc: 0.8696\nEpoch 139/1000\n - 0s - loss: 0.5985 - acc: 0.9935 - val_loss: 0.9163 - val_acc: 0.8522\nEpoch 140/1000\n - 0s - loss: 0.5826 - acc: 0.9913 - val_loss: 1.1830 - val_acc: 0.7826\nEpoch 141/1000\n - 0s - loss: 0.6247 - acc: 0.9652 - val_loss: 0.9121 - val_acc: 0.8609\nEpoch 142/1000\n - 0s - loss: 0.6266 - acc: 0.9652 - val_loss: 0.9124 - val_acc: 0.8609\nEpoch 143/1000\n - 0s - loss: 0.5723 - acc: 0.9891 - val_loss: 0.8838 - val_acc: 0.8696\nEpoch 144/1000\n - 0s - loss: 0.5601 - acc: 0.9913 - val_loss: 0.9022 - val_acc: 0.8609\nEpoch 145/1000\n - 0s - loss: 0.5538 - acc: 0.9913 - val_loss: 0.9748 - val_acc: 0.8261\nEpoch 146/1000\n - 0s - loss: 0.5522 - acc: 0.9826 - val_loss: 0.8697 - val_acc: 0.8783\nEpoch 147/1000\n - 0s - loss: 0.5520 - acc: 0.9913 - val_loss: 0.8617 - val_acc: 0.8783\nEpoch 148/1000\n - 0s - loss: 0.5456 - acc: 0.9935 - val_loss: 0.8564 - val_acc: 0.8696\nEpoch 149/1000\n - 0s - loss: 0.5336 - acc: 0.9913 - val_loss: 0.8536 - val_acc: 0.8696\nEpoch 150/1000\n - 0s - loss: 0.5289 - acc: 0.9913 - val_loss: 0.8499 - val_acc: 0.8783\nEpoch 151/1000\n - 0s - loss: 0.5363 - acc: 0.9870 - val_loss: 0.8642 - val_acc: 0.8609\nEpoch 152/1000\n - 0s - loss: 0.5504 - acc: 0.9674 - val_loss: 1.0789 - val_acc: 0.8087\nEpoch 153/1000\n - 0s - loss: 0.5398 - acc: 0.9826 - val_loss: 0.8290 - val_acc: 0.8696\nEpoch 154/1000\n - 0s - loss: 0.5092 - acc: 0.9957 - val_loss: 0.8200 - val_acc: 0.8696\nEpoch 155/1000\n - 0s - loss: 0.5055 - acc: 0.9957 - val_loss: 0.8137 - val_acc: 0.8783\nEpoch 156/1000\n - 0s - loss: 0.4969 - acc: 0.9913 - val_loss: 0.8113 - val_acc: 0.8783\nEpoch 157/1000\n - 0s - loss: 0.4946 - acc: 0.9891 - val_loss: 0.8796 - val_acc: 0.8261\nEpoch 158/1000\n - 0s - loss: 0.4965 - acc: 0.9913 - val_loss: 0.9080 - val_acc: 0.8435\nEpoch 159/1000\n - 0s - loss: 0.4811 - acc: 0.9935 - val_loss: 0.8436 - val_acc: 0.8435\nEpoch 160/1000\n - 0s - loss: 0.4873 - acc: 0.9804 - val_loss: 0.8016 - val_acc: 0.8522\nEpoch 161/1000\n - 0s - loss: 0.5046 - acc: 0.9761 - val_loss: 1.0295 - val_acc: 0.8174\nEpoch 162/1000\n - 0s - loss: 0.5006 - acc: 0.9696 - val_loss: 0.8108 - val_acc: 0.8609\nEpoch 163/1000\n - 0s - loss: 0.4643 - acc: 0.9978 - val_loss: 0.8482 - val_acc: 0.8348\nEpoch 164/1000\n - 0s - loss: 0.4657 - acc: 0.9957 - val_loss: 0.7808 - val_acc: 0.8609\nEpoch 165/1000\n - 0s - loss: 0.4509 - acc: 0.9957 - val_loss: 0.7761 - val_acc: 0.8696\nEpoch 166/1000\n - 0s - loss: 0.4683 - acc: 0.9848 - val_loss: 0.9102 - val_acc: 0.8261\nEpoch 167/1000\n - 0s - loss: 0.4497 - acc: 0.9935 - val_loss: 0.7701 - val_acc: 0.8783\nEpoch 168/1000\n - 0s - loss: 0.4352 - acc: 0.9935 - val_loss: 0.7687 - val_acc: 0.8783\nEpoch 169/1000\n - 0s - loss: 0.4451 - acc: 0.9935 - val_loss: 0.8125 - val_acc: 0.8522\nEpoch 170/1000\n - 0s - loss: 0.4363 - acc: 0.9957 - val_loss: 0.9593 - val_acc: 0.8087\nEpoch 171/1000\n - 0s - loss: 0.4435 - acc: 0.9826 - val_loss: 0.8732 - val_acc: 0.8087\nEpoch 172/1000\n - 0s - loss: 0.4737 - acc: 0.9630 - val_loss: 0.8024 - val_acc: 0.8609\nEpoch 173/1000\n - 0s - loss: 0.4222 - acc: 0.9957 - val_loss: 0.8566 - val_acc: 0.8348\nEpoch 174/1000\n - 0s - loss: 0.4246 - acc: 0.9870 - val_loss: 0.8102 - val_acc: 0.8522\nEpoch 175/1000\n - 0s - loss: 0.4577 - acc: 0.9761 - val_loss: 0.9123 - val_acc: 0.8261\nEpoch 176/1000\n - 0s - loss: 0.4175 - acc: 0.9891 - val_loss: 0.8321 - val_acc: 0.8348\nEpoch 177/1000\n - 0s - loss: 0.4066 - acc: 0.9957 - val_loss: 0.9099 - val_acc: 0.8174\nEpoch 178/1000\n - 0s - loss: 0.4236 - acc: 0.9804 - val_loss: 0.7428 - val_acc: 0.8783\nEpoch 179/1000\n - 0s - loss: 0.4052 - acc: 0.9957 - val_loss: 0.8363 - val_acc: 0.8261\nEpoch 180/1000\n - 0s - loss: 0.4080 - acc: 0.9848 - val_loss: 0.7370 - val_acc: 0.8696\nEpoch 181/1000\n - 0s - loss: 0.3923 - acc: 0.9935 - val_loss: 1.0431 - val_acc: 0.7739\nEpoch 182/1000\n - 0s - loss: 0.4350 - acc: 0.9674 - val_loss: 0.7358 - val_acc: 0.8783\nEpoch 183/1000\n - 0s - loss: 0.3981 - acc: 0.9935 - val_loss: 0.7269 - val_acc: 0.8609\nEpoch 184/1000\n - 0s - loss: 0.3799 - acc: 0.9978 - val_loss: 0.7184 - val_acc: 0.8696\nEpoch 185/1000\n - 0s - loss: 0.3897 - acc: 0.9870 - val_loss: 0.7280 - val_acc: 0.8522\nEpoch 186/1000\n - 0s - loss: 0.3817 - acc: 0.9913 - val_loss: 1.0665 - val_acc: 0.7739\nEpoch 187/1000\n - 0s - loss: 0.4094 - acc: 0.9761 - val_loss: 0.7103 - val_acc: 0.8696\nEpoch 188/1000\n - 0s - loss: 0.3715 - acc: 0.9957 - val_loss: 0.7038 - val_acc: 0.8609\nEpoch 189/1000\n - 0s - loss: 0.3700 - acc: 0.9935 - val_loss: 0.6915 - val_acc: 0.8696\nEpoch 190/1000\n - 0s - loss: 0.3652 - acc: 0.9935 - val_loss: 0.7014 - val_acc: 0.8609\n","name":"stdout"},{"output_type":"stream","text":"Epoch 191/1000\n - 0s - loss: 0.3727 - acc: 0.9891 - val_loss: 0.8469 - val_acc: 0.8174\nEpoch 192/1000\n - 0s - loss: 0.3596 - acc: 0.9935 - val_loss: 0.6995 - val_acc: 0.8783\nEpoch 193/1000\n - 0s - loss: 0.3754 - acc: 0.9804 - val_loss: 0.8544 - val_acc: 0.8261\nEpoch 194/1000\n - 0s - loss: 0.3598 - acc: 0.9935 - val_loss: 0.7228 - val_acc: 0.8609\nEpoch 195/1000\n - 0s - loss: 0.3601 - acc: 0.9913 - val_loss: 0.7641 - val_acc: 0.8435\nEpoch 196/1000\n - 0s - loss: 0.3493 - acc: 0.9957 - val_loss: 0.7691 - val_acc: 0.8435\nEpoch 197/1000\n - 0s - loss: 0.3412 - acc: 1.0000 - val_loss: 0.6992 - val_acc: 0.8609\nEpoch 198/1000\n - 0s - loss: 0.3610 - acc: 0.9804 - val_loss: 0.7147 - val_acc: 0.8609\nEpoch 199/1000\n - 0s - loss: 0.3377 - acc: 0.9957 - val_loss: 0.6971 - val_acc: 0.8522\nEpoch 200/1000\n - 0s - loss: 0.3283 - acc: 0.9935 - val_loss: 0.6963 - val_acc: 0.8696\nEpoch 201/1000\n - 0s - loss: 0.3484 - acc: 0.9891 - val_loss: 0.9773 - val_acc: 0.7826\nEpoch 202/1000\n - 0s - loss: 0.3635 - acc: 0.9826 - val_loss: 0.6987 - val_acc: 0.8522\nEpoch 203/1000\n - 0s - loss: 0.3335 - acc: 0.9957 - val_loss: 0.7879 - val_acc: 0.8348\nEpoch 204/1000\n - 0s - loss: 0.3485 - acc: 0.9826 - val_loss: 0.6919 - val_acc: 0.8522\nEpoch 205/1000\n - 0s - loss: 0.3327 - acc: 0.9957 - val_loss: 0.6754 - val_acc: 0.8696\nEpoch 206/1000\n - 0s - loss: 0.3244 - acc: 0.9935 - val_loss: 0.6904 - val_acc: 0.8609\nEpoch 207/1000\n - 0s - loss: 0.3213 - acc: 0.9978 - val_loss: 0.6672 - val_acc: 0.8783\nEpoch 208/1000\n - 0s - loss: 0.3168 - acc: 0.9957 - val_loss: 0.6847 - val_acc: 0.8609\nEpoch 209/1000\n - 0s - loss: 0.3120 - acc: 0.9935 - val_loss: 0.7593 - val_acc: 0.8348\nEpoch 210/1000\n - 0s - loss: 0.3180 - acc: 0.9978 - val_loss: 0.6761 - val_acc: 0.8609\nEpoch 211/1000\n - 0s - loss: 0.3217 - acc: 0.9848 - val_loss: 0.6869 - val_acc: 0.8609\nEpoch 212/1000\n - 0s - loss: 0.3103 - acc: 0.9978 - val_loss: 0.6725 - val_acc: 0.8522\nEpoch 213/1000\n - 0s - loss: 0.3084 - acc: 0.9957 - val_loss: 0.6677 - val_acc: 0.8609\nEpoch 214/1000\n - 0s - loss: 0.3212 - acc: 0.9870 - val_loss: 0.7267 - val_acc: 0.8435\nEpoch 215/1000\n - 0s - loss: 0.3122 - acc: 0.9870 - val_loss: 0.6535 - val_acc: 0.8609\nEpoch 216/1000\n - 0s - loss: 0.3168 - acc: 0.9913 - val_loss: 0.6687 - val_acc: 0.8609\nEpoch 217/1000\n - 0s - loss: 0.3016 - acc: 0.9935 - val_loss: 0.7263 - val_acc: 0.8435\nEpoch 218/1000\n - 0s - loss: 0.2994 - acc: 0.9935 - val_loss: 0.7725 - val_acc: 0.8261\nEpoch 219/1000\n - 0s - loss: 0.3072 - acc: 0.9891 - val_loss: 0.7259 - val_acc: 0.8348\nEpoch 220/1000\n - 0s - loss: 0.3097 - acc: 0.9826 - val_loss: 0.6526 - val_acc: 0.8609\nEpoch 221/1000\n - 0s - loss: 0.2885 - acc: 0.9978 - val_loss: 0.6467 - val_acc: 0.8609\nEpoch 222/1000\n - 0s - loss: 0.2865 - acc: 0.9978 - val_loss: 0.7069 - val_acc: 0.8522\nEpoch 223/1000\n - 0s - loss: 0.2864 - acc: 0.9935 - val_loss: 0.6417 - val_acc: 0.8522\nEpoch 224/1000\n - 0s - loss: 0.2994 - acc: 0.9870 - val_loss: 0.6471 - val_acc: 0.8609\nEpoch 225/1000\n - 0s - loss: 0.2991 - acc: 0.9891 - val_loss: 0.7145 - val_acc: 0.8435\nEpoch 226/1000\n - 0s - loss: 0.2896 - acc: 0.9935 - val_loss: 0.6457 - val_acc: 0.8609\nEpoch 227/1000\n - 0s - loss: 0.2777 - acc: 0.9935 - val_loss: 0.8374 - val_acc: 0.8087\nEpoch 228/1000\n - 0s - loss: 0.2997 - acc: 0.9870 - val_loss: 0.6468 - val_acc: 0.8522\nEpoch 229/1000\n - 0s - loss: 0.2984 - acc: 0.9935 - val_loss: 0.7954 - val_acc: 0.8174\nEpoch 230/1000\n - 0s - loss: 0.2834 - acc: 0.9913 - val_loss: 0.6416 - val_acc: 0.8609\nEpoch 231/1000\n - 0s - loss: 0.2975 - acc: 0.9848 - val_loss: 0.6353 - val_acc: 0.8783\nEpoch 232/1000\n - 0s - loss: 0.2756 - acc: 0.9978 - val_loss: 0.6463 - val_acc: 0.8522\nEpoch 233/1000\n - 0s - loss: 0.2785 - acc: 0.9935 - val_loss: 0.8560 - val_acc: 0.8174\nEpoch 234/1000\n - 0s - loss: 0.2894 - acc: 0.9891 - val_loss: 0.6438 - val_acc: 0.8609\nEpoch 235/1000\n - 0s - loss: 0.2801 - acc: 0.9935 - val_loss: 0.6385 - val_acc: 0.8609\nEpoch 236/1000\n - 0s - loss: 0.2736 - acc: 0.9978 - val_loss: 0.7525 - val_acc: 0.8435\nEpoch 237/1000\n - 0s - loss: 0.2751 - acc: 0.9957 - val_loss: 0.6817 - val_acc: 0.8609\nEpoch 238/1000\n - 0s - loss: 0.2577 - acc: 0.9978 - val_loss: 0.6363 - val_acc: 0.8609\nEpoch 239/1000\n - 0s - loss: 0.2546 - acc: 1.0000 - val_loss: 0.7650 - val_acc: 0.8348\nEpoch 240/1000\n - 0s - loss: 0.2633 - acc: 0.9978 - val_loss: 0.7046 - val_acc: 0.8435\nEpoch 241/1000\n - 0s - loss: 0.2605 - acc: 0.9957 - val_loss: 0.7196 - val_acc: 0.8522\nEpoch 242/1000\n - 0s - loss: 0.2681 - acc: 0.9891 - val_loss: 0.6414 - val_acc: 0.8522\nEpoch 243/1000\n - 0s - loss: 0.2813 - acc: 0.9826 - val_loss: 0.6664 - val_acc: 0.8522\nEpoch 244/1000\n - 0s - loss: 0.2491 - acc: 0.9978 - val_loss: 0.6201 - val_acc: 0.8696\nEpoch 245/1000\n - 0s - loss: 0.2625 - acc: 0.9957 - val_loss: 0.6872 - val_acc: 0.8522\nEpoch 246/1000\n - 0s - loss: 0.2465 - acc: 0.9957 - val_loss: 0.6117 - val_acc: 0.8696\nEpoch 247/1000\n - 0s - loss: 0.2450 - acc: 0.9978 - val_loss: 0.6233 - val_acc: 0.8696\nEpoch 248/1000\n - 0s - loss: 0.2605 - acc: 0.9913 - val_loss: 0.6312 - val_acc: 0.8522\nEpoch 249/1000\n - 0s - loss: 0.2490 - acc: 1.0000 - val_loss: 0.6201 - val_acc: 0.8696\nEpoch 250/1000\n - 0s - loss: 0.2449 - acc: 0.9978 - val_loss: 0.6146 - val_acc: 0.8609\nEpoch 251/1000\n - 0s - loss: 0.2405 - acc: 0.9935 - val_loss: 0.6665 - val_acc: 0.8609\nEpoch 252/1000\n - 0s - loss: 0.2367 - acc: 1.0000 - val_loss: 0.6167 - val_acc: 0.8696\nEpoch 253/1000\n - 0s - loss: 0.2374 - acc: 0.9978 - val_loss: 0.7098 - val_acc: 0.8348\nEpoch 254/1000\n - 0s - loss: 0.2431 - acc: 0.9957 - val_loss: 0.6143 - val_acc: 0.8522\nEpoch 255/1000\n - 0s - loss: 0.2349 - acc: 0.9978 - val_loss: 0.6310 - val_acc: 0.8609\nEpoch 256/1000\n - 0s - loss: 0.2536 - acc: 0.9935 - val_loss: 0.6884 - val_acc: 0.8435\nEpoch 257/1000\n - 0s - loss: 0.2405 - acc: 0.9957 - val_loss: 0.6378 - val_acc: 0.8522\nEpoch 258/1000\n - 0s - loss: 0.2307 - acc: 1.0000 - val_loss: 0.6244 - val_acc: 0.8522\nEpoch 259/1000\n - 0s - loss: 0.2316 - acc: 0.9978 - val_loss: 0.6234 - val_acc: 0.8609\nEpoch 260/1000\n - 0s - loss: 0.2291 - acc: 0.9957 - val_loss: 0.7414 - val_acc: 0.8348\nEpoch 261/1000\n - 0s - loss: 0.2328 - acc: 0.9957 - val_loss: 0.8498 - val_acc: 0.8000\nEpoch 262/1000\n - 0s - loss: 0.2461 - acc: 0.9913 - val_loss: 0.6142 - val_acc: 0.8609\nEpoch 263/1000\n - 0s - loss: 0.2274 - acc: 0.9957 - val_loss: 0.6720 - val_acc: 0.8609\nEpoch 264/1000\n - 0s - loss: 0.2316 - acc: 0.9935 - val_loss: 0.8312 - val_acc: 0.8087\nEpoch 265/1000\n - 0s - loss: 0.2459 - acc: 0.9891 - val_loss: 0.6042 - val_acc: 0.8609\nEpoch 266/1000\n - 0s - loss: 0.2297 - acc: 0.9957 - val_loss: 0.7026 - val_acc: 0.8261\nEpoch 267/1000\n - 0s - loss: 0.2285 - acc: 0.9935 - val_loss: 0.7373 - val_acc: 0.8261\nEpoch 268/1000\n - 0s - loss: 0.2260 - acc: 0.9935 - val_loss: 0.6028 - val_acc: 0.8696\nEpoch 269/1000\n - 0s - loss: 0.2377 - acc: 0.9913 - val_loss: 0.6420 - val_acc: 0.8609\nEpoch 270/1000\n - 0s - loss: 0.2185 - acc: 1.0000 - val_loss: 0.6009 - val_acc: 0.8609\nEpoch 271/1000\n - 0s - loss: 0.2120 - acc: 1.0000 - val_loss: 0.8115 - val_acc: 0.8087\nEpoch 272/1000\n - 0s - loss: 0.2186 - acc: 1.0000 - val_loss: 0.6473 - val_acc: 0.8609\nEpoch 273/1000\n - 0s - loss: 0.2295 - acc: 0.9913 - val_loss: 0.6043 - val_acc: 0.8609\nEpoch 274/1000\n - 0s - loss: 0.2431 - acc: 0.9870 - val_loss: 0.5914 - val_acc: 0.8609\nEpoch 275/1000\n - 0s - loss: 0.2216 - acc: 0.9957 - val_loss: 0.5934 - val_acc: 0.8696\nEpoch 276/1000\n - 0s - loss: 0.2100 - acc: 0.9957 - val_loss: 0.7039 - val_acc: 0.8435\nEpoch 277/1000\n - 0s - loss: 0.2333 - acc: 0.9848 - val_loss: 0.5922 - val_acc: 0.8696\nEpoch 278/1000\n - 0s - loss: 0.2220 - acc: 0.9935 - val_loss: 0.9165 - val_acc: 0.7826\nEpoch 279/1000\n - 0s - loss: 0.2353 - acc: 0.9826 - val_loss: 0.5847 - val_acc: 0.8696\nEpoch 280/1000\n - 0s - loss: 0.2135 - acc: 0.9913 - val_loss: 0.6364 - val_acc: 0.8609\nEpoch 281/1000\n - 0s - loss: 0.2061 - acc: 1.0000 - val_loss: 0.6081 - val_acc: 0.8522\nEpoch 282/1000\n - 0s - loss: 0.2160 - acc: 0.9957 - val_loss: 0.7243 - val_acc: 0.8174\nEpoch 283/1000\n - 0s - loss: 0.2047 - acc: 0.9957 - val_loss: 0.6911 - val_acc: 0.8348\nEpoch 284/1000\n - 0s - loss: 0.2043 - acc: 0.9957 - val_loss: 0.5954 - val_acc: 0.8609\nEpoch 285/1000\n","name":"stdout"},{"output_type":"stream","text":" - 0s - loss: 0.1963 - acc: 0.9978 - val_loss: 1.0009 - val_acc: 0.7652\nEpoch 286/1000\n - 0s - loss: 0.2619 - acc: 0.9739 - val_loss: 0.5941 - val_acc: 0.8696\nEpoch 287/1000\n - 0s - loss: 0.1995 - acc: 1.0000 - val_loss: 0.6336 - val_acc: 0.8609\nEpoch 288/1000\n - 0s - loss: 0.1984 - acc: 0.9978 - val_loss: 0.6681 - val_acc: 0.8435\nEpoch 289/1000\n - 0s - loss: 0.1969 - acc: 0.9978 - val_loss: 0.6114 - val_acc: 0.8522\nEpoch 290/1000\n - 0s - loss: 0.1968 - acc: 0.9978 - val_loss: 0.5955 - val_acc: 0.8522\nEpoch 291/1000\n - 0s - loss: 0.1914 - acc: 0.9978 - val_loss: 0.5998 - val_acc: 0.8609\nEpoch 292/1000\n - 0s - loss: 0.2014 - acc: 0.9935 - val_loss: 0.7125 - val_acc: 0.8261\nEpoch 293/1000\n - 0s - loss: 0.1965 - acc: 0.9978 - val_loss: 0.5843 - val_acc: 0.8696\nEpoch 294/1000\n - 0s - loss: 0.1922 - acc: 1.0000 - val_loss: 0.6155 - val_acc: 0.8522\nEpoch 295/1000\n - 0s - loss: 0.1922 - acc: 0.9978 - val_loss: 0.7514 - val_acc: 0.8261\nEpoch 296/1000\n - 0s - loss: 0.2007 - acc: 0.9957 - val_loss: 0.5824 - val_acc: 0.8609\nEpoch 297/1000\n - 0s - loss: 0.1863 - acc: 0.9978 - val_loss: 0.7639 - val_acc: 0.8087\nEpoch 298/1000\n - 0s - loss: 0.1924 - acc: 0.9913 - val_loss: 0.6948 - val_acc: 0.8000\nEpoch 299/1000\n - 0s - loss: 0.2198 - acc: 0.9826 - val_loss: 0.6720 - val_acc: 0.8435\nEpoch 300/1000\n - 0s - loss: 0.1994 - acc: 1.0000 - val_loss: 0.6568 - val_acc: 0.8435\nEpoch 301/1000\n - 0s - loss: 0.1920 - acc: 0.9978 - val_loss: 0.5864 - val_acc: 0.8609\nEpoch 302/1000\n - 0s - loss: 0.1872 - acc: 1.0000 - val_loss: 0.5900 - val_acc: 0.8609\nEpoch 303/1000\n - 0s - loss: 0.1929 - acc: 0.9957 - val_loss: 0.5772 - val_acc: 0.8609\nEpoch 304/1000\n - 0s - loss: 0.1809 - acc: 1.0000 - val_loss: 0.6054 - val_acc: 0.8522\nEpoch 305/1000\n - 0s - loss: 0.1853 - acc: 1.0000 - val_loss: 0.5862 - val_acc: 0.8609\nEpoch 306/1000\n - 0s - loss: 0.1789 - acc: 1.0000 - val_loss: 0.6052 - val_acc: 0.8609\nEpoch 307/1000\n - 0s - loss: 0.1891 - acc: 0.9957 - val_loss: 1.0121 - val_acc: 0.7739\nEpoch 308/1000\n - 0s - loss: 0.2226 - acc: 0.9761 - val_loss: 0.5748 - val_acc: 0.8609\nEpoch 309/1000\n - 0s - loss: 0.2408 - acc: 0.9652 - val_loss: 0.5801 - val_acc: 0.8609\nEpoch 310/1000\n - 0s - loss: 0.1842 - acc: 0.9978 - val_loss: 0.5783 - val_acc: 0.8696\nEpoch 311/1000\n - 0s - loss: 0.1827 - acc: 1.0000 - val_loss: 0.5801 - val_acc: 0.8609\nEpoch 312/1000\n - 0s - loss: 0.1844 - acc: 0.9957 - val_loss: 0.7170 - val_acc: 0.8348\nEpoch 313/1000\n - 0s - loss: 0.1909 - acc: 0.9978 - val_loss: 0.5723 - val_acc: 0.8609\nEpoch 314/1000\n - 0s - loss: 0.2005 - acc: 0.9913 - val_loss: 0.5968 - val_acc: 0.8522\nEpoch 315/1000\n - 0s - loss: 0.1745 - acc: 0.9978 - val_loss: 0.6257 - val_acc: 0.8609\nEpoch 316/1000\n - 0s - loss: 0.1769 - acc: 0.9978 - val_loss: 0.6387 - val_acc: 0.8522\nEpoch 317/1000\n - 0s - loss: 0.1716 - acc: 1.0000 - val_loss: 0.5789 - val_acc: 0.8609\nEpoch 318/1000\n - 0s - loss: 0.1725 - acc: 0.9978 - val_loss: 0.5940 - val_acc: 0.8696\nEpoch 319/1000\n - 0s - loss: 0.1768 - acc: 1.0000 - val_loss: 0.6422 - val_acc: 0.8435\nEpoch 320/1000\n - 0s - loss: 0.1700 - acc: 1.0000 - val_loss: 0.5995 - val_acc: 0.8609\nEpoch 321/1000\n - 0s - loss: 0.1708 - acc: 0.9978 - val_loss: 0.6063 - val_acc: 0.8609\nEpoch 322/1000\n - 0s - loss: 0.1723 - acc: 0.9978 - val_loss: 0.5715 - val_acc: 0.8696\nEpoch 323/1000\n - 0s - loss: 0.1670 - acc: 1.0000 - val_loss: 0.5912 - val_acc: 0.8609\nEpoch 324/1000\n - 0s - loss: 0.1798 - acc: 0.9957 - val_loss: 0.5731 - val_acc: 0.8609\nEpoch 325/1000\n - 0s - loss: 0.1719 - acc: 1.0000 - val_loss: 0.6158 - val_acc: 0.8609\nEpoch 326/1000\n - 0s - loss: 0.1703 - acc: 1.0000 - val_loss: 0.5856 - val_acc: 0.8522\nEpoch 327/1000\n - 0s - loss: 0.1670 - acc: 1.0000 - val_loss: 0.6105 - val_acc: 0.8609\nEpoch 328/1000\n - 0s - loss: 0.1693 - acc: 0.9978 - val_loss: 0.6179 - val_acc: 0.8609\nEpoch 329/1000\n - 0s - loss: 0.1630 - acc: 1.0000 - val_loss: 0.5574 - val_acc: 0.8609\nEpoch 330/1000\n - 0s - loss: 0.1629 - acc: 0.9978 - val_loss: 0.6592 - val_acc: 0.8522\nEpoch 331/1000\n - 0s - loss: 0.1641 - acc: 0.9978 - val_loss: 0.5441 - val_acc: 0.8609\nEpoch 332/1000\n - 0s - loss: 0.1663 - acc: 0.9978 - val_loss: 0.7914 - val_acc: 0.8087\nEpoch 333/1000\n - 0s - loss: 0.1756 - acc: 0.9891 - val_loss: 0.6262 - val_acc: 0.8522\nEpoch 334/1000\n - 0s - loss: 0.1656 - acc: 0.9978 - val_loss: 0.5562 - val_acc: 0.8609\nEpoch 335/1000\n - 0s - loss: 0.1597 - acc: 1.0000 - val_loss: 0.5807 - val_acc: 0.8522\nEpoch 336/1000\n - 0s - loss: 0.1602 - acc: 1.0000 - val_loss: 0.8940 - val_acc: 0.7913\nEpoch 337/1000\n - 0s - loss: 0.1787 - acc: 0.9913 - val_loss: 0.5648 - val_acc: 0.8522\nEpoch 338/1000\n - 0s - loss: 0.1601 - acc: 1.0000 - val_loss: 0.6011 - val_acc: 0.8609\nEpoch 339/1000\n - 0s - loss: 0.1579 - acc: 1.0000 - val_loss: 0.5586 - val_acc: 0.8609\nEpoch 340/1000\n - 0s - loss: 0.1574 - acc: 1.0000 - val_loss: 0.5812 - val_acc: 0.8522\nEpoch 341/1000\n - 0s - loss: 0.1767 - acc: 0.9935 - val_loss: 0.6390 - val_acc: 0.8435\nEpoch 342/1000\n - 0s - loss: 0.1686 - acc: 0.9978 - val_loss: 0.7078 - val_acc: 0.8348\nEpoch 343/1000\n - 0s - loss: 0.1768 - acc: 0.9957 - val_loss: 0.5645 - val_acc: 0.8696\nEpoch 344/1000\n - 0s - loss: 0.1586 - acc: 0.9978 - val_loss: 0.5563 - val_acc: 0.8522\nEpoch 345/1000\n - 0s - loss: 0.1559 - acc: 1.0000 - val_loss: 0.5882 - val_acc: 0.8522\nEpoch 346/1000\n - 0s - loss: 0.1507 - acc: 1.0000 - val_loss: 0.5484 - val_acc: 0.8609\nEpoch 347/1000\n - 0s - loss: 0.1585 - acc: 0.9957 - val_loss: 0.5560 - val_acc: 0.8609\nEpoch 348/1000\n - 0s - loss: 0.1628 - acc: 0.9978 - val_loss: 0.8053 - val_acc: 0.8087\nEpoch 349/1000\n - 0s - loss: 0.1729 - acc: 0.9935 - val_loss: 0.6070 - val_acc: 0.8435\nEpoch 350/1000\n - 0s - loss: 0.1647 - acc: 0.9978 - val_loss: 0.5900 - val_acc: 0.8522\nEpoch 351/1000\n - 0s - loss: 0.1544 - acc: 0.9978 - val_loss: 0.6300 - val_acc: 0.8435\nEpoch 352/1000\n - 0s - loss: 0.1605 - acc: 1.0000 - val_loss: 0.6043 - val_acc: 0.8609\nEpoch 353/1000\n - 0s - loss: 0.1486 - acc: 1.0000 - val_loss: 0.5913 - val_acc: 0.8522\nEpoch 354/1000\n - 0s - loss: 0.1456 - acc: 1.0000 - val_loss: 0.6001 - val_acc: 0.8522\nEpoch 355/1000\n - 0s - loss: 0.1556 - acc: 0.9978 - val_loss: 0.7256 - val_acc: 0.8174\nEpoch 356/1000\n - 0s - loss: 0.1536 - acc: 0.9978 - val_loss: 0.5953 - val_acc: 0.8522\nEpoch 357/1000\n - 0s - loss: 0.1556 - acc: 1.0000 - val_loss: 0.6226 - val_acc: 0.8261\nEpoch 358/1000\n - 0s - loss: 0.1658 - acc: 0.9957 - val_loss: 0.5948 - val_acc: 0.8609\nEpoch 359/1000\n - 0s - loss: 0.1507 - acc: 1.0000 - val_loss: 0.6308 - val_acc: 0.8435\nEpoch 360/1000\n - 0s - loss: 0.1446 - acc: 0.9978 - val_loss: 0.6089 - val_acc: 0.8609\nEpoch 361/1000\n - 0s - loss: 0.1495 - acc: 0.9978 - val_loss: 0.6042 - val_acc: 0.8522\nEpoch 362/1000\n - 0s - loss: 0.1511 - acc: 0.9957 - val_loss: 0.7059 - val_acc: 0.8435\nEpoch 363/1000\n - 0s - loss: 0.1466 - acc: 1.0000 - val_loss: 0.6085 - val_acc: 0.8522\nEpoch 364/1000\n - 0s - loss: 0.1491 - acc: 0.9978 - val_loss: 0.9125 - val_acc: 0.7826\nEpoch 365/1000\n - 0s - loss: 0.1565 - acc: 0.9935 - val_loss: 0.5575 - val_acc: 0.8696\nEpoch 366/1000\n - 0s - loss: 0.1497 - acc: 1.0000 - val_loss: 0.6281 - val_acc: 0.8522\nEpoch 367/1000\n - 0s - loss: 0.1464 - acc: 1.0000 - val_loss: 0.5581 - val_acc: 0.8696\nEpoch 368/1000\n - 0s - loss: 0.1507 - acc: 0.9978 - val_loss: 0.6700 - val_acc: 0.8435\nEpoch 369/1000\n - 0s - loss: 0.1447 - acc: 0.9978 - val_loss: 0.6388 - val_acc: 0.8435\nEpoch 370/1000\n - 0s - loss: 0.1405 - acc: 1.0000 - val_loss: 0.5505 - val_acc: 0.8609\nEpoch 371/1000\n - 0s - loss: 0.1384 - acc: 1.0000 - val_loss: 0.6051 - val_acc: 0.8348\nEpoch 372/1000\n - 0s - loss: 0.1440 - acc: 1.0000 - val_loss: 0.5630 - val_acc: 0.8609\nEpoch 373/1000\n - 0s - loss: 0.1393 - acc: 1.0000 - val_loss: 0.5603 - val_acc: 0.8609\nEpoch 374/1000\n - 0s - loss: 0.1360 - acc: 1.0000 - val_loss: 0.5481 - val_acc: 0.8609\nEpoch 375/1000\n - 0s - loss: 0.1421 - acc: 1.0000 - val_loss: 0.6238 - val_acc: 0.8435\nEpoch 376/1000\n - 0s - loss: 0.1419 - acc: 1.0000 - val_loss: 0.5617 - val_acc: 0.8696\nEpoch 377/1000\n - 0s - loss: 0.1371 - acc: 1.0000 - val_loss: 0.5518 - val_acc: 0.8696\nEpoch 378/1000\n - 0s - loss: 0.1356 - acc: 1.0000 - val_loss: 0.5761 - val_acc: 0.8522\nEpoch 379/1000\n - 0s - loss: 0.1423 - acc: 1.0000 - val_loss: 0.5600 - val_acc: 0.8696\n","name":"stdout"},{"output_type":"stream","text":"Epoch 380/1000\n - 0s - loss: 0.1346 - acc: 1.0000 - val_loss: 0.5568 - val_acc: 0.8783\nEpoch 381/1000\n - 0s - loss: 0.1392 - acc: 1.0000 - val_loss: 0.5914 - val_acc: 0.8522\nEpoch 382/1000\n - 0s - loss: 0.1325 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 0.8522\nEpoch 383/1000\n - 0s - loss: 0.1323 - acc: 1.0000 - val_loss: 0.6526 - val_acc: 0.8435\nEpoch 384/1000\n - 0s - loss: 0.1407 - acc: 0.9957 - val_loss: 0.5722 - val_acc: 0.8609\nEpoch 385/1000\n - 0s - loss: 0.1402 - acc: 1.0000 - val_loss: 0.5577 - val_acc: 0.8783\nEpoch 386/1000\n - 0s - loss: 0.1338 - acc: 0.9978 - val_loss: 0.5957 - val_acc: 0.8609\nEpoch 387/1000\n - 0s - loss: 0.1329 - acc: 1.0000 - val_loss: 0.5728 - val_acc: 0.8609\nEpoch 388/1000\n - 0s - loss: 0.1304 - acc: 1.0000 - val_loss: 0.6065 - val_acc: 0.8522\nEpoch 389/1000\n - 0s - loss: 0.1340 - acc: 1.0000 - val_loss: 0.8119 - val_acc: 0.8087\nEpoch 390/1000\n - 0s - loss: 0.1418 - acc: 0.9978 - val_loss: 0.6837 - val_acc: 0.8435\nEpoch 391/1000\n - 0s - loss: 0.1450 - acc: 0.9935 - val_loss: 0.5628 - val_acc: 0.8609\nEpoch 392/1000\n - 0s - loss: 0.1357 - acc: 1.0000 - val_loss: 0.8198 - val_acc: 0.8087\nEpoch 393/1000\n - 0s - loss: 0.1566 - acc: 0.9848 - val_loss: 0.5607 - val_acc: 0.8609\nEpoch 394/1000\n - 0s - loss: 0.1409 - acc: 1.0000 - val_loss: 0.5569 - val_acc: 0.8696\nEpoch 395/1000\n - 0s - loss: 0.1287 - acc: 1.0000 - val_loss: 0.5780 - val_acc: 0.8522\nEpoch 396/1000\n - 0s - loss: 0.1299 - acc: 1.0000 - val_loss: 0.5626 - val_acc: 0.8522\nEpoch 397/1000\n - 0s - loss: 0.1262 - acc: 1.0000 - val_loss: 0.5555 - val_acc: 0.8696\nEpoch 398/1000\n - 0s - loss: 0.1337 - acc: 0.9935 - val_loss: 0.5939 - val_acc: 0.8435\nEpoch 399/1000\n - 0s - loss: 0.1390 - acc: 0.9978 - val_loss: 0.5748 - val_acc: 0.8522\nEpoch 400/1000\n - 0s - loss: 0.1330 - acc: 1.0000 - val_loss: 0.5937 - val_acc: 0.8522\nEpoch 401/1000\n - 0s - loss: 0.1259 - acc: 1.0000 - val_loss: 0.5952 - val_acc: 0.8522\nEpoch 402/1000\n - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 0.6102 - val_acc: 0.8522\nEpoch 403/1000\n - 0s - loss: 0.1257 - acc: 1.0000 - val_loss: 0.6278 - val_acc: 0.8435\nEpoch 404/1000\n - 0s - loss: 0.1301 - acc: 0.9978 - val_loss: 0.5522 - val_acc: 0.8696\nEpoch 405/1000\n - 0s - loss: 0.1253 - acc: 1.0000 - val_loss: 0.5578 - val_acc: 0.8522\nEpoch 406/1000\n - 0s - loss: 0.1291 - acc: 1.0000 - val_loss: 0.6708 - val_acc: 0.8348\nEpoch 407/1000\n - 0s - loss: 0.1244 - acc: 1.0000 - val_loss: 0.6102 - val_acc: 0.8435\nEpoch 408/1000\n - 0s - loss: 0.1231 - acc: 1.0000 - val_loss: 0.6386 - val_acc: 0.8435\nEpoch 409/1000\n - 0s - loss: 0.1243 - acc: 1.0000 - val_loss: 0.5675 - val_acc: 0.8522\nEpoch 410/1000\n - 0s - loss: 0.1226 - acc: 1.0000 - val_loss: 0.5655 - val_acc: 0.8522\nEpoch 411/1000\n - 0s - loss: 0.1218 - acc: 1.0000 - val_loss: 0.6410 - val_acc: 0.8435\nEpoch 412/1000\n - 0s - loss: 0.1266 - acc: 1.0000 - val_loss: 0.5556 - val_acc: 0.8609\nEpoch 413/1000\n - 0s - loss: 0.1373 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.8522\nEpoch 414/1000\n - 0s - loss: 0.1381 - acc: 0.9957 - val_loss: 0.5520 - val_acc: 0.8696\nEpoch 415/1000\n - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 0.5485 - val_acc: 0.8609\nEpoch 416/1000\n - 0s - loss: 0.1180 - acc: 1.0000 - val_loss: 0.5669 - val_acc: 0.8609\nEpoch 417/1000\n - 0s - loss: 0.1201 - acc: 1.0000 - val_loss: 0.5533 - val_acc: 0.8609\nEpoch 418/1000\n - 0s - loss: 0.1205 - acc: 0.9978 - val_loss: 0.7589 - val_acc: 0.8261\nEpoch 419/1000\n - 0s - loss: 0.1241 - acc: 1.0000 - val_loss: 0.5486 - val_acc: 0.8609\nEpoch 420/1000\n - 0s - loss: 0.1162 - acc: 1.0000 - val_loss: 0.5456 - val_acc: 0.8696\nEpoch 421/1000\n - 0s - loss: 0.1273 - acc: 0.9957 - val_loss: 0.5535 - val_acc: 0.8522\nEpoch 422/1000\n - 0s - loss: 0.1357 - acc: 0.9957 - val_loss: 0.6421 - val_acc: 0.8435\nEpoch 423/1000\n - 0s - loss: 0.1166 - acc: 1.0000 - val_loss: 0.5519 - val_acc: 0.8696\nEpoch 424/1000\n - 0s - loss: 0.1264 - acc: 1.0000 - val_loss: 0.5479 - val_acc: 0.8609\nEpoch 425/1000\n - 0s - loss: 0.1212 - acc: 1.0000 - val_loss: 0.5399 - val_acc: 0.8696\nEpoch 426/1000\n - 0s - loss: 0.1152 - acc: 1.0000 - val_loss: 0.6750 - val_acc: 0.8435\nEpoch 427/1000\n - 0s - loss: 0.1353 - acc: 0.9935 - val_loss: 0.5390 - val_acc: 0.8609\nEpoch 428/1000\n - 0s - loss: 0.1209 - acc: 0.9978 - val_loss: 0.5407 - val_acc: 0.8696\nEpoch 429/1000\n - 0s - loss: 0.1201 - acc: 0.9978 - val_loss: 0.6831 - val_acc: 0.8348\nEpoch 430/1000\n - 0s - loss: 0.1204 - acc: 1.0000 - val_loss: 0.5718 - val_acc: 0.8522\nEpoch 431/1000\n - 0s - loss: 0.1195 - acc: 0.9978 - val_loss: 0.5777 - val_acc: 0.8522\nEpoch 432/1000\n - 0s - loss: 0.1191 - acc: 1.0000 - val_loss: 0.7244 - val_acc: 0.8261\nEpoch 433/1000\n - 0s - loss: 0.1280 - acc: 0.9978 - val_loss: 0.6004 - val_acc: 0.8522\nEpoch 434/1000\n - 0s - loss: 0.1178 - acc: 1.0000 - val_loss: 0.5789 - val_acc: 0.8522\nEpoch 435/1000\n - 0s - loss: 0.1255 - acc: 0.9978 - val_loss: 0.5819 - val_acc: 0.8348\nEpoch 436/1000\n - 0s - loss: 0.1217 - acc: 0.9978 - val_loss: 0.5508 - val_acc: 0.8609\nEpoch 437/1000\n - 0s - loss: 0.1133 - acc: 1.0000 - val_loss: 0.6956 - val_acc: 0.8435\nEpoch 438/1000\n - 0s - loss: 0.1146 - acc: 1.0000 - val_loss: 0.7200 - val_acc: 0.8348\nEpoch 439/1000\n - 0s - loss: 0.1278 - acc: 0.9978 - val_loss: 0.5679 - val_acc: 0.8522\nEpoch 440/1000\n - 0s - loss: 0.1142 - acc: 1.0000 - val_loss: 0.5306 - val_acc: 0.8696\nEpoch 441/1000\n - 0s - loss: 0.1122 - acc: 1.0000 - val_loss: 0.5731 - val_acc: 0.8522\nEpoch 442/1000\n - 0s - loss: 0.1215 - acc: 1.0000 - val_loss: 0.7292 - val_acc: 0.8261\nEpoch 443/1000\n - 0s - loss: 0.1222 - acc: 0.9957 - val_loss: 0.5318 - val_acc: 0.8522\nEpoch 444/1000\n - 0s - loss: 0.1176 - acc: 1.0000 - val_loss: 0.6369 - val_acc: 0.8435\nEpoch 445/1000\n - 0s - loss: 0.1190 - acc: 0.9978 - val_loss: 0.5544 - val_acc: 0.8696\nEpoch 446/1000\n - 0s - loss: 0.1210 - acc: 0.9978 - val_loss: 0.5444 - val_acc: 0.8609\nEpoch 447/1000\n - 0s - loss: 0.1222 - acc: 0.9978 - val_loss: 0.6487 - val_acc: 0.8435\nEpoch 448/1000\n - 0s - loss: 0.1147 - acc: 1.0000 - val_loss: 0.7062 - val_acc: 0.8348\nEpoch 449/1000\n - 0s - loss: 0.1174 - acc: 0.9957 - val_loss: 0.6424 - val_acc: 0.8522\nEpoch 450/1000\n - 0s - loss: 0.1160 - acc: 1.0000 - val_loss: 0.5360 - val_acc: 0.8696\nEpoch 451/1000\n - 0s - loss: 0.1099 - acc: 0.9978 - val_loss: 0.6437 - val_acc: 0.8348\nEpoch 452/1000\n - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.5899 - val_acc: 0.8522\nEpoch 453/1000\n - 0s - loss: 0.1112 - acc: 1.0000 - val_loss: 0.6106 - val_acc: 0.8435\nEpoch 454/1000\n - 0s - loss: 0.1184 - acc: 1.0000 - val_loss: 0.6286 - val_acc: 0.8609\nEpoch 455/1000\n - 0s - loss: 0.1124 - acc: 1.0000 - val_loss: 0.5647 - val_acc: 0.8522\nEpoch 456/1000\n - 0s - loss: 0.1117 - acc: 1.0000 - val_loss: 0.8812 - val_acc: 0.7826\nEpoch 457/1000\n - 0s - loss: 0.1345 - acc: 0.9870 - val_loss: 0.5442 - val_acc: 0.8696\nEpoch 458/1000\n - 0s - loss: 0.1081 - acc: 1.0000 - val_loss: 0.5616 - val_acc: 0.8783\nEpoch 459/1000\n - 0s - loss: 0.1143 - acc: 1.0000 - val_loss: 0.5422 - val_acc: 0.8522\nEpoch 460/1000\n - 0s - loss: 0.1122 - acc: 1.0000 - val_loss: 0.5721 - val_acc: 0.8609\nEpoch 461/1000\n - 0s - loss: 0.1234 - acc: 0.9978 - val_loss: 0.5540 - val_acc: 0.8522\nEpoch 462/1000\n - 0s - loss: 0.1145 - acc: 1.0000 - val_loss: 0.5494 - val_acc: 0.8522\nEpoch 463/1000\n - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 0.6323 - val_acc: 0.8522\nEpoch 464/1000\n - 0s - loss: 0.1050 - acc: 1.0000 - val_loss: 0.5841 - val_acc: 0.8522\nEpoch 465/1000\n - 0s - loss: 0.1074 - acc: 1.0000 - val_loss: 0.6111 - val_acc: 0.8522\nEpoch 466/1000\n - 0s - loss: 0.1059 - acc: 1.0000 - val_loss: 0.5813 - val_acc: 0.8609\nEpoch 467/1000\n - 0s - loss: 0.1086 - acc: 0.9978 - val_loss: 0.5643 - val_acc: 0.8609\nEpoch 468/1000\n - 0s - loss: 0.1023 - acc: 1.0000 - val_loss: 0.8299 - val_acc: 0.7826\nEpoch 469/1000\n - 0s - loss: 0.1176 - acc: 0.9957 - val_loss: 0.5546 - val_acc: 0.8609\nEpoch 470/1000\n - 0s - loss: 0.1073 - acc: 1.0000 - val_loss: 0.5527 - val_acc: 0.8522\nEpoch 471/1000\n - 0s - loss: 0.1075 - acc: 1.0000 - val_loss: 0.5461 - val_acc: 0.8609\nEpoch 472/1000\n - 0s - loss: 0.1080 - acc: 1.0000 - val_loss: 0.5260 - val_acc: 0.8522\nEpoch 473/1000\n - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.5593 - val_acc: 0.8522\nEpoch 474/1000\n","name":"stdout"},{"output_type":"stream","text":" - 0s - loss: 0.1087 - acc: 1.0000 - val_loss: 0.5480 - val_acc: 0.8783\nEpoch 475/1000\n - 0s - loss: 0.1127 - acc: 0.9978 - val_loss: 0.5348 - val_acc: 0.8609\nEpoch 476/1000\n - 0s - loss: 0.1030 - acc: 1.0000 - val_loss: 0.5271 - val_acc: 0.8609\nEpoch 477/1000\n - 0s - loss: 0.0999 - acc: 1.0000 - val_loss: 0.5839 - val_acc: 0.8609\nEpoch 478/1000\n - 0s - loss: 0.1038 - acc: 1.0000 - val_loss: 0.5244 - val_acc: 0.8522\nEpoch 479/1000\n - 0s - loss: 0.1032 - acc: 1.0000 - val_loss: 0.5358 - val_acc: 0.8522\nEpoch 480/1000\n - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 0.5433 - val_acc: 0.8522\nEpoch 481/1000\n - 0s - loss: 0.0997 - acc: 1.0000 - val_loss: 0.5351 - val_acc: 0.8522\nEpoch 482/1000\n - 0s - loss: 0.1030 - acc: 1.0000 - val_loss: 0.5806 - val_acc: 0.8609\nEpoch 483/1000\n - 0s - loss: 0.1036 - acc: 1.0000 - val_loss: 0.5952 - val_acc: 0.8000\nEpoch 484/1000\n - 0s - loss: 0.1217 - acc: 0.9978 - val_loss: 0.7906 - val_acc: 0.8000\nEpoch 485/1000\n - 0s - loss: 0.1146 - acc: 0.9978 - val_loss: 0.5671 - val_acc: 0.8783\nEpoch 486/1000\n - 0s - loss: 0.1121 - acc: 0.9978 - val_loss: 0.5414 - val_acc: 0.8783\nEpoch 487/1000\n - 0s - loss: 0.1018 - acc: 1.0000 - val_loss: 0.5548 - val_acc: 0.8609\nEpoch 488/1000\n - 0s - loss: 0.1044 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.8609\nEpoch 489/1000\n - 0s - loss: 0.1093 - acc: 0.9978 - val_loss: 0.5439 - val_acc: 0.8609\nEpoch 490/1000\n - 0s - loss: 0.1006 - acc: 1.0000 - val_loss: 0.7266 - val_acc: 0.8261\nEpoch 491/1000\n - 0s - loss: 0.1063 - acc: 1.0000 - val_loss: 0.5397 - val_acc: 0.8609\nEpoch 492/1000\n - 0s - loss: 0.0997 - acc: 1.0000 - val_loss: 0.6866 - val_acc: 0.8435\nEpoch 493/1000\n - 0s - loss: 0.0984 - acc: 1.0000 - val_loss: 0.5352 - val_acc: 0.8522\nEpoch 494/1000\n - 0s - loss: 0.1019 - acc: 1.0000 - val_loss: 0.7273 - val_acc: 0.8261\nEpoch 495/1000\n - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.5704 - val_acc: 0.8522\nEpoch 496/1000\n - 0s - loss: 0.1048 - acc: 1.0000 - val_loss: 0.5391 - val_acc: 0.8609\nEpoch 497/1000\n - 0s - loss: 0.0986 - acc: 0.9978 - val_loss: 0.5822 - val_acc: 0.8522\nEpoch 498/1000\n - 0s - loss: 0.1048 - acc: 0.9978 - val_loss: 0.5334 - val_acc: 0.8522\nEpoch 499/1000\n - 0s - loss: 0.0979 - acc: 1.0000 - val_loss: 0.5701 - val_acc: 0.8609\nEpoch 500/1000\n - 0s - loss: 0.1077 - acc: 1.0000 - val_loss: 0.5376 - val_acc: 0.8783\nEpoch 501/1000\n - 0s - loss: 0.1002 - acc: 1.0000 - val_loss: 0.5955 - val_acc: 0.8522\nEpoch 502/1000\n - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.5939 - val_acc: 0.8174\nEpoch 503/1000\n - 0s - loss: 0.1062 - acc: 1.0000 - val_loss: 0.5332 - val_acc: 0.8696\nEpoch 504/1000\n - 0s - loss: 0.1028 - acc: 1.0000 - val_loss: 0.8040 - val_acc: 0.8174\nEpoch 505/1000\n - 0s - loss: 0.1127 - acc: 1.0000 - val_loss: 0.5459 - val_acc: 0.8696\nEpoch 506/1000\n - 0s - loss: 0.1121 - acc: 0.9978 - val_loss: 0.5624 - val_acc: 0.8696\nEpoch 507/1000\n - 0s - loss: 0.1068 - acc: 0.9978 - val_loss: 0.7582 - val_acc: 0.8174\nEpoch 508/1000\n - 0s - loss: 0.1041 - acc: 1.0000 - val_loss: 0.5602 - val_acc: 0.8696\nEpoch 509/1000\n - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.6214 - val_acc: 0.8000\nEpoch 510/1000\n - 0s - loss: 0.1122 - acc: 0.9957 - val_loss: 0.5465 - val_acc: 0.8522\nEpoch 511/1000\n - 0s - loss: 0.0965 - acc: 1.0000 - val_loss: 0.6897 - val_acc: 0.8435\nEpoch 512/1000\n - 0s - loss: 0.0997 - acc: 0.9978 - val_loss: 0.5432 - val_acc: 0.8609\nEpoch 513/1000\n - 0s - loss: 0.0941 - acc: 1.0000 - val_loss: 0.5774 - val_acc: 0.8522\nEpoch 514/1000\n - 0s - loss: 0.0932 - acc: 1.0000 - val_loss: 0.6466 - val_acc: 0.8435\nEpoch 515/1000\n - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 0.8440 - val_acc: 0.8000\nEpoch 516/1000\n - 0s - loss: 0.1181 - acc: 0.9935 - val_loss: 0.6678 - val_acc: 0.7826\nEpoch 517/1000\n - 0s - loss: 0.1292 - acc: 0.9913 - val_loss: 0.6644 - val_acc: 0.8435\nEpoch 518/1000\n - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 0.5362 - val_acc: 0.8696\nEpoch 519/1000\n - 0s - loss: 0.0954 - acc: 1.0000 - val_loss: 0.5526 - val_acc: 0.8522\nEpoch 520/1000\n - 0s - loss: 0.0909 - acc: 1.0000 - val_loss: 0.7666 - val_acc: 0.8174\nEpoch 521/1000\n - 0s - loss: 0.0970 - acc: 1.0000 - val_loss: 0.7531 - val_acc: 0.8174\nEpoch 522/1000\n - 0s - loss: 0.1009 - acc: 0.9957 - val_loss: 0.5443 - val_acc: 0.8522\nEpoch 523/1000\n - 0s - loss: 0.0936 - acc: 1.0000 - val_loss: 0.6387 - val_acc: 0.8609\nEpoch 524/1000\n - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.5386 - val_acc: 0.8609\nEpoch 525/1000\n - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 0.8522\nEpoch 526/1000\n - 0s - loss: 0.0998 - acc: 1.0000 - val_loss: 0.5492 - val_acc: 0.8522\nEpoch 527/1000\n - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.5736 - val_acc: 0.8522\nEpoch 528/1000\n - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 0.5447 - val_acc: 0.8783\nEpoch 529/1000\n - 0s - loss: 0.0966 - acc: 1.0000 - val_loss: 0.5525 - val_acc: 0.8696\nEpoch 530/1000\n - 0s - loss: 0.0966 - acc: 1.0000 - val_loss: 0.5835 - val_acc: 0.8435\nEpoch 531/1000\n - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 0.5833 - val_acc: 0.8609\nEpoch 532/1000\n - 0s - loss: 0.0963 - acc: 0.9978 - val_loss: 0.5428 - val_acc: 0.8522\nEpoch 533/1000\n - 0s - loss: 0.0992 - acc: 0.9978 - val_loss: 0.5518 - val_acc: 0.8522\nEpoch 534/1000\n - 0s - loss: 0.0996 - acc: 1.0000 - val_loss: 0.6040 - val_acc: 0.8348\nEpoch 535/1000\n - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.5457 - val_acc: 0.8609\nEpoch 536/1000\n - 0s - loss: 0.0898 - acc: 1.0000 - val_loss: 0.6630 - val_acc: 0.8435\nEpoch 537/1000\n - 0s - loss: 0.0916 - acc: 1.0000 - val_loss: 0.5713 - val_acc: 0.8522\nEpoch 538/1000\n - 0s - loss: 0.0892 - acc: 1.0000 - val_loss: 0.6836 - val_acc: 0.8435\nEpoch 539/1000\n - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 0.5425 - val_acc: 0.8609\nEpoch 540/1000\n - 0s - loss: 0.0900 - acc: 1.0000 - val_loss: 0.5839 - val_acc: 0.8609\nEpoch 541/1000\n - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.5694 - val_acc: 0.8522\nEpoch 542/1000\n - 0s - loss: 0.1049 - acc: 0.9935 - val_loss: 0.5704 - val_acc: 0.8696\nEpoch 543/1000\n - 0s - loss: 0.0958 - acc: 1.0000 - val_loss: 0.5777 - val_acc: 0.8522\nEpoch 544/1000\n - 0s - loss: 0.0885 - acc: 1.0000 - val_loss: 0.5501 - val_acc: 0.8609\nEpoch 545/1000\n - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 0.6517 - val_acc: 0.8435\nEpoch 546/1000\n - 0s - loss: 0.0901 - acc: 1.0000 - val_loss: 0.5696 - val_acc: 0.8609\nEpoch 547/1000\n - 0s - loss: 0.0986 - acc: 0.9957 - val_loss: 0.5320 - val_acc: 0.8609\nEpoch 548/1000\n - 0s - loss: 0.0933 - acc: 1.0000 - val_loss: 0.6048 - val_acc: 0.8609\nEpoch 549/1000\n - 0s - loss: 0.1026 - acc: 0.9957 - val_loss: 0.7480 - val_acc: 0.8174\nEpoch 550/1000\n - 0s - loss: 0.0946 - acc: 0.9978 - val_loss: 0.5423 - val_acc: 0.8609\nEpoch 551/1000\n - 0s - loss: 0.0858 - acc: 1.0000 - val_loss: 0.9372 - val_acc: 0.7826\nEpoch 552/1000\n - 0s - loss: 0.1271 - acc: 0.9848 - val_loss: 0.5330 - val_acc: 0.8522\nEpoch 553/1000\n - 0s - loss: 0.0960 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 0.8696\nEpoch 554/1000\n - 0s - loss: 0.0882 - acc: 1.0000 - val_loss: 0.7013 - val_acc: 0.8348\nEpoch 555/1000\n - 0s - loss: 0.0986 - acc: 0.9978 - val_loss: 0.6394 - val_acc: 0.8435\nEpoch 556/1000\n - 0s - loss: 0.0998 - acc: 0.9978 - val_loss: 0.6647 - val_acc: 0.8000\nEpoch 557/1000\n - 0s - loss: 0.0994 - acc: 1.0000 - val_loss: 0.5534 - val_acc: 0.8522\nEpoch 558/1000\n - 0s - loss: 0.0886 - acc: 1.0000 - val_loss: 0.5875 - val_acc: 0.8609\nEpoch 559/1000\n - 0s - loss: 0.1144 - acc: 0.9870 - val_loss: 0.7273 - val_acc: 0.8348\nEpoch 560/1000\n - 0s - loss: 0.0967 - acc: 1.0000 - val_loss: 0.7030 - val_acc: 0.8348\nEpoch 561/1000\n - 0s - loss: 0.0996 - acc: 0.9978 - val_loss: 0.5688 - val_acc: 0.8522\nEpoch 562/1000\n - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.6760 - val_acc: 0.8435\nEpoch 563/1000\n - 0s - loss: 0.1132 - acc: 0.9891 - val_loss: 0.6164 - val_acc: 0.8609\nEpoch 564/1000\n - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 0.8609\nEpoch 565/1000\n - 0s - loss: 0.0865 - acc: 1.0000 - val_loss: 0.5430 - val_acc: 0.8522\nEpoch 566/1000\n - 0s - loss: 0.0921 - acc: 0.9978 - val_loss: 0.5459 - val_acc: 0.8696\nEpoch 567/1000\n - 0s - loss: 0.0911 - acc: 1.0000 - val_loss: 0.5424 - val_acc: 0.8609\nEpoch 568/1000\n - 0s - loss: 0.1012 - acc: 0.9978 - val_loss: 0.6235 - val_acc: 0.8522\n","name":"stdout"},{"output_type":"stream","text":"Epoch 569/1000\n - 0s - loss: 0.0871 - acc: 1.0000 - val_loss: 0.5451 - val_acc: 0.8522\nEpoch 570/1000\n - 0s - loss: 0.0828 - acc: 1.0000 - val_loss: 0.5404 - val_acc: 0.8696\nEpoch 571/1000\n - 0s - loss: 0.0876 - acc: 1.0000 - val_loss: 0.5599 - val_acc: 0.8609\nEpoch 572/1000\n - 0s - loss: 0.0833 - acc: 1.0000 - val_loss: 0.5292 - val_acc: 0.8609\nEpoch 573/1000\n - 0s - loss: 0.0853 - acc: 1.0000 - val_loss: 0.6253 - val_acc: 0.8522\nEpoch 574/1000\n - 0s - loss: 0.0885 - acc: 1.0000 - val_loss: 0.6311 - val_acc: 0.8522\nEpoch 575/1000\n - 0s - loss: 0.0846 - acc: 1.0000 - val_loss: 0.6026 - val_acc: 0.8522\nEpoch 576/1000\n - 0s - loss: 0.0893 - acc: 1.0000 - val_loss: 0.5439 - val_acc: 0.8609\nEpoch 577/1000\n - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.5373 - val_acc: 0.8609\nEpoch 578/1000\n - 0s - loss: 0.0822 - acc: 1.0000 - val_loss: 0.7581 - val_acc: 0.8087\nEpoch 579/1000\n - 0s - loss: 0.0894 - acc: 1.0000 - val_loss: 0.5404 - val_acc: 0.8609\nEpoch 580/1000\n - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.5977 - val_acc: 0.8435\nEpoch 581/1000\n - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.5743 - val_acc: 0.8435\nEpoch 582/1000\n - 0s - loss: 0.0837 - acc: 1.0000 - val_loss: 0.6380 - val_acc: 0.8435\nEpoch 583/1000\n - 0s - loss: 0.0832 - acc: 1.0000 - val_loss: 0.6483 - val_acc: 0.8435\nEpoch 584/1000\n - 0s - loss: 0.0855 - acc: 1.0000 - val_loss: 0.5424 - val_acc: 0.8609\nEpoch 585/1000\n - 0s - loss: 0.0810 - acc: 1.0000 - val_loss: 0.5685 - val_acc: 0.8348\nEpoch 586/1000\n - 0s - loss: 0.0927 - acc: 0.9978 - val_loss: 0.5293 - val_acc: 0.8522\nEpoch 587/1000\n - 0s - loss: 0.0951 - acc: 0.9978 - val_loss: 0.5478 - val_acc: 0.8696\nEpoch 588/1000\n - 0s - loss: 0.0997 - acc: 0.9957 - val_loss: 0.6727 - val_acc: 0.8435\nEpoch 589/1000\n - 0s - loss: 0.1032 - acc: 0.9957 - val_loss: 0.5679 - val_acc: 0.8609\nEpoch 590/1000\n - 0s - loss: 0.0922 - acc: 0.9978 - val_loss: 0.9733 - val_acc: 0.7826\nEpoch 591/1000\n - 0s - loss: 0.1230 - acc: 0.9848 - val_loss: 0.5686 - val_acc: 0.8609\nEpoch 592/1000\n - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.5566 - val_acc: 0.8609\nEpoch 593/1000\n - 0s - loss: 0.0853 - acc: 1.0000 - val_loss: 0.5402 - val_acc: 0.8609\nEpoch 594/1000\n - 0s - loss: 0.0844 - acc: 1.0000 - val_loss: 0.5464 - val_acc: 0.8609\nEpoch 595/1000\n - 0s - loss: 0.0939 - acc: 0.9978 - val_loss: 0.7422 - val_acc: 0.8261\nEpoch 596/1000\n - 0s - loss: 0.0922 - acc: 1.0000 - val_loss: 0.6006 - val_acc: 0.8522\nEpoch 597/1000\n - 0s - loss: 0.0796 - acc: 1.0000 - val_loss: 0.7010 - val_acc: 0.8348\nEpoch 598/1000\n - 0s - loss: 0.0817 - acc: 1.0000 - val_loss: 0.5338 - val_acc: 0.8609\nEpoch 599/1000\n - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.9691 - val_acc: 0.7652\nEpoch 600/1000\n - 0s - loss: 0.1148 - acc: 0.9870 - val_loss: 0.8023 - val_acc: 0.8087\nEpoch 601/1000\n - 0s - loss: 0.0938 - acc: 1.0000 - val_loss: 0.6333 - val_acc: 0.8435\nEpoch 602/1000\n - 0s - loss: 0.0929 - acc: 1.0000 - val_loss: 0.6025 - val_acc: 0.8609\nEpoch 603/1000\n - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.6257 - val_acc: 0.8435\nEpoch 604/1000\n - 0s - loss: 0.0940 - acc: 0.9978 - val_loss: 0.7567 - val_acc: 0.8261\nEpoch 605/1000\n - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.7620 - val_acc: 0.8174\nEpoch 606/1000\n - 0s - loss: 0.0840 - acc: 0.9978 - val_loss: 0.5431 - val_acc: 0.8522\nEpoch 607/1000\n - 0s - loss: 0.0854 - acc: 1.0000 - val_loss: 0.6047 - val_acc: 0.8261\nEpoch 608/1000\n - 0s - loss: 0.0951 - acc: 0.9978 - val_loss: 0.6075 - val_acc: 0.8522\nEpoch 609/1000\n - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 0.5376 - val_acc: 0.8609\nEpoch 610/1000\n - 0s - loss: 0.0842 - acc: 1.0000 - val_loss: 0.5606 - val_acc: 0.8609\nEpoch 611/1000\n - 0s - loss: 0.0835 - acc: 1.0000 - val_loss: 0.6953 - val_acc: 0.8522\nEpoch 612/1000\n - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.8522\nEpoch 613/1000\n - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.6859 - val_acc: 0.7913\nEpoch 614/1000\n - 0s - loss: 0.0967 - acc: 0.9957 - val_loss: 0.5606 - val_acc: 0.8609\nEpoch 615/1000\n - 0s - loss: 0.1004 - acc: 0.9957 - val_loss: 0.5623 - val_acc: 0.8609\nEpoch 616/1000\n - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 0.5560 - val_acc: 0.8609\nEpoch 617/1000\n - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 0.8184 - val_acc: 0.8087\nEpoch 618/1000\n - 0s - loss: 0.0911 - acc: 0.9978 - val_loss: 0.5810 - val_acc: 0.8522\nEpoch 619/1000\n - 0s - loss: 0.0823 - acc: 1.0000 - val_loss: 0.5982 - val_acc: 0.8435\nEpoch 620/1000\n - 0s - loss: 0.0975 - acc: 0.9957 - val_loss: 0.5809 - val_acc: 0.8522\nEpoch 621/1000\n - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.5940 - val_acc: 0.8609\nEpoch 622/1000\n - 0s - loss: 0.0787 - acc: 1.0000 - val_loss: 0.5588 - val_acc: 0.8522\nEpoch 623/1000\n - 0s - loss: 0.0806 - acc: 1.0000 - val_loss: 0.5407 - val_acc: 0.8522\nEpoch 624/1000\n - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.6195 - val_acc: 0.8522\nEpoch 625/1000\n - 0s - loss: 0.0773 - acc: 1.0000 - val_loss: 0.5438 - val_acc: 0.8609\nEpoch 626/1000\n - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 0.6493 - val_acc: 0.8522\nEpoch 627/1000\n - 0s - loss: 0.0800 - acc: 1.0000 - val_loss: 0.7022 - val_acc: 0.8348\nEpoch 628/1000\n - 0s - loss: 0.0793 - acc: 1.0000 - val_loss: 0.6076 - val_acc: 0.8609\nEpoch 629/1000\n - 0s - loss: 0.0752 - acc: 1.0000 - val_loss: 0.6274 - val_acc: 0.8522\nEpoch 630/1000\n - 0s - loss: 0.0748 - acc: 1.0000 - val_loss: 0.5909 - val_acc: 0.8609\nEpoch 631/1000\n - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.5342 - val_acc: 0.8609\nEpoch 632/1000\n - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 0.8696\nEpoch 633/1000\n - 0s - loss: 0.0882 - acc: 1.0000 - val_loss: 0.5576 - val_acc: 0.8609\nEpoch 634/1000\n - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.5952 - val_acc: 0.8261\nEpoch 635/1000\n - 0s - loss: 0.0906 - acc: 1.0000 - val_loss: 0.5813 - val_acc: 0.8522\nEpoch 636/1000\n - 0s - loss: 0.0775 - acc: 1.0000 - val_loss: 0.9185 - val_acc: 0.7826\nEpoch 637/1000\n - 0s - loss: 0.1031 - acc: 0.9935 - val_loss: 0.5719 - val_acc: 0.8696\nEpoch 638/1000\n - 0s - loss: 0.0800 - acc: 1.0000 - val_loss: 0.6195 - val_acc: 0.8348\nEpoch 639/1000\n - 0s - loss: 0.0806 - acc: 1.0000 - val_loss: 0.5539 - val_acc: 0.8522\nEpoch 640/1000\n - 0s - loss: 0.0890 - acc: 1.0000 - val_loss: 0.5660 - val_acc: 0.8522\nEpoch 641/1000\n - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 0.6275 - val_acc: 0.8348\nEpoch 642/1000\n - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.5546 - val_acc: 0.8783\nEpoch 643/1000\n - 0s - loss: 0.0793 - acc: 1.0000 - val_loss: 0.5671 - val_acc: 0.8696\nEpoch 644/1000\n - 0s - loss: 0.0813 - acc: 1.0000 - val_loss: 0.6050 - val_acc: 0.8348\nEpoch 645/1000\n - 0s - loss: 0.0849 - acc: 0.9978 - val_loss: 0.5734 - val_acc: 0.8609\nEpoch 646/1000\n - 0s - loss: 0.0755 - acc: 1.0000 - val_loss: 0.5519 - val_acc: 0.8522\nEpoch 647/1000\n - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 0.6544 - val_acc: 0.8435\nEpoch 648/1000\n - 0s - loss: 0.0785 - acc: 0.9978 - val_loss: 0.6644 - val_acc: 0.8435\nEpoch 649/1000\n - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.5325 - val_acc: 0.8522\nEpoch 650/1000\n - 0s - loss: 0.0763 - acc: 1.0000 - val_loss: 0.6560 - val_acc: 0.8522\nEpoch 651/1000\n - 0s - loss: 0.0792 - acc: 1.0000 - val_loss: 0.6437 - val_acc: 0.8522\nEpoch 652/1000\n - 0s - loss: 0.0747 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.8609\nEpoch 653/1000\n - 0s - loss: 0.0752 - acc: 1.0000 - val_loss: 0.5326 - val_acc: 0.8609\nEpoch 654/1000\n - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.5748 - val_acc: 0.8522\nEpoch 655/1000\n - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 0.5452 - val_acc: 0.8696\nEpoch 656/1000\n - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.5884 - val_acc: 0.8348\nEpoch 657/1000\n - 0s - loss: 0.0896 - acc: 0.9957 - val_loss: 0.5815 - val_acc: 0.8522\nEpoch 658/1000\n - 0s - loss: 0.0800 - acc: 1.0000 - val_loss: 0.6026 - val_acc: 0.8609\nEpoch 659/1000\n - 0s - loss: 0.0742 - acc: 1.0000 - val_loss: 0.5448 - val_acc: 0.8696\nEpoch 660/1000\n - 0s - loss: 0.0775 - acc: 1.0000 - val_loss: 0.5447 - val_acc: 0.8696\nEpoch 661/1000\n - 0s - loss: 0.0781 - acc: 1.0000 - val_loss: 0.5617 - val_acc: 0.8696\nEpoch 662/1000\n - 0s - loss: 0.0739 - acc: 1.0000 - val_loss: 0.6032 - val_acc: 0.8609\nEpoch 663/1000\n","name":"stdout"},{"output_type":"stream","text":" - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.5418 - val_acc: 0.8522\nEpoch 664/1000\n - 0s - loss: 0.0769 - acc: 1.0000 - val_loss: 0.5843 - val_acc: 0.8522\nEpoch 665/1000\n - 0s - loss: 0.0738 - acc: 1.0000 - val_loss: 0.5935 - val_acc: 0.8522\nEpoch 666/1000\n - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.6011 - val_acc: 0.8609\nEpoch 667/1000\n - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.5481 - val_acc: 0.8522\nEpoch 668/1000\n - 0s - loss: 0.0772 - acc: 1.0000 - val_loss: 0.5415 - val_acc: 0.8696\nEpoch 669/1000\n - 0s - loss: 0.0748 - acc: 1.0000 - val_loss: 0.5583 - val_acc: 0.8609\nEpoch 670/1000\n - 0s - loss: 0.0785 - acc: 0.9978 - val_loss: 0.5466 - val_acc: 0.8609\nEpoch 671/1000\n - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.9832 - val_acc: 0.7826\nEpoch 672/1000\n - 0s - loss: 0.0926 - acc: 1.0000 - val_loss: 0.5875 - val_acc: 0.8609\nEpoch 673/1000\n - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.5660 - val_acc: 0.8522\nEpoch 674/1000\n - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 0.5664 - val_acc: 0.8522\nEpoch 675/1000\n - 0s - loss: 0.0866 - acc: 0.9978 - val_loss: 0.5598 - val_acc: 0.8609\nEpoch 676/1000\n - 0s - loss: 0.0858 - acc: 1.0000 - val_loss: 0.6794 - val_acc: 0.8522\nEpoch 677/1000\n - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.5522 - val_acc: 0.8522\nEpoch 678/1000\n - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 0.5708 - val_acc: 0.8522\nEpoch 679/1000\n - 0s - loss: 0.0712 - acc: 1.0000 - val_loss: 0.5471 - val_acc: 0.8609\nEpoch 680/1000\n - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 0.6000 - val_acc: 0.8522\nEpoch 681/1000\n - 0s - loss: 0.0729 - acc: 1.0000 - val_loss: 0.5569 - val_acc: 0.8522\nEpoch 682/1000\n - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.5806 - val_acc: 0.8609\nEpoch 683/1000\n - 0s - loss: 0.0763 - acc: 1.0000 - val_loss: 0.5498 - val_acc: 0.8696\nEpoch 684/1000\n - 0s - loss: 0.0756 - acc: 1.0000 - val_loss: 0.5424 - val_acc: 0.8609\nEpoch 685/1000\n - 0s - loss: 0.0718 - acc: 1.0000 - val_loss: 0.5699 - val_acc: 0.8522\nEpoch 686/1000\n - 0s - loss: 0.0707 - acc: 1.0000 - val_loss: 0.6308 - val_acc: 0.8435\nEpoch 687/1000\n - 0s - loss: 0.0711 - acc: 1.0000 - val_loss: 0.6822 - val_acc: 0.8522\nEpoch 688/1000\n - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.6223 - val_acc: 0.8435\nEpoch 689/1000\n - 0s - loss: 0.0775 - acc: 1.0000 - val_loss: 0.6628 - val_acc: 0.7913\nEpoch 690/1000\n - 0s - loss: 0.0994 - acc: 0.9978 - val_loss: 0.8198 - val_acc: 0.8174\nEpoch 691/1000\n - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.5793 - val_acc: 0.8609\nEpoch 692/1000\n - 0s - loss: 0.0841 - acc: 0.9978 - val_loss: 0.5808 - val_acc: 0.8522\nEpoch 693/1000\n - 0s - loss: 0.0752 - acc: 1.0000 - val_loss: 0.6327 - val_acc: 0.8522\nEpoch 694/1000\n - 0s - loss: 0.0715 - acc: 1.0000 - val_loss: 0.5455 - val_acc: 0.8609\nEpoch 695/1000\n - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.5880 - val_acc: 0.8609\nEpoch 696/1000\n - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.6577 - val_acc: 0.8609\nEpoch 697/1000\n - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.8600 - val_acc: 0.8000\nEpoch 698/1000\n - 0s - loss: 0.0884 - acc: 0.9913 - val_loss: 0.5372 - val_acc: 0.8609\nEpoch 699/1000\n - 0s - loss: 0.0711 - acc: 1.0000 - val_loss: 0.6030 - val_acc: 0.8522\nEpoch 700/1000\n - 0s - loss: 0.0736 - acc: 1.0000 - val_loss: 0.5463 - val_acc: 0.8609\nEpoch 701/1000\n - 0s - loss: 0.0748 - acc: 1.0000 - val_loss: 0.6805 - val_acc: 0.8522\nEpoch 702/1000\n - 0s - loss: 0.0730 - acc: 1.0000 - val_loss: 0.6028 - val_acc: 0.8261\nEpoch 703/1000\n - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.8020 - val_acc: 0.8174\nEpoch 704/1000\n - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 0.7171 - val_acc: 0.8522\nEpoch 705/1000\n - 0s - loss: 0.0755 - acc: 1.0000 - val_loss: 0.6156 - val_acc: 0.8522\nEpoch 706/1000\n - 0s - loss: 0.0725 - acc: 1.0000 - val_loss: 0.6510 - val_acc: 0.7913\nEpoch 707/1000\n - 0s - loss: 0.0900 - acc: 0.9957 - val_loss: 0.6941 - val_acc: 0.8522\nEpoch 708/1000\n - 0s - loss: 0.0771 - acc: 1.0000 - val_loss: 0.6521 - val_acc: 0.8435\nEpoch 709/1000\n - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.6500 - val_acc: 0.8435\nEpoch 710/1000\n - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.5419 - val_acc: 0.8609\nEpoch 711/1000\n - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 0.5496 - val_acc: 0.8522\nEpoch 712/1000\n - 0s - loss: 0.0729 - acc: 1.0000 - val_loss: 0.5562 - val_acc: 0.8696\nEpoch 713/1000\n - 0s - loss: 0.0707 - acc: 1.0000 - val_loss: 0.5533 - val_acc: 0.8522\nEpoch 714/1000\n - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.5884 - val_acc: 0.8348\nEpoch 715/1000\n - 0s - loss: 0.0828 - acc: 0.9978 - val_loss: 0.5528 - val_acc: 0.8522\nEpoch 716/1000\n - 0s - loss: 0.0755 - acc: 1.0000 - val_loss: 0.6179 - val_acc: 0.8522\nEpoch 717/1000\n - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 0.8124 - val_acc: 0.8087\nEpoch 718/1000\n - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 0.7003 - val_acc: 0.8435\nEpoch 719/1000\n - 0s - loss: 0.0727 - acc: 1.0000 - val_loss: 0.6424 - val_acc: 0.8522\nEpoch 720/1000\n - 0s - loss: 0.0778 - acc: 1.0000 - val_loss: 0.7381 - val_acc: 0.8435\nEpoch 721/1000\n - 0s - loss: 0.0769 - acc: 1.0000 - val_loss: 0.5714 - val_acc: 0.8696\nEpoch 722/1000\n - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.5632 - val_acc: 0.8522\nEpoch 723/1000\n - 0s - loss: 0.0715 - acc: 1.0000 - val_loss: 0.8867 - val_acc: 0.7913\nEpoch 724/1000\n - 0s - loss: 0.0882 - acc: 0.9957 - val_loss: 0.5668 - val_acc: 0.8522\nEpoch 725/1000\n - 0s - loss: 0.0704 - acc: 1.0000 - val_loss: 0.6282 - val_acc: 0.8435\nEpoch 726/1000\n - 0s - loss: 0.0693 - acc: 1.0000 - val_loss: 0.5545 - val_acc: 0.8609\nEpoch 727/1000\n - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 0.6499 - val_acc: 0.8435\nEpoch 728/1000\n - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.6523 - val_acc: 0.8609\nEpoch 729/1000\n - 0s - loss: 0.0681 - acc: 1.0000 - val_loss: 0.6684 - val_acc: 0.8522\nEpoch 730/1000\n - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 0.5895 - val_acc: 0.8435\nEpoch 731/1000\n - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 0.6972 - val_acc: 0.8522\nEpoch 732/1000\n - 0s - loss: 0.0740 - acc: 1.0000 - val_loss: 0.6427 - val_acc: 0.8435\nEpoch 733/1000\n - 0s - loss: 0.0721 - acc: 1.0000 - val_loss: 0.5819 - val_acc: 0.8522\nEpoch 734/1000\n - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.5604 - val_acc: 0.8522\nEpoch 735/1000\n - 0s - loss: 0.0674 - acc: 1.0000 - val_loss: 0.5612 - val_acc: 0.8522\nEpoch 736/1000\n - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.5685 - val_acc: 0.8435\nEpoch 737/1000\n - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 0.6425 - val_acc: 0.8609\nEpoch 738/1000\n - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.6709 - val_acc: 0.8522\nEpoch 739/1000\n - 0s - loss: 0.0676 - acc: 1.0000 - val_loss: 0.8492 - val_acc: 0.7913\nEpoch 740/1000\n - 0s - loss: 0.0836 - acc: 0.9978 - val_loss: 0.5627 - val_acc: 0.8609\nEpoch 741/1000\n - 0s - loss: 0.0686 - acc: 1.0000 - val_loss: 0.5579 - val_acc: 0.8435\nEpoch 742/1000\n - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.6615 - val_acc: 0.8435\nEpoch 743/1000\n - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.5794 - val_acc: 0.8522\nEpoch 744/1000\n - 0s - loss: 0.0656 - acc: 1.0000 - val_loss: 0.5773 - val_acc: 0.8522\nEpoch 745/1000\n - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 0.5498 - val_acc: 0.8609\nEpoch 746/1000\n - 0s - loss: 0.0672 - acc: 1.0000 - val_loss: 0.6616 - val_acc: 0.7913\nEpoch 747/1000\n - 0s - loss: 0.0830 - acc: 0.9978 - val_loss: 0.8081 - val_acc: 0.8261\nEpoch 748/1000\n - 0s - loss: 0.0920 - acc: 0.9957 - val_loss: 0.5653 - val_acc: 0.8609\nEpoch 749/1000\n - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 0.5705 - val_acc: 0.8522\nEpoch 750/1000\n - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 0.7844 - val_acc: 0.8261\nEpoch 751/1000\n - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.5571 - val_acc: 0.8522\nEpoch 752/1000\n - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.6925 - val_acc: 0.8522\nEpoch 753/1000\n - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.5616 - val_acc: 0.8522\nEpoch 754/1000\n - 0s - loss: 0.0741 - acc: 1.0000 - val_loss: 0.6323 - val_acc: 0.8087\nEpoch 755/1000\n - 0s - loss: 0.0858 - acc: 0.9957 - val_loss: 0.6199 - val_acc: 0.8522\nEpoch 756/1000\n - 0s - loss: 0.0655 - acc: 1.0000 - val_loss: 0.5504 - val_acc: 0.8435\nEpoch 757/1000\n - 0s - loss: 0.0703 - acc: 1.0000 - val_loss: 0.5522 - val_acc: 0.8609\n","name":"stdout"},{"output_type":"stream","text":"Epoch 758/1000\n - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 0.5537 - val_acc: 0.8609\nEpoch 759/1000\n - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.5845 - val_acc: 0.8522\nEpoch 760/1000\n - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.5553 - val_acc: 0.8435\nEpoch 761/1000\n - 0s - loss: 0.0639 - acc: 1.0000 - val_loss: 0.5835 - val_acc: 0.8609\nEpoch 762/1000\n - 0s - loss: 0.0703 - acc: 1.0000 - val_loss: 0.5880 - val_acc: 0.8348\nEpoch 763/1000\n - 0s - loss: 0.0632 - acc: 1.0000 - val_loss: 0.5532 - val_acc: 0.8522\nEpoch 764/1000\n - 0s - loss: 0.0693 - acc: 1.0000 - val_loss: 0.5478 - val_acc: 0.8609\nEpoch 765/1000\n - 0s - loss: 0.0642 - acc: 1.0000 - val_loss: 0.6104 - val_acc: 0.8348\nEpoch 766/1000\n - 0s - loss: 0.0845 - acc: 0.9957 - val_loss: 0.6097 - val_acc: 0.8522\nEpoch 767/1000\n - 0s - loss: 0.0688 - acc: 1.0000 - val_loss: 0.5640 - val_acc: 0.8435\nEpoch 768/1000\n - 0s - loss: 0.0641 - acc: 1.0000 - val_loss: 0.7320 - val_acc: 0.8435\nEpoch 769/1000\n - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.6432 - val_acc: 0.8522\nEpoch 770/1000\n - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.8496 - val_acc: 0.8000\nEpoch 771/1000\n - 0s - loss: 0.0868 - acc: 0.9978 - val_loss: 0.5786 - val_acc: 0.8696\nEpoch 772/1000\n - 0s - loss: 0.0669 - acc: 1.0000 - val_loss: 0.5579 - val_acc: 0.8522\nEpoch 773/1000\n - 0s - loss: 0.0622 - acc: 1.0000 - val_loss: 0.6079 - val_acc: 0.8522\nEpoch 774/1000\n - 0s - loss: 0.0712 - acc: 1.0000 - val_loss: 0.6052 - val_acc: 0.8522\nEpoch 775/1000\n - 0s - loss: 0.0635 - acc: 1.0000 - val_loss: 0.5696 - val_acc: 0.8522\nEpoch 776/1000\n - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.5510 - val_acc: 0.8522\nEpoch 777/1000\n - 0s - loss: 0.0681 - acc: 1.0000 - val_loss: 0.5848 - val_acc: 0.8522\nEpoch 778/1000\n - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.6977 - val_acc: 0.7913\nEpoch 779/1000\n - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.8182 - val_acc: 0.8174\nEpoch 780/1000\n - 0s - loss: 0.0745 - acc: 1.0000 - val_loss: 0.6080 - val_acc: 0.8522\nEpoch 781/1000\n - 0s - loss: 0.0675 - acc: 1.0000 - val_loss: 0.5615 - val_acc: 0.8696\nEpoch 782/1000\n - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 0.5853 - val_acc: 0.8522\nEpoch 783/1000\n - 0s - loss: 0.0716 - acc: 0.9978 - val_loss: 0.5772 - val_acc: 0.8522\nEpoch 784/1000\n - 0s - loss: 0.0699 - acc: 0.9978 - val_loss: 0.5967 - val_acc: 0.8522\nEpoch 785/1000\n - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.6305 - val_acc: 0.8522\nEpoch 786/1000\n - 0s - loss: 0.0700 - acc: 1.0000 - val_loss: 0.6011 - val_acc: 0.8522\nEpoch 787/1000\n - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 0.5468 - val_acc: 0.8522\nEpoch 788/1000\n - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.5666 - val_acc: 0.8522\nEpoch 789/1000\n - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.6169 - val_acc: 0.8435\nEpoch 790/1000\n - 0s - loss: 0.0743 - acc: 0.9978 - val_loss: 0.6573 - val_acc: 0.8522\nEpoch 791/1000\n - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 0.6989 - val_acc: 0.8435\nEpoch 792/1000\n - 0s - loss: 0.0760 - acc: 0.9957 - val_loss: 0.5595 - val_acc: 0.8435\nEpoch 793/1000\n - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 0.5595 - val_acc: 0.8522\nEpoch 794/1000\n - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 0.5578 - val_acc: 0.8522\nEpoch 795/1000\n - 0s - loss: 0.0632 - acc: 1.0000 - val_loss: 0.5616 - val_acc: 0.8522\nEpoch 796/1000\n - 0s - loss: 0.0611 - acc: 1.0000 - val_loss: 0.5943 - val_acc: 0.8522\nEpoch 797/1000\n - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.5724 - val_acc: 0.8522\nEpoch 798/1000\n - 0s - loss: 0.0631 - acc: 1.0000 - val_loss: 0.5657 - val_acc: 0.8783\nEpoch 799/1000\n - 0s - loss: 0.0641 - acc: 1.0000 - val_loss: 0.5730 - val_acc: 0.8696\nEpoch 800/1000\n - 0s - loss: 0.0716 - acc: 1.0000 - val_loss: 0.5969 - val_acc: 0.8522\nEpoch 801/1000\n - 0s - loss: 0.0643 - acc: 1.0000 - val_loss: 0.6752 - val_acc: 0.7913\nEpoch 802/1000\n - 0s - loss: 0.0933 - acc: 0.9935 - val_loss: 0.6647 - val_acc: 0.8522\nEpoch 803/1000\n - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.5625 - val_acc: 0.8696\nEpoch 804/1000\n - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 0.6946 - val_acc: 0.8522\nEpoch 805/1000\n - 0s - loss: 0.0817 - acc: 0.9957 - val_loss: 0.6251 - val_acc: 0.8522\nEpoch 806/1000\n - 0s - loss: 0.0642 - acc: 1.0000 - val_loss: 0.5650 - val_acc: 0.8522\nEpoch 807/1000\n - 0s - loss: 0.0668 - acc: 1.0000 - val_loss: 0.6265 - val_acc: 0.8435\nEpoch 808/1000\n - 0s - loss: 0.0674 - acc: 1.0000 - val_loss: 0.5938 - val_acc: 0.8522\nEpoch 809/1000\n - 0s - loss: 0.0769 - acc: 0.9957 - val_loss: 0.6826 - val_acc: 0.8522\nEpoch 810/1000\n - 0s - loss: 0.0688 - acc: 1.0000 - val_loss: 0.6664 - val_acc: 0.8435\nEpoch 811/1000\n - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.5594 - val_acc: 0.8522\nEpoch 812/1000\n - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.5656 - val_acc: 0.8522\nEpoch 813/1000\n - 0s - loss: 0.0635 - acc: 1.0000 - val_loss: 0.6064 - val_acc: 0.8348\nEpoch 814/1000\n - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.6202 - val_acc: 0.8174\nEpoch 815/1000\n - 0s - loss: 0.0703 - acc: 1.0000 - val_loss: 0.5681 - val_acc: 0.8522\nEpoch 816/1000\n - 0s - loss: 0.0762 - acc: 0.9978 - val_loss: 0.6100 - val_acc: 0.8522\nEpoch 817/1000\n - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.5663 - val_acc: 0.8696\nEpoch 818/1000\n - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.5694 - val_acc: 0.8696\nEpoch 819/1000\n - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.5454 - val_acc: 0.8522\nEpoch 820/1000\n - 0s - loss: 0.0600 - acc: 1.0000 - val_loss: 0.6059 - val_acc: 0.8522\nEpoch 821/1000\n - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 0.6094 - val_acc: 0.8522\nEpoch 822/1000\n - 0s - loss: 0.0699 - acc: 1.0000 - val_loss: 0.7967 - val_acc: 0.8174\nEpoch 823/1000\n - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 0.5743 - val_acc: 0.8696\nEpoch 824/1000\n - 0s - loss: 0.0737 - acc: 0.9978 - val_loss: 0.6180 - val_acc: 0.8522\nEpoch 825/1000\n - 0s - loss: 0.0628 - acc: 1.0000 - val_loss: 0.7390 - val_acc: 0.8435\nEpoch 826/1000\n - 0s - loss: 0.0641 - acc: 1.0000 - val_loss: 0.6988 - val_acc: 0.8435\nEpoch 827/1000\n - 0s - loss: 0.0616 - acc: 1.0000 - val_loss: 0.7019 - val_acc: 0.8522\nEpoch 828/1000\n - 0s - loss: 0.0609 - acc: 1.0000 - val_loss: 0.5517 - val_acc: 0.8783\nEpoch 829/1000\n - 0s - loss: 0.0631 - acc: 1.0000 - val_loss: 0.5552 - val_acc: 0.8783\nEpoch 830/1000\n - 0s - loss: 0.0645 - acc: 1.0000 - val_loss: 0.5462 - val_acc: 0.8696\nEpoch 831/1000\n - 0s - loss: 0.0616 - acc: 1.0000 - val_loss: 0.6484 - val_acc: 0.7826\nEpoch 832/1000\n - 0s - loss: 0.0736 - acc: 0.9978 - val_loss: 0.7939 - val_acc: 0.8087\nEpoch 833/1000\n - 0s - loss: 0.0769 - acc: 1.0000 - val_loss: 0.6043 - val_acc: 0.8435\nEpoch 834/1000\n - 0s - loss: 0.0738 - acc: 1.0000 - val_loss: 0.7988 - val_acc: 0.8087\nEpoch 835/1000\n - 0s - loss: 0.0705 - acc: 0.9978 - val_loss: 0.5944 - val_acc: 0.8522\nEpoch 836/1000\n - 0s - loss: 0.0606 - acc: 1.0000 - val_loss: 0.6607 - val_acc: 0.8522\nEpoch 837/1000\n - 0s - loss: 0.0610 - acc: 1.0000 - val_loss: 0.5549 - val_acc: 0.8696\nEpoch 838/1000\n - 0s - loss: 0.0611 - acc: 1.0000 - val_loss: 0.5635 - val_acc: 0.8609\nEpoch 839/1000\n - 0s - loss: 0.0591 - acc: 1.0000 - val_loss: 0.7082 - val_acc: 0.8522\nEpoch 840/1000\n - 0s - loss: 0.0590 - acc: 1.0000 - val_loss: 0.6384 - val_acc: 0.8609\nEpoch 841/1000\n - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.5373 - val_acc: 0.8696\nEpoch 842/1000\n - 0s - loss: 0.0699 - acc: 0.9978 - val_loss: 0.6145 - val_acc: 0.8522\nEpoch 843/1000\n - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.6285 - val_acc: 0.8000\nEpoch 844/1000\n - 0s - loss: 0.0823 - acc: 0.9978 - val_loss: 0.8669 - val_acc: 0.8000\nEpoch 845/1000\n - 0s - loss: 0.0739 - acc: 1.0000 - val_loss: 0.8080 - val_acc: 0.8174\nEpoch 846/1000\n - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.5874 - val_acc: 0.8435\nEpoch 847/1000\n - 0s - loss: 0.0603 - acc: 1.0000 - val_loss: 0.5362 - val_acc: 0.8435\nEpoch 848/1000\n - 0s - loss: 0.0605 - acc: 1.0000 - val_loss: 0.6068 - val_acc: 0.8261\nEpoch 849/1000\n - 0s - loss: 0.0676 - acc: 1.0000 - val_loss: 0.5695 - val_acc: 0.8522\nEpoch 850/1000\n - 0s - loss: 0.0600 - acc: 1.0000 - val_loss: 0.5538 - val_acc: 0.8522\nEpoch 851/1000\n - 0s - loss: 0.0562 - acc: 1.0000 - val_loss: 0.6187 - val_acc: 0.8609\nEpoch 852/1000\n","name":"stdout"},{"output_type":"stream","text":" - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.5572 - val_acc: 0.8435\nEpoch 853/1000\n - 0s - loss: 0.0603 - acc: 1.0000 - val_loss: 0.6664 - val_acc: 0.8435\nEpoch 854/1000\n - 0s - loss: 0.0580 - acc: 1.0000 - val_loss: 0.9425 - val_acc: 0.8000\nEpoch 855/1000\n - 0s - loss: 0.1066 - acc: 0.9804 - val_loss: 0.5672 - val_acc: 0.8870\nEpoch 856/1000\n - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.8261\nEpoch 857/1000\n - 0s - loss: 0.0750 - acc: 0.9978 - val_loss: 0.5750 - val_acc: 0.8609\nEpoch 858/1000\n - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 0.6688 - val_acc: 0.8435\nEpoch 859/1000\n - 0s - loss: 0.0605 - acc: 1.0000 - val_loss: 0.5957 - val_acc: 0.8522\nEpoch 860/1000\n - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.5485 - val_acc: 0.8609\nEpoch 861/1000\n - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 0.6072 - val_acc: 0.8609\nEpoch 862/1000\n - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 0.6087 - val_acc: 0.8522\nEpoch 863/1000\n - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 0.7601 - val_acc: 0.8348\nEpoch 864/1000\n - 0s - loss: 0.0774 - acc: 0.9978 - val_loss: 0.5571 - val_acc: 0.8609\nEpoch 865/1000\n - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.5956 - val_acc: 0.8522\nEpoch 866/1000\n - 0s - loss: 0.0582 - acc: 1.0000 - val_loss: 0.6583 - val_acc: 0.8522\nEpoch 867/1000\n - 0s - loss: 0.0587 - acc: 1.0000 - val_loss: 0.5510 - val_acc: 0.8522\nEpoch 868/1000\n - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 0.6609 - val_acc: 0.8522\nEpoch 869/1000\n - 0s - loss: 0.0622 - acc: 1.0000 - val_loss: 0.7424 - val_acc: 0.8174\nEpoch 870/1000\n - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 0.6133 - val_acc: 0.8609\nEpoch 871/1000\n - 0s - loss: 0.0575 - acc: 1.0000 - val_loss: 0.5546 - val_acc: 0.8609\nEpoch 872/1000\n - 0s - loss: 0.0591 - acc: 1.0000 - val_loss: 0.6458 - val_acc: 0.8435\nEpoch 873/1000\n - 0s - loss: 0.0636 - acc: 0.9978 - val_loss: 0.6353 - val_acc: 0.8522\nEpoch 874/1000\n - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.5457 - val_acc: 0.8522\nEpoch 875/1000\n - 0s - loss: 0.0562 - acc: 1.0000 - val_loss: 0.6683 - val_acc: 0.8522\nEpoch 876/1000\n - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.8199 - val_acc: 0.8000\nEpoch 877/1000\n - 0s - loss: 0.0717 - acc: 0.9957 - val_loss: 0.5468 - val_acc: 0.8522\nEpoch 878/1000\n - 0s - loss: 0.0641 - acc: 1.0000 - val_loss: 0.5567 - val_acc: 0.8696\nEpoch 879/1000\n - 0s - loss: 0.0641 - acc: 0.9978 - val_loss: 0.6130 - val_acc: 0.8609\nEpoch 880/1000\n - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 0.5708 - val_acc: 0.8696\nEpoch 881/1000\n - 0s - loss: 0.0652 - acc: 0.9978 - val_loss: 0.8048 - val_acc: 0.7913\nEpoch 882/1000\n - 0s - loss: 0.0754 - acc: 1.0000 - val_loss: 0.5856 - val_acc: 0.8522\nEpoch 883/1000\n - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 0.5794 - val_acc: 0.8522\nEpoch 884/1000\n - 0s - loss: 0.0561 - acc: 1.0000 - val_loss: 0.6484 - val_acc: 0.8435\nEpoch 885/1000\n - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.5538 - val_acc: 0.8696\nEpoch 886/1000\n - 0s - loss: 0.0659 - acc: 1.0000 - val_loss: 0.5544 - val_acc: 0.8609\nEpoch 887/1000\n - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.5591 - val_acc: 0.8522\nEpoch 888/1000\n - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 0.5906 - val_acc: 0.8609\nEpoch 889/1000\n - 0s - loss: 0.0585 - acc: 1.0000 - val_loss: 0.5554 - val_acc: 0.8522\nEpoch 890/1000\n - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.5681 - val_acc: 0.8609\nEpoch 891/1000\n - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.6447 - val_acc: 0.7913\nEpoch 892/1000\n - 0s - loss: 0.0722 - acc: 0.9978 - val_loss: 0.5879 - val_acc: 0.8435\nEpoch 893/1000\n - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 0.6698 - val_acc: 0.8435\nEpoch 894/1000\n - 0s - loss: 0.0727 - acc: 0.9978 - val_loss: 0.7640 - val_acc: 0.8348\nEpoch 895/1000\n - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.5617 - val_acc: 0.8609\nEpoch 896/1000\n - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 0.8368 - val_acc: 0.7913\nEpoch 897/1000\n - 0s - loss: 0.0746 - acc: 0.9978 - val_loss: 0.5832 - val_acc: 0.8522\nEpoch 898/1000\n - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 0.6236 - val_acc: 0.8609\nEpoch 899/1000\n - 0s - loss: 0.0579 - acc: 1.0000 - val_loss: 0.6189 - val_acc: 0.8435\nEpoch 900/1000\n - 0s - loss: 0.0581 - acc: 1.0000 - val_loss: 0.5431 - val_acc: 0.8609\nEpoch 901/1000\n - 0s - loss: 0.0585 - acc: 1.0000 - val_loss: 0.6011 - val_acc: 0.8522\nEpoch 902/1000\n - 0s - loss: 0.0625 - acc: 1.0000 - val_loss: 0.7890 - val_acc: 0.8348\nEpoch 903/1000\n - 0s - loss: 0.0745 - acc: 0.9978 - val_loss: 0.5784 - val_acc: 0.8522\nEpoch 904/1000\n - 0s - loss: 0.0597 - acc: 1.0000 - val_loss: 0.5978 - val_acc: 0.8522\nEpoch 905/1000\n - 0s - loss: 0.0574 - acc: 1.0000 - val_loss: 0.5638 - val_acc: 0.8522\nEpoch 906/1000\n - 0s - loss: 0.0691 - acc: 0.9978 - val_loss: 0.8431 - val_acc: 0.8087\nEpoch 907/1000\n - 0s - loss: 0.0701 - acc: 1.0000 - val_loss: 0.5699 - val_acc: 0.8522\nEpoch 908/1000\n - 0s - loss: 0.0591 - acc: 1.0000 - val_loss: 0.5650 - val_acc: 0.8522\nEpoch 909/1000\n - 0s - loss: 0.0554 - acc: 1.0000 - val_loss: 0.8285 - val_acc: 0.8000\nEpoch 910/1000\n - 0s - loss: 0.0656 - acc: 0.9978 - val_loss: 0.6261 - val_acc: 0.8609\nEpoch 911/1000\n - 0s - loss: 0.0566 - acc: 1.0000 - val_loss: 0.5772 - val_acc: 0.8696\nEpoch 912/1000\n - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.6388 - val_acc: 0.8435\nEpoch 913/1000\n - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.5695 - val_acc: 0.8609\nEpoch 914/1000\n - 0s - loss: 0.0779 - acc: 0.9891 - val_loss: 0.6411 - val_acc: 0.8435\nEpoch 915/1000\n - 0s - loss: 0.0743 - acc: 0.9957 - val_loss: 0.6870 - val_acc: 0.7826\nEpoch 916/1000\n - 0s - loss: 0.0738 - acc: 1.0000 - val_loss: 0.6661 - val_acc: 0.8348\nEpoch 917/1000\n - 0s - loss: 0.0631 - acc: 1.0000 - val_loss: 0.5865 - val_acc: 0.8696\nEpoch 918/1000\n - 0s - loss: 0.0732 - acc: 0.9957 - val_loss: 0.6495 - val_acc: 0.8000\nEpoch 919/1000\n - 0s - loss: 0.0768 - acc: 0.9978 - val_loss: 0.6240 - val_acc: 0.8435\nEpoch 920/1000\n - 0s - loss: 0.0596 - acc: 1.0000 - val_loss: 0.5784 - val_acc: 0.8522\nEpoch 921/1000\n - 0s - loss: 0.0540 - acc: 1.0000 - val_loss: 0.5574 - val_acc: 0.8609\nEpoch 922/1000\n - 0s - loss: 0.0577 - acc: 1.0000 - val_loss: 0.6712 - val_acc: 0.8435\nEpoch 923/1000\n - 0s - loss: 0.0615 - acc: 1.0000 - val_loss: 0.6035 - val_acc: 0.8522\nEpoch 924/1000\n - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.5718 - val_acc: 0.8609\nEpoch 925/1000\n - 0s - loss: 0.0546 - acc: 1.0000 - val_loss: 0.5497 - val_acc: 0.8522\nEpoch 926/1000\n - 0s - loss: 0.0550 - acc: 1.0000 - val_loss: 0.5505 - val_acc: 0.8522\nEpoch 927/1000\n - 0s - loss: 0.0551 - acc: 1.0000 - val_loss: 0.7275 - val_acc: 0.8435\nEpoch 928/1000\n - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.5868 - val_acc: 0.8522\nEpoch 929/1000\n - 0s - loss: 0.0615 - acc: 1.0000 - val_loss: 0.5593 - val_acc: 0.8609\nEpoch 930/1000\n - 0s - loss: 0.0537 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.8348\nEpoch 931/1000\n - 0s - loss: 0.0631 - acc: 1.0000 - val_loss: 0.5825 - val_acc: 0.8609\nEpoch 932/1000\n - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 0.6577 - val_acc: 0.8435\nEpoch 933/1000\n - 0s - loss: 0.0574 - acc: 1.0000 - val_loss: 0.5835 - val_acc: 0.8696\nEpoch 934/1000\n - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 0.5767 - val_acc: 0.8609\nEpoch 935/1000\n - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.7705 - val_acc: 0.8435\nEpoch 936/1000\n - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.6266 - val_acc: 0.8261\nEpoch 937/1000\n - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.5816 - val_acc: 0.8609\nEpoch 938/1000\n - 0s - loss: 0.0631 - acc: 1.0000 - val_loss: 0.5872 - val_acc: 0.8609\nEpoch 939/1000\n - 0s - loss: 0.0593 - acc: 1.0000 - val_loss: 0.6491 - val_acc: 0.8435\nEpoch 940/1000\n - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 0.5730 - val_acc: 0.8696\nEpoch 941/1000\n - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.5640 - val_acc: 0.8696\nEpoch 942/1000\n - 0s - loss: 0.0548 - acc: 1.0000 - val_loss: 0.5668 - val_acc: 0.8696\nEpoch 943/1000\n - 0s - loss: 0.0558 - acc: 1.0000 - val_loss: 0.5915 - val_acc: 0.8522\nEpoch 944/1000\n - 0s - loss: 0.0536 - acc: 1.0000 - val_loss: 0.5849 - val_acc: 0.8348\nEpoch 945/1000\n - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.6135 - val_acc: 0.8522\nEpoch 946/1000\n - 0s - loss: 0.0596 - acc: 1.0000 - val_loss: 0.5500 - val_acc: 0.8696\n","name":"stdout"},{"output_type":"stream","text":"Epoch 947/1000\n - 0s - loss: 0.0589 - acc: 1.0000 - val_loss: 0.7518 - val_acc: 0.8435\nEpoch 948/1000\n - 0s - loss: 0.0697 - acc: 0.9978 - val_loss: 0.6103 - val_acc: 0.8522\nEpoch 949/1000\n - 0s - loss: 0.0544 - acc: 1.0000 - val_loss: 0.6368 - val_acc: 0.8000\nEpoch 950/1000\n - 0s - loss: 0.0741 - acc: 1.0000 - val_loss: 0.5921 - val_acc: 0.8522\nEpoch 951/1000\n - 0s - loss: 0.0524 - acc: 1.0000 - val_loss: 0.7185 - val_acc: 0.8522\nEpoch 952/1000\n - 0s - loss: 0.0608 - acc: 0.9978 - val_loss: 0.5768 - val_acc: 0.8696\nEpoch 953/1000\n - 0s - loss: 0.0588 - acc: 1.0000 - val_loss: 0.8425 - val_acc: 0.8000\nEpoch 954/1000\n - 0s - loss: 0.0729 - acc: 0.9978 - val_loss: 0.5662 - val_acc: 0.8609\nEpoch 955/1000\n - 0s - loss: 0.0564 - acc: 1.0000 - val_loss: 0.5891 - val_acc: 0.8522\nEpoch 956/1000\n - 0s - loss: 0.1059 - acc: 0.9804 - val_loss: 0.6264 - val_acc: 0.8261\nEpoch 957/1000\n - 0s - loss: 0.0741 - acc: 1.0000 - val_loss: 1.0223 - val_acc: 0.7913\nEpoch 958/1000\n - 0s - loss: 0.0770 - acc: 0.9957 - val_loss: 0.6217 - val_acc: 0.8522\nEpoch 959/1000\n - 0s - loss: 0.0571 - acc: 1.0000 - val_loss: 0.7923 - val_acc: 0.8174\nEpoch 960/1000\n - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.8763 - val_acc: 0.8000\nEpoch 961/1000\n - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.6246 - val_acc: 0.8522\nEpoch 962/1000\n - 0s - loss: 0.0565 - acc: 1.0000 - val_loss: 0.7274 - val_acc: 0.8522\nEpoch 963/1000\n - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 0.6944 - val_acc: 0.8435\nEpoch 964/1000\n - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.6784 - val_acc: 0.8522\nEpoch 965/1000\n - 0s - loss: 0.0538 - acc: 1.0000 - val_loss: 0.7005 - val_acc: 0.8522\nEpoch 966/1000\n - 0s - loss: 0.0542 - acc: 1.0000 - val_loss: 0.6147 - val_acc: 0.8435\nEpoch 967/1000\n - 0s - loss: 0.0537 - acc: 1.0000 - val_loss: 0.5575 - val_acc: 0.8609\nEpoch 968/1000\n - 0s - loss: 0.0563 - acc: 1.0000 - val_loss: 0.5532 - val_acc: 0.8609\nEpoch 969/1000\n - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.6859 - val_acc: 0.8522\nEpoch 970/1000\n - 0s - loss: 0.0547 - acc: 1.0000 - val_loss: 0.5616 - val_acc: 0.8696\nEpoch 971/1000\n - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 0.5672 - val_acc: 0.8435\nEpoch 972/1000\n - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.5632 - val_acc: 0.8609\nEpoch 973/1000\n - 0s - loss: 0.0587 - acc: 1.0000 - val_loss: 0.5644 - val_acc: 0.8522\nEpoch 974/1000\n - 0s - loss: 0.0561 - acc: 1.0000 - val_loss: 0.5662 - val_acc: 0.8609\nEpoch 975/1000\n - 0s - loss: 0.0538 - acc: 1.0000 - val_loss: 0.6982 - val_acc: 0.8522\nEpoch 976/1000\n - 0s - loss: 0.0645 - acc: 1.0000 - val_loss: 0.5677 - val_acc: 0.8609\nEpoch 977/1000\n - 0s - loss: 0.0540 - acc: 1.0000 - val_loss: 0.6673 - val_acc: 0.8522\nEpoch 978/1000\n - 0s - loss: 0.0588 - acc: 1.0000 - val_loss: 0.5664 - val_acc: 0.8522\nEpoch 979/1000\n - 0s - loss: 0.0614 - acc: 1.0000 - val_loss: 0.7575 - val_acc: 0.7913\nEpoch 980/1000\n - 0s - loss: 0.0781 - acc: 0.9978 - val_loss: 0.7179 - val_acc: 0.8435\nEpoch 981/1000\n - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.7335 - val_acc: 0.8522\nEpoch 982/1000\n - 0s - loss: 0.0557 - acc: 1.0000 - val_loss: 0.6072 - val_acc: 0.8609\nEpoch 983/1000\n - 0s - loss: 0.0563 - acc: 1.0000 - val_loss: 0.5719 - val_acc: 0.8783\nEpoch 984/1000\n - 0s - loss: 0.0540 - acc: 1.0000 - val_loss: 0.6218 - val_acc: 0.8522\nEpoch 985/1000\n - 0s - loss: 0.0557 - acc: 1.0000 - val_loss: 0.5568 - val_acc: 0.8522\nEpoch 986/1000\n - 0s - loss: 0.0548 - acc: 1.0000 - val_loss: 0.5866 - val_acc: 0.8522\nEpoch 987/1000\n - 0s - loss: 0.0521 - acc: 1.0000 - val_loss: 0.6303 - val_acc: 0.8174\nEpoch 988/1000\n - 0s - loss: 0.0596 - acc: 1.0000 - val_loss: 0.5612 - val_acc: 0.8609\nEpoch 989/1000\n - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 0.8604 - val_acc: 0.8000\nEpoch 990/1000\n - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 0.5724 - val_acc: 0.8522\nEpoch 991/1000\n - 0s - loss: 0.0553 - acc: 1.0000 - val_loss: 0.5674 - val_acc: 0.8609\nEpoch 992/1000\n - 0s - loss: 0.0526 - acc: 1.0000 - val_loss: 0.5600 - val_acc: 0.8609\nEpoch 993/1000\n - 0s - loss: 0.0565 - acc: 1.0000 - val_loss: 0.5708 - val_acc: 0.8522\nEpoch 994/1000\n - 0s - loss: 0.0535 - acc: 1.0000 - val_loss: 0.5570 - val_acc: 0.8522\nEpoch 995/1000\n - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 0.7449 - val_acc: 0.8435\nEpoch 996/1000\n - 0s - loss: 0.0544 - acc: 1.0000 - val_loss: 0.6088 - val_acc: 0.8174\nEpoch 997/1000\n - 0s - loss: 0.0759 - acc: 0.9957 - val_loss: 0.6259 - val_acc: 0.8522\nEpoch 998/1000\n - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.5844 - val_acc: 0.8522\nEpoch 999/1000\n - 0s - loss: 0.0569 - acc: 1.0000 - val_loss: 0.8379 - val_acc: 0.7913\nEpoch 1000/1000\n - 0s - loss: 0.0583 - acc: 1.0000 - val_loss: 0.5688 - val_acc: 0.8522\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(eval_loss, eval_accuracy) = model.evaluate(validation_data, validation_labels, batch_size= batch_size, verbose=1)\n\nprint(\"Validation Accuracy: {:.4f}%\".format(eval_accuracy * 100))\nprint(\"Validation Loss: {}\".format(eval_loss))","execution_count":44,"outputs":[{"output_type":"stream","text":"115/115 [==============================] - 0s 85us/step\nValidation Accuracy: 85.2174%\nValidation Loss: 0.5688307274942813\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = test_generator.filenames\ntruth = test_generator.classes\nlabel = test_generator.class_indices\nindexlabel = dict((value, key) for key, value in label.items())\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\npreds = model.predict(test_data)\n\npredictions = [i.argmax() for i in preds]\ny_true = [i.argmax() for i in test_labels]\ncm = confusion_matrix(y_pred=predictions, y_true=y_true)\n\nprint('Test Accuracy: {}'.format(accuracy_score(y_true=y_true, y_pred=predictions)))","execution_count":45,"outputs":[{"output_type":"stream","text":"Test Accuracy: 0.9411764705882353\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"axes.grid\"] = False\nplt.rcParams.update({'font.size': 20})\n\nlabels = []\n\nlabel = test_generator.class_indices\nindexlabel = dict((value, key) for key, value in label.items())\n\nfor k,v in indexlabel.items():\n    labels.append(v)\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion Matrix')\n\n    print(cm)\n#     fig = plt.figure()\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n#     plt.title(title)\n#     plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n#     plt.savefig('plots/3.InceptionV3-2-Private-DataSet-CM.png', bbox_inches='tight', dpi = 100) \n\n\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(cm, classes=labels, title=' ')","execution_count":46,"outputs":[{"output_type":"stream","text":"Confusion Matrix\n[[52  3]\n [ 2 28]]\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\ny_pred=predictions\ny_pred_probabilities=y_pred\n\n# y_pred = np.argmax(y_pred,axis = 1) \ny_actual = y_true\n\nclassnames=[]\nfor classname in test_generator.class_indices:\n    classnames.append(classname)\n\nconfusion_mtx = confusion_matrix(y_actual, y_pred) \nprint(confusion_mtx)\ntarget_names = classnames\nprint(classification_report(y_actual, y_pred, target_names=target_names))","execution_count":47,"outputs":[{"output_type":"stream","text":"[[52  3]\n [ 2 28]]\n              precision    recall  f1-score   support\n\n    negative       0.96      0.95      0.95        55\n    positive       0.90      0.93      0.92        30\n\n    accuracy                           0.94        85\n   macro avg       0.93      0.94      0.94        85\nweighted avg       0.94      0.94      0.94        85\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total=sum(sum(cm))\n\nsensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\nprint('Sensitivity : ', sensitivity )\n\nSpecificity = cm[1,1]/(cm[1,1]+cm[0,1])\nprint('Specificity : ', Specificity )","execution_count":54,"outputs":[{"output_type":"stream","text":"Sensitivity :  0.9629629629629629\nSpecificity :  0.9032258064516129\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_class = model.predict(test_data, verbose=1)\n\ny_pred_class = [np.argmax(r) for r in y_pred_class]\ntest_y = [np.argmax(r) for r in test_labels]\n\n\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, roc_curve\n\n# Precision\nprint('Precision = ', precision_score(test_y, y_pred_class, average='weighted'))\n# (None, 'micro', 'macro', 'weighted', 'samples')\n\n# Recall\nprint('Recall = ', recall_score(test_y, y_pred_class, average='weighted'))\n\n# f1_score\nprint('f1_score = ', f1_score(test_y, y_pred_class, average='weighted'))","execution_count":55,"outputs":[{"output_type":"stream","text":"85/85 [==============================] - 0s 631us/step\nPrecision =  0.9418792606648394\nRecall =  0.9411764705882353\nf1_score =  0.9413887979616573\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\n\ndef multiclass_roc_auc_score(y_test, y_pred, average=\"weighted\"):\n    label_binarizer = LabelBinarizer()\n    label_binarizer.fit(y_test)\n\n    truth = label_binarizer.transform(y_test)\n    pred = label_binarizer.transform(y_pred)\n    return roc_auc_score(truth, pred, average=average)\n# roc_auc_score\nprint('roc_auc_score = ', multiclass_roc_auc_score(test_y, y_pred_class))","execution_count":56,"outputs":[{"output_type":"stream","text":"roc_auc_score =  0.9393939393939394\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save('extracted_features/xception_modified_model.h5')\n# model.save_weights('extracted_features/xception_modified_weights.h5')","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score, classification_report\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n\nk_fold = KFold(n_splits=5, shuffle=True, random_state=5)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.asarray(test_labels)\ny_test = np.argmax(y_test, axis=1)\n\ny_train = np.asarray(train_labels)\ny_train = np.argmax(y_train, axis=1)","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('Bagging Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":60,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9869565217391304\nTest accuracy 0.8588235294117647\nBagging Classifier test accuracies 0.8588\n              precision    recall  f1-score   support\n\n           0       0.85      0.95      0.90        55\n           1       0.88      0.70      0.78        30\n\n    accuracy                           0.86        85\n   macro avg       0.86      0.82      0.84        85\nweighted avg       0.86      0.86      0.85        85\n\n0.8588235294117647\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nscoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"BaggingClassifier - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":61,"outputs":[{"output_type":"stream","text":"Scores Mean: 76.3889 and (STDEV 0.1328)\nBest result for fold 6\nBest accuracy is 1.0\nScores of all folds: [0.88888889 0.88888889 0.66666667 0.77777778 0.66666667 0.625\n 1.         0.625      0.625      0.875     ]\nBaggingClassifier - Test Accuracy on all folds: 0.76 (+/- 0.27)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators = 300 )\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('AdaBoost Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":62,"outputs":[{"output_type":"stream","text":"Train accuracy 1.0\nTest accuracy 0.8\nAdaBoost Classifier test accuracies 0.8000\n              precision    recall  f1-score   support\n\n           0       0.80      0.93      0.86        55\n           1       0.81      0.57      0.67        30\n\n    accuracy                           0.80        85\n   macro avg       0.80      0.75      0.76        85\nweighted avg       0.80      0.80      0.79        85\n\n0.8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(AdaBoost Classifier) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":63,"outputs":[{"output_type":"stream","text":"Scores Mean: 78.8889 and (STDEV 0.1556)\nBest result for fold 1\nBest accuracy is 1.0\nScores of all folds: [0.77777778 1.         0.66666667 1.         0.44444444 0.75\n 0.75       0.875      0.875      0.75      ]\n(AdaBoost Classifier) Test Accuracy on all folds: 0.79 (+/- 0.31)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier()\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('XGB Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":64,"outputs":[{"output_type":"stream","text":"Train accuracy 1.0\nTest accuracy 0.8705882352941177\nXGB Classifier test accuracies 0.8706\n              precision    recall  f1-score   support\n\n           0       0.85      0.96      0.91        55\n           1       0.91      0.70      0.79        30\n\n    accuracy                           0.87        85\n   macro avg       0.88      0.83      0.85        85\nweighted avg       0.88      0.87      0.87        85\n\n0.8705882352941177\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"XGB - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":65,"outputs":[{"output_type":"stream","text":"Scores Mean: 85.2778 and (STDEV 0.1216)\nBest result for fold 5\nBest accuracy is 1.0\nScores of all folds: [0.77777778 0.77777778 0.66666667 0.88888889 0.66666667 1.\n 1.         1.         0.875      0.875     ]\nXGB - Test Accuracy on all folds: 0.85 (+/- 0.24)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier()\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('DecisionTree Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":66,"outputs":[{"output_type":"stream","text":"Train accuracy 1.0\nTest accuracy 0.788235294117647\nDecisionTree Classifier test accuracies 0.7882\n              precision    recall  f1-score   support\n\n           0       0.82      0.85      0.84        55\n           1       0.71      0.67      0.69        30\n\n    accuracy                           0.79        85\n   macro avg       0.77      0.76      0.76        85\nweighted avg       0.79      0.79      0.79        85\n\n0.788235294117647\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"DecisionTree - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":67,"outputs":[{"output_type":"stream","text":"Scores Mean: 77.5000 and (STDEV 0.1356)\nBest result for fold 1\nBest accuracy is 0.8888888888888888\nScores of all folds: [0.77777778 0.88888889 0.88888889 0.88888889 0.55555556 0.5\n 0.875      0.875      0.75       0.75      ]\nDecisionTree - Test Accuracy on all folds: 0.78 (+/- 0.27)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=5)\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('RandomForest Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":68,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9804347826086957\nTest accuracy 0.8\nRandomForest Classifier test accuracies 0.8000\n              precision    recall  f1-score   support\n\n           0       0.84      0.85      0.85        55\n           1       0.72      0.70      0.71        30\n\n    accuracy                           0.80        85\n   macro avg       0.78      0.78      0.78        85\nweighted avg       0.80      0.80      0.80        85\n\n0.8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, test_labels, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(RandomForest) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":69,"outputs":[{"output_type":"stream","text":"Scores Mean: 74.3056 and (STDEV 0.1344)\nBest result for fold 1\nBest accuracy is 0.8888888888888888\nScores of all folds: [0.66666667 0.88888889 0.77777778 0.66666667 0.55555556 0.5\n 0.875      0.875      0.75       0.875     ]\n(RandomForest) Test Accuracy on all folds: 0.74 (+/- 0.27)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Light GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nlgbm=lgb.LGBMClassifier(n_estimators=1000, class_weight=\"balanced\", reg_alpha=0.1, reg_lambda=0.1, learning_rate=0.001, num_leaves=400,\n                        random_state=523, boosting='dart')\n\nlgbm_scores=cross_val_score(lgbm,train_data, y_train, cv=10)\nprint(lgbm_scores)\nprint(\"Train accuracy mean and std %.2f\" %np.mean(lgbm_scores),\"+/- %.2f\"%np.std(lgbm_scores))","execution_count":70,"outputs":[{"output_type":"stream","text":"[0.61702128 0.80851064 0.82978723 0.89361702 0.7173913  0.84782609\n 0.84444444 0.84444444 0.8        0.88888889]\nTrain accuracy mean and std 0.81 +/- 0.08\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm.fit(train_data, y_train)\ny_pred=lgbm.predict(test_data)\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":71,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.87      0.87      0.87        55\n           1       0.77      0.77      0.77        30\n\n    accuracy                           0.84        85\n   macro avg       0.82      0.82      0.82        85\nweighted avg       0.84      0.84      0.84        85\n\n0.8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(lgbm, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(LighGBM) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":72,"outputs":[{"output_type":"stream","text":"Scores Mean: 76.8056 and (STDEV 0.1284)\nBest result for fold 0\nBest accuracy is 1.0\nScores of all folds: [1.         0.66666667 0.55555556 0.66666667 0.66666667 0.875\n 0.75       0.875      0.875      0.75      ]\n(LighGBM) Test Accuracy on all folds: 0.77 (+/- 0.26)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 12})\n\nimport seaborn\nplt.style.use('seaborn-white')\n\nplt.figure()\nN = epochs\nplt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\n\nplt.legend(loc=\"center right\")","execution_count":73,"outputs":[{"output_type":"execute_result","execution_count":73,"data":{"text/plain":"<matplotlib.legend.Legend at 0x7f6633281b38>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparameters = {\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"max_depth\": [1, 2, 3, 5, 10, None], \n    \"min_samples_split\": [2, 3, 5, 10],\n    \"min_samples_leaf\": [1, 5, 10, 20]\n}\n\ntree_model = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5).fit(train_data, y_train)\nprint(accuracy_score(y_train, tree_model.predict(train_data)))\nprint(tree_model.best_score_)\n# print(accuracy_score(y_val, tree_model.predict(X_val)))\nprint(tree_model.best_params_)\nprint(tree_model.best_estimator_)","execution_count":76,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n","name":"stderr"},{"output_type":"stream","text":"0.9304347826086956\n0.8021739130434783\n{'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\nDecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=10, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}