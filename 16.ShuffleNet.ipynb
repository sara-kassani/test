{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"tensorflow Version\", tf.__version__)\n",
    "print(\"dim_ordering:\", K.image_dim_ordering())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Input, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.core import Dropout, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 2\n",
    "batch_size = 32\n",
    "img_height, img_width = 224, 224\n",
    "input_shape = (img_height, img_width, 3)\n",
    "epochs = 50\n",
    "\n",
    "nb_train_samples = 103104\n",
    "nb_validation_samples = 12288\n",
    "nb_test_samples = 697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train/'\n",
    "validation_dir = 'data/validation'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = np.random.seed(1142)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.layers import Activation, Add, Concatenate, GlobalAveragePooling2D,GlobalMaxPooling2D, Input, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization, Lambda\n",
    "from keras.applications.mobilenet import DepthwiseConv2D\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ShuffleNet(include_top=True, input_tensor=None, scale_factor=1.0, pooling='max',\n",
    "               input_shape=(224,224,3), groups=1, load_model=None, num_shuffle_units=[3, 7, 3],\n",
    "               bottleneck_ratio=0.25, classes=1000):\n",
    "    \"\"\"\n",
    "    ShuffleNet implementation for Keras 2\n",
    "    ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\n",
    "    Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun\n",
    "    https://arxiv.org/pdf/1707.01083.pdf\n",
    "    Note that only TensorFlow is supported for now, therefore it only works\n",
    "    with the data format `image_data_format='channels_last'` in your Keras\n",
    "    config at `~/.keras/keras.json`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    include_top: bool(True)\n",
    "         whether to include the fully-connected layer at the top of the network.\n",
    "    input_tensor:\n",
    "        optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n",
    "    scale_factor:\n",
    "        scales the number of output channels\n",
    "    input_shape:\n",
    "    pooling:\n",
    "        Optional pooling mode for feature extraction\n",
    "        when `include_top` is `False`.\n",
    "        - `None` means that the output of the model\n",
    "            will be the 4D tensor output of the\n",
    "            last convolutional layer.\n",
    "        - `avg` means that global average pooling\n",
    "            will be applied to the output of the\n",
    "            last convolutional layer, and thus\n",
    "            the output of the model will be a\n",
    "            2D tensor.\n",
    "        - `max` means that global max pooling will\n",
    "            be applied.\n",
    "    groups: int\n",
    "        number of groups per channel\n",
    "    num_shuffle_units: list([3,7,3])\n",
    "        number of stages (list length) and the number of shufflenet units in a\n",
    "        stage beginning with stage 2 because stage 1 is fixed\n",
    "        e.g. idx 0 contains 3 + 1 (first shuffle unit in each stage differs) shufflenet units for stage 2\n",
    "        idx 1 contains 7 + 1 Shufflenet Units for stage 3 and\n",
    "        idx 2 contains 3 + 1 Shufflenet Units\n",
    "    bottleneck_ratio:\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    classes: int(1000)\n",
    "        number of classes to predict\n",
    "    Returns\n",
    "    -------\n",
    "        A Keras model instance\n",
    "    References\n",
    "    ----------\n",
    "    - [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices]\n",
    "      (http://www.arxiv.org/pdf/1707.01083.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    if K.backend() != 'tensorflow':\n",
    "        raise RuntimeError('Only TensorFlow backend is currently supported, '\n",
    "                           'as other backends do not support ')\n",
    "\n",
    "    name = \"ShuffleNet_%.2gX_g%d_br_%.2g_%s\" % (scale_factor, groups, bottleneck_ratio, \"\".join([str(x) for x in num_shuffle_units]))\n",
    "\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=28,\n",
    "                                      require_flatten=include_top,\n",
    "                                      data_format=K.image_data_format())\n",
    "\n",
    "    out_dim_stage_two = {1: 144, 2: 200, 3: 240, 4: 272, 8: 384}\n",
    "    if groups not in out_dim_stage_two:\n",
    "        raise ValueError(\"Invalid number of groups.\")\n",
    "\n",
    "    if pooling not in ['max','avg']:\n",
    "        raise ValueError(\"Invalid value for pooling.\")\n",
    "\n",
    "    if not (float(scale_factor) * 4).is_integer():\n",
    "        raise ValueError(\"Invalid value for scale_factor. Should be x over 4.\")\n",
    "\n",
    "    exp = np.insert(np.arange(0, len(num_shuffle_units), dtype=np.float32), 0, 0)\n",
    "    out_channels_in_stage = 2 ** exp\n",
    "    out_channels_in_stage *= out_dim_stage_two[groups]  # calculate output channels for each stage\n",
    "    out_channels_in_stage[0] = 24  # first stage has always 24 output channels\n",
    "    out_channels_in_stage *= scale_factor\n",
    "    out_channels_in_stage = out_channels_in_stage.astype(int)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # create shufflenet architecture\n",
    "    x = Conv2D(filters=out_channels_in_stage[0], kernel_size=(3, 3), padding='same',\n",
    "               use_bias=False, strides=(2, 2), activation=\"relu\", name=\"conv1\")(img_input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name=\"maxpool1\")(x)\n",
    "\n",
    "    # create stages containing shufflenet units beginning at stage 2\n",
    "    for stage in range(0, len(num_shuffle_units)):\n",
    "        repeat = num_shuffle_units[stage]\n",
    "        x = _block(x, out_channels_in_stage, repeat=repeat,\n",
    "                   bottleneck_ratio=bottleneck_ratio,\n",
    "                   groups=groups, stage=stage + 2)\n",
    "\n",
    "    if pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D(name=\"global_pool\")(x)\n",
    "    elif pooling == 'max':\n",
    "        x = GlobalMaxPooling2D(name=\"global_pool\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = Dense(units=classes, name=\"fc\")(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name=name)\n",
    "\n",
    "    if load_model is not None:\n",
    "        model.load_weights('', by_name=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _block(x, channel_map, bottleneck_ratio, repeat=1, groups=1, stage=1):\n",
    "    \"\"\"\n",
    "    creates a bottleneck block containing `repeat + 1` shuffle units\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    channel_map: list\n",
    "        list containing the number of output channels for a stage\n",
    "    repeat: int(1)\n",
    "        number of repetitions for a shuffle unit with stride 1\n",
    "    groups: int(1)\n",
    "        number of groups per channel\n",
    "    bottleneck_ratio: float\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    stage: int(1)\n",
    "        stage number\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    x = _shuffle_unit(x, in_channels=channel_map[stage - 2],\n",
    "                      out_channels=channel_map[stage - 1], strides=2,\n",
    "                      groups=groups, bottleneck_ratio=bottleneck_ratio,\n",
    "                      stage=stage, block=1)\n",
    "\n",
    "    for i in range(1, repeat + 1):\n",
    "        x = _shuffle_unit(x, in_channels=channel_map[stage - 1],\n",
    "                          out_channels=channel_map[stage - 1], strides=1,\n",
    "                          groups=groups, bottleneck_ratio=bottleneck_ratio,\n",
    "                          stage=stage, block=(i + 1))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _shuffle_unit(inputs, in_channels, out_channels, groups, bottleneck_ratio, strides=2, stage=1, block=1):\n",
    "    \"\"\"\n",
    "    creates a shuffleunit\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    in_channels:\n",
    "        number of input channels\n",
    "    out_channels:\n",
    "        number of output channels\n",
    "    strides:\n",
    "        An integer or tuple/list of 2 integers,\n",
    "        specifying the strides of the convolution along the width and height.\n",
    "    groups: int(1)\n",
    "        number of groups per channel\n",
    "    bottleneck_ratio: float\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    stage: int(1)\n",
    "        stage number\n",
    "    block: int(1)\n",
    "        block number\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = -1\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    prefix = 'stage%d/block%d' % (stage, block)\n",
    "\n",
    "    #if strides >= 2:\n",
    "        #out_channels -= in_channels\n",
    "\n",
    "    # default: 1/4 of the output channel of a ShuffleNet Unit\n",
    "    bottleneck_channels = int(out_channels * bottleneck_ratio)\n",
    "    groups = (1 if stage == 2 and block == 1 else groups)\n",
    "\n",
    "    x = _group_conv(inputs, in_channels, out_channels=bottleneck_channels,\n",
    "                    groups=(1 if stage == 2 and block == 1 else groups),\n",
    "                    name='%s/1x1_gconv_1' % prefix)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_gconv_1' % prefix)(x)\n",
    "    x = Activation('relu', name='%s/relu_gconv_1' % prefix)(x)\n",
    "\n",
    "    x = Lambda(channel_shuffle, arguments={'groups': groups}, name='%s/channel_shuffle' % prefix)(x)\n",
    "    x = DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", use_bias=False,\n",
    "                        strides=strides, name='%s/1x1_dwconv_1' % prefix)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_dwconv_1' % prefix)(x)\n",
    "\n",
    "    x = _group_conv(x, bottleneck_channels, out_channels=out_channels if strides == 1 else out_channels - in_channels,\n",
    "                    groups=groups, name='%s/1x1_gconv_2' % prefix)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_gconv_2' % prefix)(x)\n",
    "\n",
    "    if strides < 2:\n",
    "        ret = Add(name='%s/add' % prefix)([x, inputs])\n",
    "    else:\n",
    "        avg = AveragePooling2D(pool_size=3, strides=2, padding='same', name='%s/avg_pool' % prefix)(inputs)\n",
    "        ret = Concatenate(bn_axis, name='%s/concat' % prefix)([x, avg])\n",
    "\n",
    "    ret = Activation('relu', name='%s/relu_out' % prefix)(ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _group_conv(x, in_channels, out_channels, groups, kernel=1, stride=1, name=''):\n",
    "    \"\"\"\n",
    "    grouped convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    in_channels:\n",
    "        number of input channels\n",
    "    out_channels:\n",
    "        number of output channels\n",
    "    groups:\n",
    "        number of groups per channel\n",
    "    kernel: int(1)\n",
    "        An integer or tuple/list of 2 integers, specifying the\n",
    "        width and height of the 2D convolution window.\n",
    "        Can be a single integer to specify the same value for\n",
    "        all spatial dimensions.\n",
    "    stride: int(1)\n",
    "        An integer or tuple/list of 2 integers,\n",
    "        specifying the strides of the convolution along the width and height.\n",
    "        Can be a single integer to specify the same value for all spatial dimensions.\n",
    "    name: str\n",
    "        A string to specifies the layer name\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    if groups == 1:\n",
    "        return Conv2D(filters=out_channels, kernel_size=kernel, padding='same',\n",
    "                      use_bias=False, strides=stride, name=name)(x)\n",
    "\n",
    "    # number of intput channels per group\n",
    "    ig = in_channels // groups\n",
    "    group_list = []\n",
    "\n",
    "    assert out_channels % groups == 0\n",
    "\n",
    "    for i in range(groups):\n",
    "        offset = i * ig\n",
    "        group = Lambda(lambda z: z[:, :, :, offset: offset + ig], name='%s/g%d_slice' % (name, i))(x)\n",
    "        group_list.append(Conv2D(int(0.5 + out_channels / groups), kernel_size=kernel, strides=stride,\n",
    "                                 use_bias=False, padding='same', name='%s_/g%d' % (name, i))(group))\n",
    "    return Concatenate(name='%s/concat' % name)(group_list)\n",
    "\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    groups: int\n",
    "        number of groups per channel\n",
    "    Returns\n",
    "    -------\n",
    "        channel shuffled output tensor\n",
    "    Examples\n",
    "    --------\n",
    "    Example for a 1D Array with 3 groups\n",
    "    >>> d = np.array([0,1,2,3,4,5,6,7,8])\n",
    "    >>> x = np.reshape(d, (3,3))\n",
    "    >>> x = np.transpose(x, [1,0])\n",
    "    >>> x = np.reshape(x, (9,))\n",
    "    '[0 1 2 3 4 5 6 7 8] --> [0 3 6 1 4 7 2 5 8]'\n",
    "    \"\"\"\n",
    "    height, width, in_channels = x.shape.as_list()[1:]\n",
    "    channels_per_group = in_channels // groups\n",
    "\n",
    "    x = K.reshape(x, [-1, height, width, groups, channels_per_group])\n",
    "    x = K.permute_dimensions(x, (0, 1, 2, 4, 3))  # transpose\n",
    "    x = K.reshape(x, [-1, height, width, in_channels])\n",
    "\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ShuffleNet(include_top=False, input_shape=(224, 224, 3), classes=2)\n",
    "x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "prediction = Dense(output_classes, activation=tf.nn.softmax)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd_opt = SGD(lr = 0.02, decay=75e-6, momentum=0.9, nesterov=True)\n",
    "# adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)\n",
    "adam_opt=Adam(lr = 0.0001, beta_1=0.6, beta_2=0.99, epsilon=None, decay=0.0, amsgrad=True)\n",
    "sgd_opt = SGD(lr=1e-06, momentum=0.0, decay=0.0, nesterov=False)\n",
    "rmsp_opt = RMSprop(lr=1e-4, decay=0.9)\n",
    "\n",
    "model.compile(optimizer= adam_opt, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, verbose = 1)]\n",
    "\n",
    "history = model.fit_generator(\n",
    "  train_generator,\n",
    "  steps_per_epoch = nb_train_samples // batch_size,\n",
    "  epochs = epochs,\n",
    "  validation_data = validation_generator,\n",
    "  validation_steps = nb_validation_samples // batch_size,\n",
    "  callbacks = callbacks)\n",
    "\n",
    "# with open('models/vgg19_history.txt','w') as f:\n",
    "#     f.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Acc', 'Test Acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Test Loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# N = epochs\n",
    "# plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Validation Score: ', score[0])\n",
    "print ('Validation Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_generator.filenames\n",
    "truth = test_generator.classes\n",
    "label = test_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "predicts = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "#     fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "#     plt.savefig('plots/3.InceptionV3-2-Private-DataSet-CM.png', bbox_inches='tight', dpi = 100) \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm, classes=labels, title=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = predicts\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = test_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in test_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy = (cm[0,0]+cm[1,1]) / total\n",
    "print ('Accuracy : ', accuracy*100)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "print('Sensitivity : ', sensitivity*100 )\n",
    "\n",
    "Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print('Specificity : ', Specificity*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "th = 0.3\n",
    "\n",
    "acc = accuracy_score(truth,predict_class > th)\n",
    "prec = precision_score(truth,predict_class > th)\n",
    "f1 = f1_score(truth,predict_class > th)\n",
    "recall = recall_score(truth,predict_class > th)\n",
    "\n",
    "print('Accuracy:  {:.4f}'.format(acc*100))\n",
    "print('Precision: {:.4f}'.format(prec*100))\n",
    "print('Recall:    {:.4f}'.format(recall*100))\n",
    "print('F1:        {:.4f}'.format(f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_generator.classes, predict_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 1\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#plotting sensitivity and specificity\n",
    "plt.figure()\n",
    "plt.plot(thresholds, 1-fpr, label = 'specificity')\n",
    "plt.plot(thresholds, tpr, label = 'sensitivity')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/14.MobileNetV2-BreaKHis-Model.h5')\n",
    "model.save_weights('models/14.MobileNetV2-BreaKHis-Weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt2=Adam(lr=1e-05, beta_1=0.6, beta_2=0.9, epsilon=None, decay=0.0, amsgrad=True)\n",
    "model.compile(optimizer= adam_opt2, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, verbose = 1)]\n",
    "\n",
    "history = model.fit_generator(\n",
    "  train_generator,\n",
    "  steps_per_epoch = nb_train_samples // batch_size,\n",
    "  epochs = epochs,\n",
    "  validation_data = validation_generator,\n",
    "  validation_steps = nb_validation_samples // batch_size,\n",
    "  callbacks = callbacks)\n",
    "\n",
    "# with open('models/vgg19_history2.txt','w') as f:\n",
    "#     f.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Acc', 'Test Acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Test Loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# N = 12\n",
    "# plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Validation Score: ', score[0])\n",
    "print ('Validation Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_generator.filenames\n",
    "truth = test_generator.classes\n",
    "label = test_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "predicts = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "#     fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "#     plt.savefig('plots/3.InceptionV3-2-Private-DataSet-CM.png', bbox_inches='tight', dpi = 100) \n",
    "# fig.savefig('plots/1.Xception-CM.png') \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plot_confusion_matrix(cm, classes=labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "y_pred = predicts\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = test_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in test_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy = (cm[0,0]+cm[1,1]) / total\n",
    "print ('Accuracy : ', accuracy*100)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "print('Sensitivity : ', sensitivity*100 )\n",
    "\n",
    "Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print('Specificity : ', Specificity*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "th = 0.3\n",
    "\n",
    "acc = accuracy_score(truth,predict_class > th)\n",
    "prec = precision_score(truth,predict_class > th)\n",
    "f1 = f1_score(truth,predict_class > th)\n",
    "recall = recall_score(truth,predict_class > th)\n",
    "\n",
    "print('Accuracy:  {:.4f}'.format(acc*100))\n",
    "print('Precision: {:.4f}'.format(prec*100))\n",
    "print('Recall:    {:.4f}'.format(recall*100))\n",
    "print('F1:        {:.4f}'.format(f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_generator.classes, predict_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 1\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#plotting sensitivity and specificity\n",
    "plt.figure()\n",
    "plt.plot(thresholds, 1-fpr, label = 'specificity')\n",
    "plt.plot(thresholds, tpr, label = 'sensitivity')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/14.MobileNetV2-2-BreaKHis-Model.h5')\n",
    "model.save_weights('models/14.MobileNetV2-2-BreaKHis-Weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
