{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, partial, rand, space_eval\n",
    "from sklearn.metrics import log_loss\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "# import keras.callbacks as kcall\n",
    "from keras.optimizers import Adam, RMSprop,SGD\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, concatenate\n",
    "from keras import optimizers, metrics, models\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"tensorflow Version\", tf.__version__)\n",
    "# print(\"dim_ordering:\", K.image_dim_ordering())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_height, img_width = 380, 380\n",
    "input_shape = (img_height, img_width, 3)\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input/isbi19-data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../input/isbi19-data/train/'\n",
    "test_dir = '../input/isbi19-data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[:, :, ::-1]\n",
    "    # Zero-center by imagenet mean pixel\n",
    "    x[:, :, 0] -= 103.939\n",
    "    x[:, :, 1] -= 116.779\n",
    "    x[:, :, 2] -= 123.68\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = np.random.seed(1142)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "    preprocessing_function = preprocess_input,\n",
    "    validation_split= 0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    subset = 'training',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    subset = 'validation',\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255, preprocessing_function = preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n",
    "predict_size_test = int(math.ceil(nb_test_samples / batch_size))\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "print(\"nb_train_samples:\", nb_train_samples)\n",
    "print(\"nb_validation_samples:\", nb_validation_samples)\n",
    "print(\"nb_test_samples:\", nb_test_samples)\n",
    "\n",
    "print(\"\\npredict_size_train:\", predict_size_train)\n",
    "print(\"predict_size_validation:\", predict_size_validation)\n",
    "print(\"predict_size_test:\", predict_size_test)\n",
    "\n",
    "print(\"\\n num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"extracted_features\")\n",
    "extracted_features_dir = \"extracted_features/\"\n",
    "model_name = \"VGG19_descriptors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_weights =\"../input/full-keras-pretrained-no-top/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "inception_weights =\"../input/full-keras-pretrained-no-top//inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "vgg16_weights =\"../input/full-keras-pretrained-no-top/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "denseNet201_weights =\"../input/full-keras-pretrained-no-top/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "denseNet121_weights =\"../input/full-keras-pretrained-no-top/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "resenet50_weights =\"../input/full-keras-pretrained-no-top/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "inception_resnet_v2_weights =\"../input/full-keras-pretrained-no-top/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "nasnet_weights =\"../input/full-keras-pretrained-no-top/nasnet_large_no_top.h5\"\n",
    "nasnet_mobile_weights =\"../input/full-keras-pretrained-no-top/nasnet_mobile_no_top.h5\"\n",
    "mobilenet_weights =\"../input/full-keras-pretrained-no-top/mobilenet_1_0_224_tf_no_top.h5\"\n",
    "xception_weights = \"../input/full-keras-pretrained-no-top/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications import DenseNet201\n",
    "from keras.applications import DenseNet121\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications import NASNetLarge, NASNetMobile\n",
    "from keras.applications import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape = input_shape)  \n",
    "\n",
    "base_model1=DenseNet121(weights=denseNet121_weights, include_top=False, pooling = \"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, layer in enumerate(model.layers):\n",
    "#     print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_final_model = base_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = bottleneck_final_model.predict_generator(train_generator, predict_size_train, max_q_size=1, pickle_safe=False)\n",
    "np.save(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_validation = bottleneck_final_model.predict_generator(validation_generator, predict_size_validation)\n",
    "np.save(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_test = bottleneck_final_model.predict_generator(test_generator, predict_size_test)\n",
    "np.save(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import get_session\n",
    "# from keras.backend.tensorflow_backend import clear_session\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "# def reset_keras_tf_session():\n",
    "#     \"\"\"\n",
    "#     this function clears the gpu memory and set the \n",
    "#     tf session to not use the whole gpu\n",
    "#     \"\"\"\n",
    "#     sess = get_session()\n",
    "#     clear_session()\n",
    "#     sess.close()\n",
    "#     sess = get_session()\n",
    "\n",
    "# #     config = tf.ConfigProto()\n",
    "# #     config.gpu_options.allow_growth = True\n",
    "# #     set_session(tf.Session(config=config))\n",
    "\n",
    "# reset_keras_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy')\n",
    "validation_data = np.load(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy')\n",
    "test_data = np.load(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy')\n",
    "\n",
    "train_labels = train_generator.classes\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "\n",
    "validation_labels = validation_generator.classes\n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "test_labels = test_generator.classes\n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "         'lr': hp.choice('lr',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n",
    "#          'dropout': hp.choice('dropout', [0.4, 0.5, 0.6, 0.7]),\n",
    "#          'batch_size': hp.choice('batch_size', [64]),\n",
    "#          'epochs': hp.choice('epochs', [15, 20, 25, 30, 50]),\n",
    "#          'optimizer': hp.choice('optimizer',['sgd','adam','rmsprop']),\n",
    "#          'optimizer': hp.choice('optimizer',['rmsprop']),\n",
    "#          'optimizer': hp.choice('optimizer',['adam']),\n",
    "#          'beta_1':hp.choice('beta_1',[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8, 0.9]),\n",
    "#          'beta_2':hp.choice('beta_2',[0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.999,0.99,0.995]),\n",
    "#          'momentum':hp.choice('momentum',[0.3,0.5,0.7,0.9,1]),\n",
    "#          'amsgrad':hp.choice('amsgrad',[False,True]),\n",
    "#          'nesterov':hp.choice('nesterov',[False,True]),\n",
    "        'rho':hp.choice('rho',[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8, 0.9]),\n",
    "        'hidden1':hp.choice('hidden1',[4096,2048,1024,512,256]),\n",
    "        'hidden2':hp.choice('hidden2',[2048,1024,512,256,128,]),\n",
    "        'hidden3':hp.choice('hidden3',[512,256,128,64,32]),    \n",
    "        'bias_reg': hp.choice('bias_reg',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n",
    "        'act_reg': hp.choice('act_reg',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n",
    "        'ker_reg': hp.choice('ker_reg',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n",
    "        'activation_function':hp.choice('activation_function',[\"relu\",\"elu\",\"selu\",\"softplus\",\"tanh\",])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def f_nn(params):   \n",
    "    print ('Parameters testing: ', params)\n",
    "#     dropout_rate = 0.5\n",
    "#     adam_opt=Adam(lr=params[\"lr\"], beta_1=params[\"beta_1\"], beta_2=params['beta_2'])\n",
    "#     sgd=SGD(lr=params[\"lr\"], momentum=params['momentum'], decay=0.0, nesterov=params['nesterov'])\n",
    "    rmsprop_opt=RMSprop(lr=params[\"lr\"], rho=params['rho'])\n",
    "\n",
    "    model = Sequential()\n",
    "    # model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(params[\"hidden1\"], activation=params[\"activation_function\"], kernel_regularizer=l2(params[\"ker_reg\"]), bias_regularizer=l2(params[\"bias_reg\"]), activity_regularizer=l1(params[\"act_reg\"])))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(params[\"hidden2\"], activation=params[\"activation_function\"], kernel_regularizer=l2(params[\"ker_reg\"]), bias_regularizer=l2(params[\"bias_reg\"]), activity_regularizer=l1(params[\"act_reg\"])))\n",
    "   \n",
    "    model.add(Dense(params[\"hidden3\"], activation=params[\"activation_function\"], kernel_regularizer=l2(params[\"ker_reg\"]), bias_regularizer=l2(params[\"bias_reg\"]), activity_regularizer=l1(params[\"act_reg\"])))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=rmsprop_opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(train_data, train_labels,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(validation_data, validation_labels),\n",
    "                        verbose= 2,\n",
    "                        callbacks=get_callbacks(params))\n",
    "\n",
    "    (eval_loss, eval_accuracy) = model.evaluate(validation_data, validation_labels, batch_size= batch_size, verbose=1)\n",
    "\n",
    "    print(\"Validation Accuracy: {:.4f}%\".format(eval_accuracy * 100))\n",
    "    print(\"Validation Loss: {}\".format(eval_loss))\n",
    "\n",
    "    print(\"*_*\" * 50)\n",
    "#     best_epoch = np.argmax(history.history['val_acc'])\n",
    "#     best_val_acc = np.max(history.history['val_acc'])\n",
    "#     print('Epoch {} - val acc: {}'.format(best_epoch, best_val_acc))\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "    return {'loss': eval_loss, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(params):\n",
    "    callbacks =[EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(f_nn, space, algo=tpe.suggest, max_evals = 300, trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Done!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
