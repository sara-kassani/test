{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, TensorBoard, CSVLogger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "validation_dir = 'data/validation'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train 0\n",
      "data/train/benign 981\n",
      "data/train/malignant 207\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk(train_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/validation 0\n",
      "data/validation/benign 117\n",
      "data/validation/malignant 27\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk(validation_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/test 0\n",
      "data/test/benign 117\n",
      "data/test/malignant 27\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk(test_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 2\n",
    "batch_size = 32 \n",
    "img_height, img_width = 256, 256\n",
    "\n",
    "nb_train_samples = 1188\n",
    "nb_validation_samples = 144\n",
    "nb_test_samples = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1188 images belonging to 2 classes.\n",
      "Found 144 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < 15:\n",
    "        return .01\n",
    "    elif epoch < 28:\n",
    "        return .002\n",
    "    else:\n",
    "        return .0004\n",
    "\n",
    "jobs_base_dir = 'jobs'\n",
    "job_name = 'Xception_job'\n",
    "model_name = 'Xception'\n",
    "job_path = \"{}/{}\".format(jobs_base_dir, job_name)\n",
    "tensorboard_dir = \"{}/{}\".format(job_path, \"tensorboard\")\n",
    "  \n",
    "weights_path = \"{}/{}\".format(job_path, \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger(\"{}/{}.log\".format(job_path, model_name))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "tensorboard = TensorBoard(log_dir=\"{}\".format(tensorboard_dir), histogram_freq=0, batch_size=32, write_graph=True,\n",
    "                          write_grads=False,\n",
    "                          write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "droput_rate= 0.5\n",
    "adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)\n",
    "\n",
    "resnet50_weights = 'pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = (ResNet50(include_top=False, pooling='avg', weights=resnet50_weights))\n",
    "x = model.output\n",
    "# x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "model = Model(input = model.input, output = predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= adam_opt,\n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 850s 23s/step - loss: 1.8523 - acc: 0.7230 - val_loss: 4.1222 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.12216, saving model to jobs/Xception_job/weights.01-4.12.hdf5\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 835s 23s/step - loss: 1.7101 - acc: 0.7601 - val_loss: 3.5040 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.12216 to 3.50404, saving model to jobs/Xception_job/weights.02-3.50.hdf5\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 851s 23s/step - loss: 1.5920 - acc: 0.7914 - val_loss: 4.1766 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.50404\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 811s 22s/step - loss: 1.4986 - acc: 0.7655 - val_loss: 2.3127 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.50404 to 2.31273, saving model to jobs/Xception_job/weights.04-2.31.hdf5\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 829s 22s/step - loss: 1.4239 - acc: 0.7789 - val_loss: 1.9117 - val_acc: 0.4861\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.31273 to 1.91174, saving model to jobs/Xception_job/weights.05-1.91.hdf5\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 850s 23s/step - loss: 1.2140 - acc: 0.7973 - val_loss: 1.5497 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.91174 to 1.54974, saving model to jobs/Xception_job/weights.06-1.55.hdf5\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 809s 22s/step - loss: 1.1987 - acc: 0.8082 - val_loss: 1.3425 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.54974 to 1.34247, saving model to jobs/Xception_job/weights.07-1.34.hdf5\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 831s 22s/step - loss: 1.3257 - acc: 0.7655 - val_loss: 1.0662 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.34247 to 1.06618, saving model to jobs/Xception_job/weights.08-1.07.hdf5\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 828s 22s/step - loss: 1.2663 - acc: 0.7627 - val_loss: 1.6530 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.06618\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 834s 23s/step - loss: 1.3626 - acc: 0.7466 - val_loss: 1.0493 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.06618 to 1.04925, saving model to jobs/Xception_job/weights.10-1.05.hdf5\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 849s 23s/step - loss: 1.1347 - acc: 0.8125 - val_loss: 1.1040 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.04925\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 837s 23s/step - loss: 1.1205 - acc: 0.7780 - val_loss: 0.9884 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.04925 to 0.98841, saving model to jobs/Xception_job/weights.12-0.99.hdf5\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 854s 23s/step - loss: 1.0798 - acc: 0.8167 - val_loss: 1.0409 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.98841\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 814s 22s/step - loss: 1.1271 - acc: 0.7924 - val_loss: 1.1343 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.98841\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 833s 23s/step - loss: 1.3716 - acc: 0.7598 - val_loss: 1.1544 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.98841\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 834s 23s/step - loss: 1.3591 - acc: 0.7900 - val_loss: 1.0596 - val_acc: 0.7986\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.98841\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 853s 23s/step - loss: 1.1452 - acc: 0.8133 - val_loss: 1.0491 - val_acc: 0.7986\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.98841\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 814s 22s/step - loss: 1.1193 - acc: 0.8099 - val_loss: 0.9836 - val_acc: 0.7986\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.98841 to 0.98364, saving model to jobs/Xception_job/weights.18-0.98.hdf5\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 833s 23s/step - loss: 1.0481 - acc: 0.8208 - val_loss: 0.9381 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.98364 to 0.93807, saving model to jobs/Xception_job/weights.19-0.94.hdf5\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 832s 22s/step - loss: 1.0011 - acc: 0.8149 - val_loss: 0.9105 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.93807 to 0.91047, saving model to jobs/Xception_job/weights.20-0.91.hdf5\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 832s 22s/step - loss: 1.0658 - acc: 0.7926 - val_loss: 0.8904 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.91047 to 0.89045, saving model to jobs/Xception_job/weights.21-0.89.hdf5\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 830s 22s/step - loss: 0.9595 - acc: 0.8117 - val_loss: 0.8552 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.89045 to 0.85517, saving model to jobs/Xception_job/weights.22-0.86.hdf5\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 848s 23s/step - loss: 0.9033 - acc: 0.8328 - val_loss: 0.8604 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.85517\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 810s 22s/step - loss: 0.9113 - acc: 0.7982 - val_loss: 0.8537 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.85517 to 0.85374, saving model to jobs/Xception_job/weights.24-0.85.hdf5\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 827s 22s/step - loss: 0.8900 - acc: 0.8151 - val_loss: 0.8285 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.85374 to 0.82848, saving model to jobs/Xception_job/weights.25-0.83.hdf5\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 847s 23s/step - loss: 0.8690 - acc: 0.8226 - val_loss: 0.8216 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.82848 to 0.82155, saving model to jobs/Xception_job/weights.26-0.82.hdf5\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 815s 22s/step - loss: 0.8813 - acc: 0.8217 - val_loss: 0.8151 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.82155 to 0.81510, saving model to jobs/Xception_job/weights.27-0.82.hdf5\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 861s 23s/step - loss: 0.8941 - acc: 0.8074 - val_loss: 0.7978 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.81510 to 0.79778, saving model to jobs/Xception_job/weights.28-0.80.hdf5\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 829s 22s/step - loss: 0.8327 - acc: 0.8151 - val_loss: 0.7880 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.79778 to 0.78803, saving model to jobs/Xception_job/weights.29-0.79.hdf5\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 827s 22s/step - loss: 0.8256 - acc: 0.8276 - val_loss: 0.7871 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.78803 to 0.78708, saving model to jobs/Xception_job/weights.30-0.79.hdf5\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 825s 22s/step - loss: 0.8124 - acc: 0.8069 - val_loss: 0.7755 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.78708 to 0.77554, saving model to jobs/Xception_job/weights.31-0.78.hdf5\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 825s 22s/step - loss: 0.8210 - acc: 0.8034 - val_loss: 0.7697 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.77554 to 0.76974, saving model to jobs/Xception_job/weights.32-0.77.hdf5\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 808s 22s/step - loss: 0.8140 - acc: 0.8236 - val_loss: 0.7716 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.76974\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 827s 22s/step - loss: 0.7957 - acc: 0.8159 - val_loss: 0.7697 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.76974 to 0.76974, saving model to jobs/Xception_job/weights.34-0.77.hdf5\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 846s 23s/step - loss: 0.7867 - acc: 0.8243 - val_loss: 0.7680 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.76974 to 0.76802, saving model to jobs/Xception_job/weights.35-0.77.hdf5\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 825s 22s/step - loss: 0.8007 - acc: 0.8159 - val_loss: 0.7573 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.76802 to 0.75726, saving model to jobs/Xception_job/weights.36-0.76.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "37/37 [==============================] - 826s 22s/step - loss: 0.7954 - acc: 0.8208 - val_loss: 0.7572 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.75726 to 0.75723, saving model to jobs/Xception_job/weights.37-0.76.hdf5\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 825s 22s/step - loss: 0.7822 - acc: 0.8185 - val_loss: 0.7581 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.75723\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 827s 22s/step - loss: 0.7653 - acc: 0.8093 - val_loss: 0.7572 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.75723 to 0.75720, saving model to jobs/Xception_job/weights.39-0.76.hdf5\n",
      "Epoch 40/50\n",
      "31/37 [========================>.....] - ETA: 2:07 - loss: 0.7576 - acc: 0.8165"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs = 50,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[lr_scheduler, csv_logger, checkpointer, tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Acc', 'Test Acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Test Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = epochs\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"categorical_accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_categorical_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Test Score: ', score[0])\n",
    "print ('Test Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = validation_generator.filenames\n",
    "truth = validation_generator.classes\n",
    "label = validation_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "predicts = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "#     fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig('plots/1.Xception-CM.png', bbox_inches='tight', dpi = 100) \n",
    "# fig.savefig('plots/1.Xception-CM.png') \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm, classes=labels,\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve for each class\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "y_pred = predicts\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = validation_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in validation_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "y_actual_binary = label_binarize(y_actual, classes=[0, 1, 2, 3, 4,5,6])\n",
    "y_pred_binary = y_pred_probabilities#label_binarize(y_pred_probabilities, classes=[0, 1, 2, 3, 4])\n",
    "n_classes=7\n",
    "lw=2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_actual_binary[:, i], y_pred_binary[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_actual_binary.ravel(), y_pred_binary.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "\n",
    "colors = cycle(['red','blue','green','yellow','orange', 'aqua', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(classnames[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=2)\n",
    "\n",
    "\n",
    "colors = cycle(['red','blue','green','yellow','orange', 'aqua', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(classnames[i], roc_auc[i]))\n",
    "\n",
    "    \n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-classs')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/1.Xception-ROC.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/2.ResNet50-Thyroid-Binary-Classification-Model.h5')\n",
    "model.save_weights('models/2.ResNet50-Thyroid-Binary-Classification-Weights.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
