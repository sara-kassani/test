{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"tensorflow Version\", tf.__version__)\n",
    "print(\"dim_ordering:\", K.image_dim_ordering())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Input, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.core import Dropout, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 2\n",
    "batch_size = 32\n",
    "img_height, img_width = 224, 224\n",
    "input_shape = (img_height, img_width, 3)\n",
    "epochs = 50\n",
    "\n",
    "nb_train_samples = 103104\n",
    "nb_validation_samples = 12288\n",
    "nb_test_samples = 697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train/'\n",
    "validation_dir = 'data/validation'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = np.random.seed(1142)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Multiply,GlobalAveragePooling2D, Add, Dense, Activation\n",
    "from keras.layers import ZeroPadding2D, BatchNormalization, Flatten\n",
    "from keras.layers import Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Lambda\n",
    "import tensorflow as tf\n",
    "from keras.initializers import glorot_uniform\n",
    "import os\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "\n",
    "\n",
    "def res_conv(X, filters, base, s):\n",
    "    \n",
    "    name_base = base + '_branch'\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    ##### Branch1 is the main path and Branch2 is the shortcut path #####\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    ##### Branch1 #####\n",
    "    # First component of Branch1 \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '1_bn_1')(X)\n",
    "    X= Activation('relu', name=name_base + '1_relu_1')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '1_conv_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "    # Second component of Branch1\n",
    "    X = BatchNormalization(axis=-1, name=name_base + '1_bn_2')(X)\n",
    "    X = Activation('relu', name=name_base + '1_relu_2')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(3,3), strides=(s,s), padding='same', name=name_base + '1_conv_2', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # Third component of Branch1\n",
    "    X = BatchNormalization(axis=-1, name=name_base + '1_bn_3')(X)\n",
    "    X = Activation('relu', name=name_base + '1_relu_3')(X)\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '1_conv_3', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    ##### Branch2 ####\n",
    "    X_shortcut = BatchNormalization(axis=-1, name=name_base + '2_bn_1')(X_shortcut)\n",
    "    X_shortcut= Activation('relu', name=name_base + '2_relu_1')(X_shortcut)\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1,1), strides=(s,s), padding='valid', name=name_base + '2_conv_1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    \n",
    "    # Final step: Add Branch1 and Branch2\n",
    "    X = Add(name=base + '_Add')([X, X_shortcut])\n",
    "\n",
    "    return X\n",
    "\n",
    "def res_identity(X, filters, base):\n",
    "    \n",
    "    name_base = base + '_branch'\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    ##### Branch1 is the main path and Branch2 is the shortcut path #####\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    ##### Branch1 #####\n",
    "    # First component of Branch1 \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '1_bn_1')(X)\n",
    "    Shortcut= Activation('relu', name=name_base + '1_relu_1')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '1_conv_1', kernel_initializer=glorot_uniform(seed=0))(Shortcut)\n",
    "\n",
    "    # Second component of Branch1\n",
    "    X = BatchNormalization(axis=-1, name=name_base + '1_bn_2')(X)\n",
    "    X = Activation('relu', name=name_base + '1_relu_2')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(3,3), strides=(1,1), padding='same', name=name_base + '1_conv_2', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # Third component of Branch1\n",
    "    X = BatchNormalization(axis=-1, name=name_base + '1_bn_3')(X)\n",
    "    X = Activation('relu', name=name_base + '1_relu_3')(X)\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '1_conv_3', kernel_initializer=glorot_uniform(seed=0))(X)    \n",
    "    \n",
    "    # Final step: Add Branch1 and the original Input itself\n",
    "    X = Add(name=base + '_Add')([X, X_shortcut])\n",
    "\n",
    "    return X\n",
    "\n",
    "def Trunk_block(X, F, base):\n",
    "    \n",
    "    name_base = base\n",
    "    \n",
    "    X = res_identity(X, F, name_base + '_Residual_id_1')\n",
    "    X = res_identity(X, F, name_base + '_Residual_id_2')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def interpolation(input_tensor, ref_tensor,name): # resizes input_tensor wrt. ref_tensor\n",
    "    H, W = ref_tensor.get_shape()[1], ref_tensor.get_shape()[2]\n",
    "\n",
    "    return tf.image.resize_nearest_neighbor(input_tensor, [H.value, W.value],name=name)\n",
    "\n",
    "def Attention_1(X, filters, base):\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    name_base = base\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Pre_Residual_id')\n",
    "    \n",
    "    X_Trunk = Trunk_block(X, filters, name_base+ '_Trunk')\n",
    "    \n",
    "    X = MaxPooling2D((3,3), strides=(2,2), padding='same', name=name_base+ '_Mask_pool_3')(X)\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_3_Down')\n",
    "    \n",
    "    Residual_id_3_Down_shortcut = X\n",
    "    \n",
    "    Residual_id_3_Down_branched = res_identity(X, filters, name_base+ '_Mask_Residual_id_3_Down_branched')\n",
    "    \n",
    "    X = MaxPooling2D((3,3), strides=(2,2), padding='same', name=name_base+ '_Mask_pool_2')(X)\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_2_Down')\n",
    "    \n",
    "    Residual_id_2_Down_shortcut = X\n",
    "    \n",
    "    Residual_id_2_Down_branched = res_identity(X, filters, name_base+ '_Mask_Residual_id_2_Down_branched')\n",
    "    \n",
    "    X = MaxPooling2D((3,3), strides=(2,2), padding='same', name=name_base+ '_Mask_pool_1')(X)\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_1_Down')\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_1_Up')\n",
    "    \n",
    "    temp_name1 = name_base+ \"_Mask_Interpool_1\"\n",
    "    \n",
    "    X = Lambda(interpolation, arguments={'ref_tensor': Residual_id_2_Down_shortcut,'name':temp_name1})(X)\n",
    "                                          \n",
    "    X = Add(name=base + '_Mask_Add_after_Interpool_1')([X, Residual_id_2_Down_branched])\n",
    "                                          \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_2_Up')\n",
    "    \n",
    "    temp_name2 = name_base+ \"_Mask_Interpool_2\"\n",
    "    \n",
    "    X = Lambda(interpolation, arguments={'ref_tensor': Residual_id_3_Down_shortcut,'name':temp_name2})(X)\n",
    "                                          \n",
    "    X = Add(name=base + '_Mask_Add_after_Interpool_2')([X, Residual_id_3_Down_branched])\n",
    "                                          \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_3_Up')\n",
    "    \n",
    "    temp_name3 = name_base+ \"_Mask_Interpool_3\"\n",
    "    \n",
    "    X = Lambda(interpolation, arguments={'ref_tensor': X_Trunk,'name':temp_name3})(X)\n",
    "                                          \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '_Mask_Interpool_3_bn_1')(X)\n",
    "                                          \n",
    "    X = Activation('relu', name=name_base + '_Mask_Interpool_3_relu_1')(X)\n",
    "                                          \n",
    "    X = Conv2D(F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '_Mask_Interpool_3_conv_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '_Mask_Interpool_3_bn_2')(X)\n",
    "                                          \n",
    "    X = Activation('relu', name=name_base + '_Mask_Interpool_3_relu_2')(X)\n",
    "                                          \n",
    "    X = Conv2D(F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '_Mask_Interpool_3_conv_2', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    X = Activation('sigmoid', name=name_base+'_Mask_sigmoid')(X)\n",
    "      \n",
    "    X = Multiply(name=name_base+'_Mutiply')([X_Trunk,X])\n",
    "    \n",
    "    X = Add(name=name_base+'_Add')([X_Trunk,X])\n",
    "\n",
    "    X = res_identity(X, filters, name_base+ '_Post_Residual_id')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def Attention_2(X, filters, base):\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    name_base = base\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Pre_Residual_id')\n",
    "    \n",
    "    X_Trunk = Trunk_block(X, filters, name_base+ '_Trunk')\n",
    "    \n",
    "    X = MaxPooling2D((3,3), strides=(2,2), padding='same', name=name_base+ '_Mask_pool_2')(X)\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_2_Down')\n",
    "    \n",
    "    Residual_id_2_Down_shortcut = X\n",
    "    \n",
    "    Residual_id_2_Down_branched = res_identity(X, filters, name_base+ '_Mask_Residual_id_2_Down_branched')\n",
    "    \n",
    "    X = MaxPooling2D((3,3), strides=(2,2), padding='same', name=name_base+ '_Mask_pool_1')(X)\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_1_Down')\n",
    "                                          \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_1_Up')\n",
    "    \n",
    "    temp_name1 = name_base+ \"_Mask_Interpool_1\"\n",
    "    \n",
    "    X = Lambda(interpolation, arguments={'ref_tensor': Residual_id_2_Down_shortcut,'name':temp_name1})(X)\n",
    "                                          \n",
    "    X = Add(name=base + '_Mask_Add_after_Interpool_1')([X, Residual_id_2_Down_branched])\n",
    "                                          \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_2_Up')\n",
    "    \n",
    "    temp_name2 = name_base+ \"_Mask_Interpool_2\"\n",
    "    \n",
    "    X = Lambda(interpolation, arguments={'ref_tensor': X_Trunk,'name':temp_name2})(X)\n",
    "                                          \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '_Mask_Interpool_2_bn_1')(X)\n",
    "                                          \n",
    "    X = Activation('relu', name=name_base + '_Mask_Interpool_2_relu_1')(X)\n",
    "                                          \n",
    "    X = Conv2D(F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '_Mask_Interpool_2_conv_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '_Mask_Interpool_2_bn_2')(X)\n",
    "                                          \n",
    "    X = Activation('relu', name=name_base + '_Mask_Interpool_2_relu_2')(X)\n",
    "                                          \n",
    "    X = Conv2D(F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '_Mask_Interpool_2_conv_2', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    X = Activation('sigmoid', name=name_base+'_Mask_sigmoid')(X)\n",
    "      \n",
    "    X = Multiply(name=name_base+'_Mutiply')([X_Trunk,X])\n",
    "    \n",
    "    X = Add(name=name_base+'_Add')([X_Trunk,X])\n",
    "\n",
    "    X = res_identity(X, filters, name_base+ '_Post_Residual_id')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def Attention_3(X, filters, base):\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    name_base = base\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Pre_Residual_id')\n",
    "    \n",
    "    X_Trunk = Trunk_block(X, filters, name_base+ '_Trunk')\n",
    "    \n",
    "    X = MaxPooling2D((3,3), strides=(2,2), padding='same', name=name_base+ '_Mask_pool_1')(X)\n",
    "    \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_1_Down')\n",
    "                                          \n",
    "    X = res_identity(X, filters, name_base+ '_Mask_Residual_id_1_Up')\n",
    "    \n",
    "    temp_name2 = name_base+ \"_Mask_Interpool_1\"\n",
    "    \n",
    "    X = Lambda(interpolation, arguments={'ref_tensor': X_Trunk,'name':temp_name2})(X)\n",
    "                                          \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '_Mask_Interpool_2_bn_1')(X)\n",
    "                                          \n",
    "    X = Activation('relu', name=name_base + '_Mask_Interpool_2_relu_1')(X)\n",
    "                                          \n",
    "    X = Conv2D(F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '_Mask_Interpool_2_conv_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    X = BatchNormalization(axis=-1, name=name_base + '_Mask_Interpool_2_bn_2')(X)\n",
    "                                          \n",
    "    X = Activation('relu', name=name_base + '_Mask_Interpool_2_relu_2')(X)\n",
    "                                          \n",
    "    X = Conv2D(F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=name_base + '_Mask_Interpool_2_conv_2', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    X = Activation('sigmoid', name=name_base+'_Mask_sigmoid')(X)\n",
    "      \n",
    "    X = Multiply(name=name_base+'_Mutiply')([X_Trunk,X])\n",
    "    \n",
    "    X = Add(name=name_base+'_Add')([X_Trunk,X])\n",
    "\n",
    "    X = res_identity(X, filters, name_base+ '_Post_Residual_id')\n",
    "    \n",
    "    return X\n",
    "\n",
    "class Resnet_Attention_56():\n",
    "    @staticmethod\n",
    "    def Resnet_Attention_56(input_shape,classes=100,weights=\"imagenet\"):\n",
    "\n",
    "        X_input = Input(input_shape)\n",
    "\n",
    "        X = Conv2D(64, (7,7), strides=(2,2), padding='same', name='conv_1', kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
    "        X = BatchNormalization(axis=-1, name='bn_1')(X)\n",
    "        X = Activation('relu', name='relu_1')(X)\n",
    "        X = MaxPooling2D((3,3), strides=(2,2), padding='same' ,name='pool_1')(X)\n",
    "        X = res_conv(X, [64,64,256], 'Residual_conv_1', 1)\n",
    "\n",
    "        ### Attention 1 Start\n",
    "        X = Attention_1(X, [64,64,256], 'Attention_1')\n",
    "        ### Attention 1 End\n",
    "\n",
    "        X = res_conv(X, [128,128,512], 'Residual_conv_2', 2)\n",
    "\n",
    "        ### Attention 2 Start\n",
    "        X = Attention_2(X, [128,128,512], 'Attention_2')\n",
    "        ### Attention 2 End\n",
    "\n",
    "        X = res_conv(X, [256,256,1024], 'Residual_conv_3', 2)\n",
    "\n",
    "        ### Attention 3 Start\n",
    "        X = Attention_3(X, [256,256,1024], 'Attention_3')\n",
    "        ### Attention 3 End\n",
    "\n",
    "        X = res_conv(X, [512,512,2048], 'Residual_conv_4', 2)\n",
    "\n",
    "        X = res_identity(X, [512,512,2048], 'Residual_id_1')\n",
    "        X = res_identity(X, [512,512,2048], 'Residual_id_2')\n",
    "        X = BatchNormalization(axis=-1, name='bn_2')(X)\n",
    "        X = Activation('relu', name='relu_2')(X)\n",
    "        X = AveragePooling2D((7,7), strides=(1,1), name='avg_pool')(X)\n",
    "        X = Flatten()(X)\n",
    "        X = Dense(classes, name='Dense_1')(X)\n",
    "        X = Activation('softmax', name='classifier')(X)\n",
    "\n",
    "        model = Model(inputs=X_input, outputs=X, name='attention_56')\n",
    "\n",
    "        if os.path.isfile(weights):\n",
    "            model.load_weights(weights)\n",
    "            print(\"Model loaded\")\n",
    "        else:\n",
    "            print(\"No model is found\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model= Resnet_Attention_56(input_shape=(224, 224, 3),classes=2,weights='imagenet')\n",
    "x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "prediction = Dense(output_classes, activation=tf.nn.softmax)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd_opt = SGD(lr = 0.02, decay=75e-6, momentum=0.9, nesterov=True)\n",
    "# adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)\n",
    "adam_opt=Adam(lr = 0.0001, beta_1=0.6, beta_2=0.99, epsilon=None, decay=0.0, amsgrad=True)\n",
    "sgd_opt = SGD(lr=1e-06, momentum=0.0, decay=0.0, nesterov=False)\n",
    "rmsp_opt = RMSprop(lr=1e-4, decay=0.9)\n",
    "\n",
    "model.compile(optimizer= adam_opt, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, verbose = 1)]\n",
    "\n",
    "history = model.fit_generator(\n",
    "  train_generator,\n",
    "  steps_per_epoch = nb_train_samples // batch_size,\n",
    "  epochs = epochs,\n",
    "  validation_data = validation_generator,\n",
    "  validation_steps = nb_validation_samples // batch_size,\n",
    "  callbacks = callbacks)\n",
    "\n",
    "# with open('models/vgg19_history.txt','w') as f:\n",
    "#     f.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Acc', 'Test Acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Test Loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# N = epochs\n",
    "# plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Validation Score: ', score[0])\n",
    "print ('Validation Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_generator.filenames\n",
    "truth = test_generator.classes\n",
    "label = test_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "predicts = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "#     fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "#     plt.savefig('plots/3.InceptionV3-2-Private-DataSet-CM.png', bbox_inches='tight', dpi = 100) \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm, classes=labels, title=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = predicts\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = test_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in test_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy = (cm[0,0]+cm[1,1]) / total\n",
    "print ('Accuracy : ', accuracy*100)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "print('Sensitivity : ', sensitivity*100 )\n",
    "\n",
    "Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print('Specificity : ', Specificity*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "th = 0.3\n",
    "\n",
    "acc = accuracy_score(truth,predict_class > th)\n",
    "prec = precision_score(truth,predict_class > th)\n",
    "f1 = f1_score(truth,predict_class > th)\n",
    "recall = recall_score(truth,predict_class > th)\n",
    "\n",
    "print('Accuracy:  {:.4f}'.format(acc*100))\n",
    "print('Precision: {:.4f}'.format(prec*100))\n",
    "print('Recall:    {:.4f}'.format(recall*100))\n",
    "print('F1:        {:.4f}'.format(f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_generator.classes, predict_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 1\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#plotting sensitivity and specificity\n",
    "plt.figure()\n",
    "plt.plot(thresholds, 1-fpr, label = 'specificity')\n",
    "plt.plot(thresholds, tpr, label = 'sensitivity')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/14.MobileNetV2-BreaKHis-Model.h5')\n",
    "model.save_weights('models/14.MobileNetV2-BreaKHis-Weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt2=Adam(lr=1e-05, beta_1=0.6, beta_2=0.9, epsilon=None, decay=0.0, amsgrad=True)\n",
    "model.compile(optimizer= adam_opt2, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, verbose = 1)]\n",
    "\n",
    "history = model.fit_generator(\n",
    "  train_generator,\n",
    "  steps_per_epoch = nb_train_samples // batch_size,\n",
    "  epochs = epochs,\n",
    "  validation_data = validation_generator,\n",
    "  validation_steps = nb_validation_samples // batch_size,\n",
    "  callbacks = callbacks)\n",
    "\n",
    "# with open('models/vgg19_history2.txt','w') as f:\n",
    "#     f.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Acc', 'Test Acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Test Loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# N = 12\n",
    "# plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Validation Score: ', score[0])\n",
    "print ('Validation Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_generator.filenames\n",
    "truth = test_generator.classes\n",
    "label = test_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "predicts = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "#     fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "#     plt.savefig('plots/3.InceptionV3-2-Private-DataSet-CM.png', bbox_inches='tight', dpi = 100) \n",
    "# fig.savefig('plots/1.Xception-CM.png') \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plot_confusion_matrix(cm, classes=labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "y_pred = predicts\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = test_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in test_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy = (cm[0,0]+cm[1,1]) / total\n",
    "print ('Accuracy : ', accuracy*100)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "print('Sensitivity : ', sensitivity*100 )\n",
    "\n",
    "Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print('Specificity : ', Specificity*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "th = 0.3\n",
    "\n",
    "acc = accuracy_score(truth,predict_class > th)\n",
    "prec = precision_score(truth,predict_class > th)\n",
    "f1 = f1_score(truth,predict_class > th)\n",
    "recall = recall_score(truth,predict_class > th)\n",
    "\n",
    "print('Accuracy:  {:.4f}'.format(acc*100))\n",
    "print('Precision: {:.4f}'.format(prec*100))\n",
    "print('Recall:    {:.4f}'.format(recall*100))\n",
    "print('F1:        {:.4f}'.format(f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_generator.classes, predict_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 1\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#plotting sensitivity and specificity\n",
    "plt.figure()\n",
    "plt.plot(thresholds, 1-fpr, label = 'specificity')\n",
    "plt.plot(thresholds, tpr, label = 'sensitivity')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/14.MobileNetV2-2-BreaKHis-Model.h5')\n",
    "model.save_weights('models/14.MobileNetV2-2-BreaKHis-Weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
