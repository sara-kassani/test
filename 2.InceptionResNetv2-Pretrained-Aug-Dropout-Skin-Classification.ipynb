{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15436567640692153044, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 9202108990\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 7733679108197573041\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "random_seed = np.random.seed(42)\n",
    "from glob import glob\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "import keras.callbacks as kcall\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.initializers import he_normal, glorot_normal\n",
    "from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, ZeroPadding2D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "validation_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train 0\n",
      "data/train\\AKIEC 3856\n",
      "data/train\\BCC 4773\n",
      "data/train\\BKL 3275\n",
      "data/train\\DF 4152\n",
      "data/train\\MEL 3308\n",
      "data/train\\NV 4975\n",
      "data/train\\VASC 3488\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk(train_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/validation 0\n",
      "data/validation\\AKIEC 1552\n",
      "data/validation\\BCC 1692\n",
      "data/validation\\BKL 1479\n",
      "data/validation\\DF 1632\n",
      "data/validation\\MEL 1664\n",
      "data/validation\\NV 1730\n",
      "data/validation\\VASC 1484\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk(validation_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27827 images belonging to 7 classes.\n",
      "Found 11233 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "seed = 1\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "\n",
    "# target_size = (height, width)\n",
    "target_size = (299, 299)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        batch_size = batch_size,\n",
    "        target_size = target_size,       \n",
    "        class_mode = 'categorical',\n",
    "        seed=seed,\n",
    "        shuffle = True)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        batch_size = batch_size,\n",
    "        target_size = target_size,        \n",
    "        class_mode = 'categorical',\n",
    "        seed=seed,\n",
    "        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "        \n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intilizing variables\n",
    "output_classes = 7\n",
    "\n",
    "epochs = 50\n",
    "adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)\n",
    "# inceptionresnetv2_weights= 'pretrained-models/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 134s 1us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionResNetV2(\n",
    "    include_top=False, weights= \"imagenet\", input_tensor=None, input_shape=(299, 299, 3), pooling='max')\n",
    "\n",
    "x = base_model.output\n",
    "# x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(7, activation=\"softmax\", kernel_initializer=glorot_normal(),\n",
    "                    kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "for layer in base_model.layers[:-4]:\n",
    "    # print(layer)\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= adam_opt,\n",
    "              metrics = [\"categorical_accuracy\", fmeasure, recall, precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2048/2048 [==============================] - 641s 313ms/step - loss: 3.2817 - categorical_accuracy: 0.1417 - fmeasure: 0.0974 - recall: 0.0749 - precision: 0.1420 - val_loss: 3.4316 - val_categorical_accuracy: 0.1442 - val_fmeasure: 0.0921 - val_recall: 0.0689 - val_precision: 0.1444\n",
      "Epoch 2/50\n",
      "2048/2048 [==============================] - 631s 308ms/step - loss: 3.0983 - categorical_accuracy: 0.1445 - fmeasure: 0.0894 - recall: 0.0655 - precision: 0.1442 - val_loss: 3.2006 - val_categorical_accuracy: 0.1460 - val_fmeasure: 0.0825 - val_recall: 0.0568 - val_precision: 0.1585ure: 0.0\n",
      "Epoch 3/50\n",
      "2048/2048 [==============================] - 632s 308ms/step - loss: 2.9804 - categorical_accuracy: 0.1437 - fmeasure: 0.0838 - recall: 0.0596 - precision: 0.1455 - val_loss: 2.8890 - val_categorical_accuracy: 0.1502 - val_fmeasure: 0.0584 - val_recall: 0.0374 - val_precision: 0.1457\n",
      "Epoch 4/50\n",
      "1446/2048 [====================>.........] - ETA: 2:05 - loss: 2.9179 - categorical_accuracy: 0.1423 - fmeasure: 0.0763 - recall: 0.0529 - precision: 0.1416"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "    steps_per_epoch=2048,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_final.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Test Score: ', score[0])\n",
    "print ('Test Accuracy: ',score[1])\n",
    "print ('fmeasure: ', score[2])\n",
    "print ('Recall: ', score[3])\n",
    "print ('Precision: ', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = epochs\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"categorical_accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_categorical_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = validation_generator.filenames\n",
    "truth = validation_generator.classes\n",
    "label = validation_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "predicts = model.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig('plots/3.DenseNet201-CM.png', bbox_inches='tight', dpi = 100)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm, classes=labels,\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve for each class\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = predicts\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = validation_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in validation_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "y_actual_binary = label_binarize(y_actual, classes=[0, 1, 2, 3, 4,5,6])\n",
    "y_pred_binary = y_pred_probabilities#label_binarize(y_pred_probabilities, classes=[0, 1, 2, 3, 4])\n",
    "n_classes=7\n",
    "lw=2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_actual_binary[:, i], y_pred_binary[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_actual_binary.ravel(), y_pred_binary.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "\n",
    "colors = cycle(['red','blue','green','yellow','orange', 'aqua', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(classnames[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=6)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=6)\n",
    "\n",
    "\n",
    "colors = cycle(['red','blue','green','yellow','orange', 'aqua', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(classnames[i], roc_auc[i]))\n",
    "\n",
    "    \n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-classs')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/3.DenseNet201-ROC.png', dpi = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/3.DenseNet201-Adam-Dropout-Model.h5')\n",
    "model.save_weights('models/3.DenseNet201-Adam-Dropout-Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"models/3.DenseNet201-Adam-Dropout-Model.h5\", custom_objects={'fmeasure': fmeasure,  'recall': recall, 'precision': precision })\n",
    "model.load_weights(\"models/3.DenseNet201-Adam-Dropout-Weights.h5.h5\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
